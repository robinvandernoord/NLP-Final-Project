{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"question_generation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e5b614819fd54f00ad0608829b8f91de":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3c979b1074da45299bc1026ebc7cd369","IPY_MODEL_ea5d1b0dac7f4d1c952f127e123c051c","IPY_MODEL_a4ad27ed0d9242449679599a4e0c9666"],"layout":"IPY_MODEL_4ac2c44141df41ca86e19f041efb3e45"}},"3c979b1074da45299bc1026ebc7cd369":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d77db7326534837914ced2acfaac97d","placeholder":"​","style":"IPY_MODEL_950a7c7768cf4c4c95cf2eb6358f0583","value":"100%"}},"ea5d1b0dac7f4d1c952f127e123c051c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_210837ebee874a62993d85c0e12157f2","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_661a7edc01e348d98464586fdd168ccf","value":1}},"a4ad27ed0d9242449679599a4e0c9666":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e81d802679f34904b9c8d741abd2a93e","placeholder":"​","style":"IPY_MODEL_8dc440ec07664c479140dce7a13d0fe7","value":" 1/1 [00:00&lt;00:00, 12.31it/s]"}},"4ac2c44141df41ca86e19f041efb3e45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d77db7326534837914ced2acfaac97d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"950a7c7768cf4c4c95cf2eb6358f0583":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"210837ebee874a62993d85c0e12157f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"661a7edc01e348d98464586fdd168ccf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e81d802679f34904b9c8d741abd2a93e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8dc440ec07664c479140dce7a13d0fe7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Unsupervised Question + Answer Generation"],"metadata":{"id":"2Ayjqf-k9CyG"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"duu1Qj-zoehO","executionInfo":{"status":"ok","timestamp":1649523446274,"user_tz":-120,"elapsed":897,"user":{"displayName":"Robin van der Noord","userId":"10122466808794390914"}},"outputId":"4d6dca09-1030-4675-a587-13059638c720"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Apr  9 16:56:58 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   69C    P0    30W /  70W |   6216MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","source":["## Loading Models"],"metadata":{"id":"PQ5JJJM99BdB"}},{"cell_type":"code","source":["!pip install farm-haystack datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CN5lPTn3EoaS","outputId":"79a55d32-fd41-41ae-82e9-5879e65958f9","executionInfo":{"status":"ok","timestamp":1649523454677,"user_tz":-120,"elapsed":8406,"user":{"displayName":"Robin van der Noord","userId":"10122466808794390914"}}},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: farm-haystack in /usr/local/lib/python3.7/dist-packages (1.3.0)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.0.0)\n","Requirement already satisfied: elasticsearch<=7.10,>=7.7 in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (7.10.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (4.63.0)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (1.9.0)\n","Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (1.2.2)\n","Requirement already satisfied: elastic-apm in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (6.9.1)\n","Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (1.0.2)\n","Requirement already satisfied: quantulum3 in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (0.7.10)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (3.2.5)\n","Requirement already satisfied: azure-ai-formrecognizer==3.2.0b2 in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (3.2.0b2)\n","Requirement already satisfied: azure-core<1.23 in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (1.22.1)\n","Requirement already satisfied: python-docx in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (0.8.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (1.3.5)\n","Requirement already satisfied: transformers==4.13.0 in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (4.13.0)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (4.3.3)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (0.3.4)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (2.6.3)\n","Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (2.0.8)\n","Requirement already satisfied: mlflow<=1.13.1 in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (1.13.1)\n","Requirement already satisfied: mmh3 in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (3.0.0)\n","Requirement already satisfied: torch<1.11,>1.9 in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (1.10.0+cu111)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (1.4.1)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (8.12.0)\n","Requirement already satisfied: posthog in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (1.4.6)\n","Requirement already satisfied: tika in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (1.24)\n","Requirement already satisfied: sentence-transformers>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (2.2.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (4.11.3)\n","Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (from farm-haystack) (1.0.9)\n","Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from azure-ai-formrecognizer==3.2.0b2->farm-haystack) (1.15.0)\n","Requirement already satisfied: azure-common~=1.1 in /usr/local/lib/python3.7/dist-packages (from azure-ai-formrecognizer==3.2.0b2->farm-haystack) (1.1.28)\n","Requirement already satisfied: msrest>=0.6.21 in /usr/local/lib/python3.7/dist-packages (from azure-ai-formrecognizer==3.2.0b2->farm-haystack) (0.6.21)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0->farm-haystack) (2019.12.20)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0->farm-haystack) (0.5.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0->farm-haystack) (0.0.49)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0->farm-haystack) (1.21.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0->farm-haystack) (3.6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0->farm-haystack) (21.3)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0->farm-haystack) (0.10.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0->farm-haystack) (6.0)\n","Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch<=7.10,>=7.7->farm-haystack) (1.25.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch<=7.10,>=7.7->farm-haystack) (2021.10.8)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.13.0->farm-haystack) (3.10.0.2)\n","Requirement already satisfied: prometheus-flask-exporter in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack) (0.20.1)\n","Requirement already satisfied: databricks-cli>=0.8.7 in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack) (0.16.6)\n","Requirement already satisfied: azure-storage-blob>=12.0.0 in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack) (12.11.0)\n","Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack) (0.4.2)\n","Requirement already satisfied: querystring-parser in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack) (1.2.4)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack) (0.4)\n","Requirement already satisfied: gitpython>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack) (3.1.27)\n","Requirement already satisfied: alembic<=1.4.1 in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack) (1.4.1)\n","Requirement already satisfied: gunicorn in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack) (20.1.0)\n","Requirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack) (1.1.4)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack) (3.17.3)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack) (2.8.2)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack) (1.3.0)\n","Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack) (1.4.32)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack) (7.1.2)\n","Requirement already satisfied: docker>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from mlflow<=1.13.1->farm-haystack) (5.0.3)\n","Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.7/dist-packages (from alembic<=1.4.1->mlflow<=1.13.1->farm-haystack) (1.0.4)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic<=1.4.1->mlflow<=1.13.1->farm-haystack) (1.2.0)\n","Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm-haystack) (36.0.2)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.1.4->azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm-haystack) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.0.0->mlflow<=1.13.1->farm-haystack) (2.21)\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow<=1.13.1->farm-haystack) (0.8.9)\n","Requirement already satisfied: pyjwt>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow<=1.13.1->farm-haystack) (2.3.0)\n","Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow<=1.13.1->farm-haystack) (3.2.0)\n","Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from docker>=4.0.0->mlflow<=1.13.1->farm-haystack) (1.3.2)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython>=2.1.0->mlflow<=1.13.1->farm-haystack) (4.0.9)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow<=1.13.1->farm-haystack) (5.0.0)\n","Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.21->azure-ai-formrecognizer==3.2.0b2->farm-haystack) (1.3.1)\n","Requirement already satisfied: isodate>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from msrest>=0.6.21->azure-ai-formrecognizer==3.2.0b2->farm-haystack) (0.6.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.13.0->farm-haystack) (3.0.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->farm-haystack) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->farm-haystack) (2.10)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->farm-haystack) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->farm-haystack) (1.1.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.0->farm-haystack) (0.11.1+cu111)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.4.0->farm-haystack) (0.1.96)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->mlflow<=1.13.1->farm-haystack) (1.1.2)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.3.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow<=1.13.1->farm-haystack) (1.0.1)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow<=1.13.1->farm-haystack) (1.1.0)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow<=1.13.1->farm-haystack) (2.11.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask->mlflow<=1.13.1->farm-haystack) (2.0.1)\n","Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.7/dist-packages (from gunicorn->mlflow<=1.13.1->farm-haystack) (57.4.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->farm-haystack) (3.7.0)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->farm-haystack) (5.4.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->farm-haystack) (0.18.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->farm-haystack) (2018.9)\n","Requirement already satisfied: backoff<2.0.0,>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from posthog->farm-haystack) (1.11.1)\n","Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.7/dist-packages (from posthog->farm-haystack) (1.6)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter->mlflow<=1.13.1->farm-haystack) (0.13.1)\n","Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx->farm-haystack) (4.2.6)\n","Requirement already satisfied: num2words in /usr/local/lib/python3.7/dist-packages (from quantulum3->farm-haystack) (0.5.10)\n","Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (from quantulum3->farm-haystack) (2.1.0)\n","Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words->quantulum3->farm-haystack) (0.6.2)\n","Requirement already satisfied: jarowinkler<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from rapidfuzz->farm-haystack) (1.0.2)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers>=0.4.0->farm-haystack) (7.1.2)\n"]}]},{"cell_type":"markdown","source":["### Questions Model"],"metadata":{"id":"YfBu2KuIE_Ir"}},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f3fq55GMERAx","outputId":"8c175fb9-98cb-4453-f28b-6e4ea2774dc1","executionInfo":{"status":"ok","timestamp":1649523466073,"user_tz":-120,"elapsed":11403,"user":{"displayName":"Robin van der Noord","userId":"10122466808794390914"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO - haystack.modeling.utils -  Using devices: CUDA\n","INFO - haystack.modeling.utils -  Number of GPUs: 1\n"]}],"source":["from haystack.nodes import QuestionGenerator\n","\n","QG_model = 'valhalla/t5-base-e2e-qg'  # default\n","# QG_model = 'valhalla/t5-small-e2e-qg'  # small\n","# QG_model = 'allenai/unifiedqa-t5-base' # n/a\n","\n","question_generator = QuestionGenerator(model_name_or_path=QG_model)"]},{"cell_type":"markdown","source":["### Questions and Answers Pipeline"],"metadata":{"id":"OqP1Vmp2E8Ac"}},{"cell_type":"code","source":["from haystack.pipeline import QuestionAnswerGenerationPipeline\n","from haystack.nodes import FARMReader\n","\n","# squad is used to generate the answers for the generated questions\n","reader = FARMReader(\"deepset/roberta-base-squad2\")\n","\n","qag_pipeline = QuestionAnswerGenerationPipeline(question_generator, reader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O6YWwj-6GPpE","outputId":"d982b693-c86a-44ad-d699-2200887095e0","executionInfo":{"status":"ok","timestamp":1649523479896,"user_tz":-120,"elapsed":13828,"user":{"displayName":"Robin van der Noord","userId":"10122466808794390914"}}},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO - haystack.modeling.utils -  Using devices: CUDA\n","INFO - haystack.modeling.utils -  Number of GPUs: 1\n","INFO - haystack.modeling.model.language_model -  LOADING MODEL\n","INFO - haystack.modeling.model.language_model -  =============\n","INFO - haystack.modeling.model.language_model -  Could not find deepset/roberta-base-squad2 locally.\n","INFO - haystack.modeling.model.language_model -  Looking on Transformers Model Hub (in local cache and online)...\n","INFO - haystack.modeling.model.language_model -  Loaded deepset/roberta-base-squad2\n","INFO - haystack.modeling.logger -  ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n","INFO - haystack.modeling.utils -  Using devices: CUDA\n","INFO - haystack.modeling.utils -  Number of GPUs: 1\n","INFO - haystack.modeling.infer -  Got ya 2 parallel workers to do inference ...\n","INFO - haystack.modeling.infer -   0     0  \n","INFO - haystack.modeling.infer -  /w\\   /w\\ \n","INFO - haystack.modeling.infer -  /'\\   / \\ \n","WARNING - haystack.nodes.base -  Unnamed __init__ parameters will not be saved to YAML if Pipeline.save_to_yaml() is called!\n"]}]},{"cell_type":"markdown","source":["### Loading Data"],"metadata":{"id":"C-d_DXUW9NWP"}},{"cell_type":"code","source":["import io\n","from dataclasses import dataclass\n","\n","# qag pipeline expects an object with content and id properties for some reason, so we fake that here:\n","@dataclass\n","class Doc:\n","    id: int\n","    content: str\n","\n","    def __repr__(self):\n","        # only show part of document when representing:\n","        c = self.content\n","        return f\"<Doc {self.id} - {c[:3]}...{c[-3:]}>\""],"metadata":{"id":"PY2eadFFGrrM","executionInfo":{"status":"ok","timestamp":1649523479896,"user_tz":-120,"elapsed":8,"user":{"displayName":"Robin van der Noord","userId":"10122466808794390914"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","import pandas as pd\n","_dataset = load_dataset('GroNLP/ik-nlp-22_slp')\n","df_train = pd.DataFrame(_dataset['train'])\n","\n","df_train_summaries = df_train.loc[df_train['section'] == \"Summary\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["e5b614819fd54f00ad0608829b8f91de","3c979b1074da45299bc1026ebc7cd369","ea5d1b0dac7f4d1c952f127e123c051c","a4ad27ed0d9242449679599a4e0c9666","4ac2c44141df41ca86e19f041efb3e45","4d77db7326534837914ced2acfaac97d","950a7c7768cf4c4c95cf2eb6358f0583","210837ebee874a62993d85c0e12157f2","661a7edc01e348d98464586fdd168ccf","e81d802679f34904b9c8d741abd2a93e","8dc440ec07664c479140dce7a13d0fe7"]},"id":"hCABaUZ_hCrI","executionInfo":{"status":"ok","timestamp":1649523482161,"user_tz":-120,"elapsed":2272,"user":{"displayName":"Robin van der Noord","userId":"10122466808794390914"}},"outputId":"d0715d48-e7a5-4fe6-8372-20ca4a86ef76"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING - datasets.builder -  No config specified, defaulting to: ik_nlp22_slp/paragraphs\n","WARNING - datasets.builder -  Reusing dataset ik_nlp22_slp (/root/.cache/huggingface/datasets/GroNLP___ik_nlp22_slp/paragraphs/1.0.0/6c89281b2028a8a126102dda2c3fb94b1a5ccea59943d26857ae138c7aa782f8)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5b614819fd54f00ad0608829b8f91de"}},"metadata":{}}]},{"cell_type":"code","source":["%%script echo v1, non-cleaned\n","\n","docs = {}\n","\n","# for index, row in df_train.iterrows():\n","for index, row in df_train_summaries.iterrows():\n","    docs[str(index)] = Doc(index, row['text'])"],"metadata":{"id":"Fn1VZ37LhU65","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649523482584,"user_tz":-120,"elapsed":425,"user":{"displayName":"Robin van der Noord","userId":"10122466808794390914"}},"outputId":"0ebc075c-7c01-49ef-85c0-b4e6331a78d9"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["v1, non-cleaned\n"]}]},{"cell_type":"code","source":["# %%script echo v2, cleaned\n","\n","# v2, removes bullet point character and splits sentences on it\n","# and remove '-' or ' ' at the start of the sentence\n","\n","docs = {}\n","\n","index = 0\n","for _, row in df_train_summaries.iterrows():\n","\n","    for text in filter(lambda _: _, row['text'].split('•')):\n","        index += 1\n","        text = text.strip(' -')\n","        docs[str(index)] = Doc(index, text)\n","\n","print(\"amount of docs:\", len(docs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sUQ482a65PNw","executionInfo":{"status":"ok","timestamp":1649523482585,"user_tz":-120,"elapsed":4,"user":{"displayName":"Robin van der Noord","userId":"10122466808794390914"}},"outputId":"5c897321-ce0a-402c-cfd5-54df80bf5a23"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["amount of docs: 71\n"]}]},{"cell_type":"markdown","source":["## Paraphrasing Input Data"],"metadata":{"id":"vnp489UV7DnP"}},{"cell_type":"code","source":["from transformers import pipeline\n","\n","PARAPHRASE_MODEL = 'tuner007/pegasus_paraphrase'\n","\n","# device = 0 to use GPU \n","paraphrase_pipeline = pipeline(\"text2text-generation\", model=PARAPHRASE_MODEL, device=0)"],"metadata":{"id":"f_LXeGSL7Fy_","executionInfo":{"status":"ok","timestamp":1649523515985,"user_tz":-120,"elapsed":33402,"user":{"displayName":"Robin van der Noord","userId":"10122466808794390914"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["def paraphrase(text, n=4):\n","    # generate 4 alternatives, skip paraphrases that are not questions (= don't end with ?)\n","    return [p['generated_text'] for p in paraphrase_pipeline(text, num_return_sequences=n)]"],"metadata":{"id":"ZwHMXUWn8noR","executionInfo":{"status":"ok","timestamp":1649523515986,"user_tz":-120,"elapsed":8,"user":{"displayName":"Robin van der Noord","userId":"10122466808794390914"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["import spacy\n","nlp = spacy.load('en_core_web_sm')\n","\n","def split_sentences(sent):\n","    return nlp(sent).sents"],"metadata":{"id":"eFLkZXsg8B9Z","executionInfo":{"status":"ok","timestamp":1649523516693,"user_tz":-120,"elapsed":714,"user":{"displayName":"Robin van der Noord","userId":"10122466808794390914"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["%%script echo v1\n","\n","# extra: extending the documents with paraphrases\n","\n","for doc in list(docs.values()):\n","    for sentence in split_sentences(doc.content):\n","\n","        # max length of paraphrase() is 55 tokens, cut off after:\n","        for alternative in paraphrase(str(sentence[:55]), 3):\n","            index += 1 # continued from previous block\n","            docs[str(index)] = Doc(index, alternative)\n","\n","\n","print(\"amount of docs:\", len(docs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nWfDVcJ-ZsM5","executionInfo":{"status":"ok","timestamp":1649523517111,"user_tz":-120,"elapsed":420,"user":{"displayName":"Robin van der Noord","userId":"10122466808794390914"}},"outputId":"4f874030-f20d-45d2-ceab-11bf557f215e"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["v1\n"]}]},{"cell_type":"code","source":["# %%script echo v2, cleaned\n","\n","# extra: extending the documents with paraphrases (improved, cleaned)\n","\n","for doc in list(docs.values()):\n","    for sentence in split_sentences(doc.content):\n","        # convert to string and clean:\n","        sentence = str(sentence).strip(' -')\n","\n","        # max length of paraphrase() is 60, so we split on sentences to prevent long input\n","        for alternative in paraphrase(sentence, 3):\n","            index += 1 # continued from previous block\n","            docs[str(index)] = Doc(index, alternative)\n","\n","\n","print(\"amount of docs:\", len(docs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l2ge6oce4DTZ","executionInfo":{"status":"ok","timestamp":1649523576144,"user_tz":-120,"elapsed":59035,"user":{"displayName":"Robin van der Noord","userId":"10122466808794390914"}},"outputId":"12e58308-d630-4791-c96a-aa3656ee299e"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/pipelines/base.py:1079: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  UserWarning,\n"]},{"output_type":"stream","name":"stdout","text":["amount of docs: 175\n"]}]},{"cell_type":"code","source":["from time import time\n","from IPython.display import clear_output\n","\n","def generate_qa_pairs(d):\n","    qa_pairs = qag_pipeline.run(documents=d)\n","\n","    pairs = []\n","    for pair in qa_pairs['results']:\n","        _ans = pair['answers']\n","        if not _ans:\n","            continue\n","\n","        ans = _ans[0]\n","\n","        id = ans.document_id\n","        if '-' in id:\n","            id = id.split('-')[0]\n","\n","        doc = docs[id]  # 0-0 -> 0\n","        \n","        pairs.append({\n","            'question': pair['query'],\n","            'answer': ans.answer,\n","            # 'context': ans.context, # <- very short, not full paragraph\n","            'context': doc.content,\n","            'start_pos': ans.offsets_in_document[0].start,\n","        })\n","\n","    return pairs"],"metadata":{"id":"m466kLWClH0Y","executionInfo":{"status":"ok","timestamp":1649523576144,"user_tz":-120,"elapsed":9,"user":{"displayName":"Robin van der Noord","userId":"10122466808794390914"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["%%script echo skipping\n","\n","# method 1: pass all docs at once\n","# note: this method seems to yield only a few qa pairs\n","t = time()\n","\n","result = generate_qa_pairs(docs.values())\n","\n","clear_output(wait=True)\n","print(len(result), \"pairs generated\")\n","print(\"Generating took:\", time() - t)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1sIRbmKUlbFB","executionInfo":{"status":"ok","timestamp":1649523576552,"user_tz":-120,"elapsed":416,"user":{"displayName":"Robin van der Noord","userId":"10122466808794390914"}},"outputId":"a55ddd77-39df-446d-c4c4-0543dd9f95ef"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["skipping\n"]}]},{"cell_type":"code","source":["# method 2: pass document by document\n","t = time()\n","\n","result = []\n","d_n = len(docs)\n","for idx, doc in enumerate(docs.values()):\n","    _result = generate_qa_pairs([doc])\n","    result.extend(_result)\n","    print(f\"{idx}/{d_n}: +{len(_result)} = {len(result)}\")\n","\n","clear_output(wait=True)\n","print(len(result), \"pairs generated\")\n","print(\"Generating took:\", time() - t)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xWy4Y30Mlfr4","executionInfo":{"status":"ok","timestamp":1649523705577,"user_tz":-120,"elapsed":129026,"user":{"displayName":"Robin van der Noord","userId":"10122466808794390914"}},"outputId":"4ed3aae8-5a04-427d-c2b1-16c22dc2fdb2"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["274 pairs generated\n","Generating took: 128.46997022628784\n"]}]},{"cell_type":"code","source":["from csv import DictWriter\n","\n","with open('qa.csv', 'w') as f:\n","    writer = DictWriter(f, fieldnames=['question', 'answer', 'context', 'start_pos'])\n","    writer.writeheader()\n","    for pair in result:\n","        if not pair['question'].endswith('?'):\n","            # skip non-questions\n","            continue\n","\n","        writer.writerow(pair)"],"metadata":{"id":"Xmd2PsS0Hvam","executionInfo":{"status":"ok","timestamp":1649523705578,"user_tz":-120,"elapsed":17,"user":{"displayName":"Robin van der Noord","userId":"10122466808794390914"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dWHUQvKwz_Vf","executionInfo":{"status":"ok","timestamp":1649523705578,"user_tz":-120,"elapsed":14,"user":{"displayName":"Robin van der Noord","userId":"10122466808794390914"}},"outputId":"362749e9-18f1-43b6-dedd-e10b24c47346"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'answer': 'the regular expression',\n","  'context': \"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:\",\n","  'question': ' What is a fundamental tool in language processing?',\n","  'start_pos': 67},\n"," {'answer': 'basic text normalization tasks',\n","  'context': \"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:\",\n","  'question': ' What did this chapter show how to perform?',\n","  'start_pos': 117},\n"," {'answer': 'language processing',\n","  'context': \"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:\",\n","  'question': ' The minimum edit distance algorithm was introduced for comparing strings.',\n","  'start_pos': 46},\n"," {'answer': 'minimum edit distance algorithm',\n","  'context': \"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:\",\n","  'question': ' What is comparing strings?',\n","  'start_pos': 265},\n"," {'answer': 'a summary',\n","  'context': \"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:\",\n","  'question': ' What are the main points we covered about these ideas?',\n","  'start_pos': 327},\n"," {'answer': 'The regular expression language',\n","  'context': 'The regular expression language is a powerful tool for pattern-matching.',\n","  'question': ' What is a powerful tool for pattern-matching?',\n","  'start_pos': 0},\n"," {'answer': 'concatenation of symbols, disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ, $) and precedence operators',\n","  'context': 'Basic operations in regular expressions include concatenation of symbols, disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ, $) and precedence operators ((,) ).',\n","  'question': ' What are some basic operations in regular expressions?',\n","  'start_pos': 48},\n"," {'answer': 'simple regular expression substitutions or finite automata',\n","  'context': 'Word tokenization and normalization are generally done by cascades of simple regular expression substitutions or finite automata.',\n","  'question': ' Word tokenization and normalization are usually done by cascades of what?',\n","  'start_pos': 70},\n"," {'answer': 'The Porter algorithm',\n","  'context': 'The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.',\n","  'question': ' What is a simple and efficient way to do stemming?',\n","  'start_pos': 0},\n"," {'answer': 'affixes',\n","  'context': 'The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.',\n","  'question': ' What does the Porter algorithm strip off?',\n","  'start_pos': 81},\n"," {'answer': 'the minimum number of operations it takes to edit one into the other',\n","  'context': 'The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.',\n","  'question': ' What is the minimum edit distance between two strings?',\n","  'start_pos': 49},\n"," {'answer': 'Minimum edit distance',\n","  'context': 'The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.',\n","  'question': ' What can be computed by dynamic programming?',\n","  'start_pos': 119},\n"," {'answer': 'n-gram',\n","  'context': 'This chapter introduced language modeling and the n-gram, one of the most widely used tools in language processing.',\n","  'question': ' What is one of the most widely used tools in language processing?',\n","  'start_pos': 50},\n"," {'answer': 'to predict a word from preceding words',\n","  'context': 'Language models offer a way to assign a probability to a sentence or other sequence of words, and to predict a word from preceding words.',\n","  'question': ' Language models offer a way to assign a probability to a sentence or other sequence of words?',\n","  'start_pos': 98},\n"," {'answer': 'n-grams',\n","  'context': 'n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).',\n","  'question': ' What are Markov models that estimate words from a fixed window of previous words?',\n","  'start_pos': 0},\n"," {'answer': 'by counting in a corpus and normalizing',\n","  'context': 'n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).',\n","  'question': ' How can n-gram probabilities be estimated?',\n","  'start_pos': 123},\n"," {'answer': 'intrinsically using perplexity',\n","  'context': 'n-gram language models are evaluated extrinsically in some task, or intrinsically using perplexity.',\n","  'question': ' What are n-gram language models evaluated extrinsically in some task?',\n","  'start_pos': 68},\n"," {'answer': 'the geometric mean of the inverse test set probability computed by the model',\n","  'context': 'The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.',\n","  'question': ' What is the perplexity of a test set according to a language model?',\n","  'start_pos': 62},\n"," {'answer': 'The perplexity',\n","  'context': 'The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.',\n","  'question': ' The geometric mean of the inverse test set probability computed by the model is what?',\n","  'start_pos': 0},\n"," {'answer': 'n-grams',\n","  'context': 'Smoothing algorithms provide a more sophisticated way to estimate the probability of n-grams. Commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through backoff or interpolation.',\n","  'question': ' Smoothing algorithms provide a more sophisticated way to estimate the probability of what?',\n","  'start_pos': 85},\n"," {'answer': 'interpolation',\n","  'context': 'Smoothing algorithms provide a more sophisticated way to estimate the probability of n-grams. Commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through backoff or interpolation.',\n","  'question': ' Commonly used smoothing algorithms rely on lower-order n-gram counts through backoff or what else?',\n","  'start_pos': 194},\n"," {'answer': 'discounting',\n","  'context': 'Both backoff and interpolation require discounting to create a probability distribution.',\n","  'question': ' What do both backoff and interpolation require to create a probability distribution?',\n","  'start_pos': 39},\n"," {'answer': 'interpolated Kneser-Ney smoothing algorithm mixes a discounted probability with a lower-order continuation probability.',\n","  'context': 'Kneser-Ney smoothing makes use of the probability of a word being a novel continuation. The interpolated Kneser-Ney smoothing algorithm mixes a discounted probability with a lower-order continuation probability.',\n","  'question': ' Kneser-Ney smoothing makes use of the probability of a word being a novel continuation.',\n","  'start_pos': 92},\n"," {'answer': 'interpolated Kneser-Ney smoothing algorithm',\n","  'context': 'Kneser-Ney smoothing makes use of the probability of a word being a novel continuation. The interpolated Kneser-Ney smoothing algorithm mixes a discounted probability with a lower-order continuation probability.',\n","  'question': ' What mix a discounted probability with a lower-order continuation probability?',\n","  'start_pos': 92},\n"," {'answer': 'Bayes model for classification',\n","  'context': 'This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.',\n","  'question': ' What model was introduced in this chapter?',\n","  'start_pos': 34},\n"," {'answer': 'text categorization task of sentiment analysis',\n","  'context': 'This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.',\n","  'question': ' What task did this chapter apply the Bayes model to?',\n","  'start_pos': 87},\n"," {'answer': 'classification',\n","  'context': 'Many language processing tasks can be viewed as tasks of classification.',\n","  'question': ' Many language processing tasks can be viewed as tasks of what?',\n","  'start_pos': 57},\n"," {'answer': 'sentiment analysis, spam detection, language identification, and authorship attribution',\n","  'context': 'Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.',\n","  'question': ' Text categorization assigns an entire text a class from a finite set of what?',\n","  'start_pos': 107},\n"," {'answer': 'sentiment analysis, spam detection, language identification, and authorship attribution',\n","  'context': 'Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.',\n","  'question': ' What are some of the tasks that text categorisation includes?',\n","  'start_pos': 107},\n"," {'answer': 'Sentiment analysis',\n","  'context': 'Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object.',\n","  'question': ' What classifies a text as reflecting the positive or negative orientation that a writer expresses toward some object?',\n","  'start_pos': 0},\n"," {'answer': \"position doesn't matter\",\n","  'context': \"Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class)\",\n","  'question': ' What is Naive Bayes a generative model that makes the bag of words assumption?',\n","  'start_pos': 74},\n"," {'answer': 'words are conditionally independent of each other given the class',\n","  'context': \"Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class)\",\n","  'question': ' What is the conditional independence assumption made?',\n","  'start_pos': 144},\n"," {'answer': 'given the class',\n","  'context': \"Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class)\",\n","  'question': ' How are words conditionally independent of each other?',\n","  'start_pos': 194},\n"," {'answer': 'many text classification tasks',\n","  'context': 'Naive Bayes with binarized features seems to work better for many text classification tasks.',\n","  'question': ' Naive Bayes with binarized features seems to work better for what?',\n","  'start_pos': 61},\n"," {'answer': 'precision and recall',\n","  'context': 'Classifiers are evaluated based on precision and recall.',\n","  'question': ' What are classifiers evaluated based on?',\n","  'start_pos': 35},\n"," {'answer': 'using distinct training, dev, and test sets',\n","  'context': 'Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.',\n","  'question': ' How are classifiers trained?',\n","  'start_pos': 24},\n"," {'answer': 'cross-validation',\n","  'context': 'Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.',\n","  'question': ' What type of training is included in the training set?',\n","  'start_pos': 90},\n"," {'answer': 'Statistical significance tests',\n","  'context': 'Statistical significance tests should be used to determine whether we can be confident that one version of a classifier is better than another.',\n","  'question': ' What should be used to determine if one version of a classifier is better than another?',\n","  'start_pos': 0},\n"," {'answer': 'harms that may be caused by the model',\n","  'context': 'Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.',\n","  'question': ' What should designers of classifiers carefully consider?',\n","  'start_pos': 51},\n"," {'answer': 'model characteristics',\n","  'context': 'Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.',\n","  'question': ' What should classifier designers report in a model card?',\n","  'start_pos': 151},\n"," {'answer': 'logistic regression',\n","  'context': 'This chapter introduced the logistic regression model of classification.',\n","  'question': ' What model of classification was introduced in this chapter?',\n","  'start_pos': 28},\n"," {'answer': 'Logistic regression',\n","  'context': 'Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.',\n","  'question': ' What is supervised machine learning classifier?',\n","  'start_pos': 0},\n"," {'answer': 'A threshold',\n","  'context': 'Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.',\n","  'question': ' What is used to make a decision?',\n","  'start_pos': 228},\n"," {'answer': 'multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability',\n","  'context': 'Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.',\n","  'question': ' How does logistic regression extract real-valued features from input?',\n","  'start_pos': 115},\n"," {'answer': 'two',\n","  'context': 'Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).',\n","  'question': ' How many classes can logistic regression be used with?',\n","  'start_pos': 37},\n"," {'answer': 'n-ary text classification',\n","  'context': 'Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).',\n","  'question': ' What is a good example of a multinomial logistic regression?',\n","  'start_pos': 164},\n"," {'answer': 'softmax',\n","  'context': 'Multinomial logistic regression uses the softmax function to compute probabilities.',\n","  'question': ' What function does multinomial logistic regression use to compute probabilities?',\n","  'start_pos': 41},\n"," {'answer': 'a loss function',\n","  'context': 'The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.',\n","  'question': ' What are the weights learned from a labeled training set via?',\n","  'start_pos': 78},\n"," {'answer': 'cross-entropy loss',\n","  'context': 'The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.',\n","  'question': ' What must be minimized?',\n","  'start_pos': 107},\n"," {'answer': 'Minimizing this loss function',\n","  'context': 'Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.',\n","  'question': ' What is a convex optimization problem?',\n","  'start_pos': 0},\n"," {'answer': 'iterative algorithms like gradient descent',\n","  'context': 'Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.',\n","  'question': ' What is used to find the optimal weights?',\n","  'start_pos': 68},\n"," {'answer': 'Regularization is used to avoid overfitting',\n","  'context': 'Regularization is used to avoid overfitting.',\n","  'question': ' Is regularization used to avoid overfitting?',\n","  'start_pos': 0},\n"," {'answer': 'Logistic regression',\n","  'context': 'Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features.',\n","  'question': ' What is one of the most useful analytic tools?',\n","  'start_pos': 0},\n"," {'answer': 'individual features',\n","  'context': 'Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features.',\n","  'question': ' Logistic regression studies the importance of what?',\n","  'start_pos': 131},\n"," {'answer': 'a vector',\n","  'context': 'In vector semantics, a word is modeled as a vector-a point in high-dimensional space, also called an embedding. In this chapter we focus on static embeddings, in each each word is mapped to a fixed embedding.',\n","  'question': ' In vector semantics, a word is modeled as what?',\n","  'start_pos': 42},\n"," {'answer': 'an embedding',\n","  'context': 'In vector semantics, a word is modeled as a vector-a point in high-dimensional space, also called an embedding. In this chapter we focus on static embeddings, in each each word is mapped to a fixed embedding.',\n","  'question': ' What is a point in high-dimensional space also called?',\n","  'start_pos': 98},\n"," {'answer': 'sparse and dense',\n","  'context': 'Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.',\n","  'question': ' What are the two classes of vector semantic models?',\n","  'start_pos': 46},\n"," {'answer': 'co-occurrence counts',\n","  'context': 'Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.',\n","  'question': ' What are cells in sparse models functions of?',\n","  'start_pos': 165},\n"," {'answer': 'term',\n","  'context': 'Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.',\n","  'question': ' The term-document matrix has a row for each word in the vocabulary and a column for each document.',\n","  'start_pos': 237},\n"," {'answer': 'Two',\n","  'context': 'Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.',\n","  'question': ' How many sparse weightings are common?',\n","  'start_pos': 435},\n"," {'answer': 'tf-idf',\n","  'context': 'Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.',\n","  'question': ' What weighting weights each cell by its term frequency?',\n","  'start_pos': 473},\n"," {'answer': 'tf-idf',\n","  'context': 'Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.',\n","  'question': ' What weighting weights each cell by its term frequency and inverse document frequency?',\n","  'start_pos': 473},\n"," {'answer': 'PPMI (pointwise positive mutual information',\n","  'context': 'Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.',\n","  'question': ' What is most common for for word-context matrices?',\n","  'start_pos': 572},\n"," {'answer': '50-1000',\n","  'context': \"Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.\",\n","  'question': ' What is the dimensionality of dense vector models?',\n","  'start_pos': 40},\n"," {'answer': 'Word2vec algorithms like skip-gram',\n","  'context': \"Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.\",\n","  'question': ' What is a popular way to compute dense embeddings?',\n","  'start_pos': 49},\n"," {'answer': \"Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words\",\n","  'context': \"Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.\",\n","  'question': \" Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'?\",\n","  'start_pos': 131},\n"," {'answer': \"probability that two words are 'likely to occur nearby in text'. This probability\",\n","  'context': \"Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.\",\n","  'question': ' What is computed from the dot product between the embeddings for two words?',\n","  'start_pos': 196},\n"," {'answer': 'stochastic gradient descent',\n","  'context': 'Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.',\n","  'question': ' What does Skip-gram use to train the classifier?',\n","  'start_pos': 15},\n"," {'answer': 'embeddings',\n","  'context': 'Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.',\n","  'question': ' What is a high dot product with embeddings of words that occur nearby?',\n","  'start_pos': 80},\n"," {'answer': 'word co-occurrence',\n","  'context': 'Other important embedding algorithms include GloVe, a method based on ratios of word co-occurrence probabilities.',\n","  'question': ' GloVe is a method based on what kind of probabilities?',\n","  'start_pos': 80},\n"," {'answer': 'some function of the dot product between vectors',\n","  'context': 'Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.',\n","  'question': ' What are word and document similarities computed by?',\n","  'start_pos': 86},\n"," {'answer': 'The cosine of two vectors',\n","  'context': 'Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.',\n","  'question': ' What is the most popular such metric?',\n","  'start_pos': 136},\n"," {'answer': 'neural units',\n","  'context': 'Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device.',\n","  'question': ' What are neural networks made out of?',\n","  'start_pos': 33},\n"," {'answer': 'human neurons',\n","  'context': 'Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device.',\n","  'question': ' What were neural networks originally inspired by?',\n","  'start_pos': 70},\n"," {'answer': 'non-linear',\n","  'context': 'Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit.',\n","  'question': ' Each neural unit multiplies input values by a weight vector, adds a bias, and then applies what type of activation function?',\n","  'start_pos': 93},\n"," {'answer': 'each unit in layer i',\n","  'context': 'In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles.',\n","  'question': ' In a fully connected, feedforward network, what is connected to each unit in layer i + 1?',\n","  'start_pos': 43},\n"," {'answer': 'feedforward network',\n","  'context': 'In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles.',\n","  'question': ' There are no cycles in what network?',\n","  'start_pos': 22},\n"," {'answer': 'representations',\n","  'context': 'The power of neural networks comes from the ability of early layers to learn representations that can be utilized by later layers in the network.',\n","  'question': ' The power of neural networks comes from the ability of early layers to learn what?',\n","  'start_pos': 77},\n"," {'answer': 'optimization algorithms like gradient descent',\n","  'context': 'Neural networks are trained by optimization algorithms like gradient descent.',\n","  'question': ' What are neural networks trained by?',\n","  'start_pos': 31},\n"," {'answer': 'backward differentiation on a computation graph',\n","  'context': 'Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.',\n","  'question': ' What is error backpropagation?',\n","  'start_pos': 23},\n"," {'answer': 'compute the gradients of the loss function for a network',\n","  'context': 'Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.',\n","  'question': ' What is backward differentiation on a computation graph used for?',\n","  'start_pos': 83},\n"," {'answer': 'a neural network',\n","  'context': 'Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words.',\n","  'question': ' What do neural language models use as a probabilistic classifier?',\n","  'start_pos': 27},\n"," {'answer': 'the probability of the next word given the previous n words',\n","  'context': 'Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words.',\n","  'question': ' What does a neural network compute?',\n","  'start_pos': 86},\n"," {'answer': 'Neural language models',\n","  'context': 'Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.',\n","  'question': ' Pretrained embeddings can be used by what type of model?',\n","  'start_pos': 0},\n"," {'answer': 'embeddings',\n","  'context': 'Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.',\n","  'question': ' Neural language models can learn what from scratch?',\n","  'start_pos': 67},\n"," {'answer': 'parts of speech and named entities',\n","  'context': 'This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:',\n","  'question': ' What did this chapter introduce?',\n","  'start_pos': 24},\n"," {'answer': 'partof-speech tagging and named entity recognition',\n","  'context': 'This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:',\n","  'question': ' What tasks were introduced in this chapter?',\n","  'start_pos': 77},\n"," {'answer': 'between 40 and 200',\n","  'context': 'Languages generally have a small set of closed class words that are highly frequent, ambiguous, and act as function words, and open-class words like nouns, verbs, adjectives. Various part-of-speech tagsets exist, of between 40 and 200 tags.',\n","  'question': ' How many part-of-speech tagsets exist?',\n","  'start_pos': 216},\n"," {'answer': 'Part-of-speech tagging',\n","  'context': 'Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words.',\n","  'question': ' What is the process of assigning a part-of-speech label to a sequence of words?',\n","  'start_pos': 0},\n"," {'answer': 'Named entities',\n","  'context': \"Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.\",\n","  'question': ' What are words for proper nouns?',\n","  'start_pos': 0},\n"," {'answer': 'organizations',\n","  'context': \"Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.\",\n","  'question': ' Named entities refer to people, places, and what else?',\n","  'start_pos': 82},\n"," {'answer': 'generative approach, HMM tagging, and a discriminative approach, CRF tagging',\n","  'context': 'Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.',\n","  'question': ' What are two common approaches to sequence modeling?',\n","  'start_pos': 49},\n"," {'answer': 'HMM tagging',\n","  'context': 'Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.',\n","  'question': ' What is a generative approach called?',\n","  'start_pos': 70},\n"," {'answer': 'discriminative',\n","  'context': 'Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.',\n","  'question': ' How is CRF tagging defined?',\n","  'start_pos': 89},\n"," {'answer': 'maximum likelihood estimation on tag-labeled training corpora',\n","  'context': 'The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence',\n","  'question': ' How are the probabilities in HMM taggers estimated?',\n","  'start_pos': 50},\n"," {'answer': 'Viterbi',\n","  'context': 'The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence',\n","  'question': ' What algorithm is used for decoding, finding the most likely tag sequence?',\n","  'start_pos': 117},\n"," {'answer': 'CRF taggers',\n","  'context': 'Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.',\n","  'question': ' What is another name for Conditional Random Fields?',\n","  'start_pos': 29},\n"," {'answer': 'Conditional Random Fields',\n","  'context': 'Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.',\n","  'question': ' What do CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence?',\n","  'start_pos': 0},\n"," {'answer': 'inference',\n","  'context': 'Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.',\n","  'question': ' What is the Viterbi algorithm used for?',\n","  'start_pos': 297},\n"," {'answer': 'Viterbi algorithm',\n","  'context': 'Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.',\n","  'question': ' What is another name for the Forward-Backward algorithm?',\n","  'start_pos': 275},\n"," {'answer': 'language problems',\n","  'context': 'This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:',\n","  'question': ' What are recurrent neural networks and transformers?',\n","  'start_pos': 118},\n"," {'answer': 'language problems',\n","  'context': 'This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:',\n","  'question': ' What are transformers used for?',\n","  'start_pos': 118},\n"," {'answer': 'one element at a time',\n","  'context': 'In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1.',\n","  'question': ' How are sequences processed in Recurrent Neural Networks?',\n","  'start_pos': 60},\n"," {'answer': 'the current input at t and the hidden layer from time t − 1',\n","  'context': 'In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1.',\n","  'question': ' What is the output of each neural unit at time t based on?',\n","  'start_pos': 143},\n"," {'answer': 'with a straightforward extension of the backpropagation algorithm',\n","  'context': 'RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).',\n","  'question': ' How can RNNs be trained?',\n","  'start_pos': 20},\n"," {'answer': 'BPTT',\n","  'context': 'RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).',\n","  'question': ' What is backpropagation through time?',\n","  'start_pos': 126},\n"," {'answer': 'problems like vanishing gradients',\n","  'context': 'Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers.',\n","  'question': ' Why do simple recurrent networks fail on long inputs?',\n","  'start_pos': 57},\n"," {'answer': 'LSTMs',\n","  'context': 'Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers.',\n","  'question': ' Modern systems use more complex gated architectures like what?',\n","  'start_pos': 160},\n"," {'answer': 'complex gated architectures',\n","  'context': 'Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers.',\n","  'question': ' LSTMs explicitly decide what to remember and forget in their hidden and context layers?',\n","  'start_pos': 124},\n"," {'answer': 'self-attention',\n","  'context': 'Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word.',\n","  'question': ' Transformers are non-recurrent networks based on what?',\n","  'start_pos': 49},\n"," {'answer': 'attention heads that each model how the surrounding words are relevant for the processing of the current word',\n","  'context': 'Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word.',\n","  'question': ' A selfattention layer maps input sequences to output sequences of the same length?',\n","  'start_pos': 166},\n"," {'answer': 'a single attention layer',\n","  'context': 'A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks.',\n","  'question': ' What is a transformer block composed of?',\n","  'start_pos': 32},\n"," {'answer': 'Transformer blocks',\n","  'context': 'A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks.',\n","  'question': ' What can be stacked to make deeper and more powerful networks?',\n","  'start_pos': 156},\n"," {'answer': ':',\n","  'context': 'Common language-based applications for RNNs and transformers include:',\n","  'question': ' What are some common language-based applications for RNNs and transformers?',\n","  'start_pos': 68},\n"," {'answer': 'assigning a probability to a sequence, or to the next element of a sequence given the preceding words',\n","  'context': 'Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.',\n","  'question': ' What is Probabilistic language modeling?',\n","  'start_pos': 33},\n"," {'answer': 'a trained language model',\n","  'context': 'Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.',\n","  'question': ' What is automatic regressive generation using?',\n","  'start_pos': 170},\n"," {'answer': 'each element of a sequence',\n","  'context': 'Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.',\n","  'question': ' What is assigned a label?',\n","  'start_pos': 250},\n"," {'answer': 'Sequence',\n","  'context': 'Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.',\n","  'question': ' What classification is used when an entire text is assigned to a category?',\n","  'start_pos': 299},\n"," {'answer': 'Machine translation',\n","  'context': 'Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.',\n","  'question': ' What is one of the most widely used applications of NLP?',\n","  'start_pos': 0},\n"," {'answer': 'MT',\n","  'context': 'Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.',\n","  'question': ' The encoder-decoder model was first developed for what?',\n","  'start_pos': 123},\n"," {'answer': 'lexical',\n","  'context': 'Languages have divergences, both structural and lexical, that make translation difficult.',\n","  'question': ' Languages have divergences, both structural and what else?',\n","  'start_pos': 48},\n"," {'answer': 'linguistic field of typology',\n","  'context': 'The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects.',\n","  'question': ' What field investigates some of these differences?',\n","  'start_pos': 4},\n"," {'answer': 'their objects',\n","  'context': 'The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects.',\n","  'question': ' Languages can be classified by their position along typological dimensions like whether verbs precede what?',\n","  'start_pos': 175},\n"," {'answer': 'an encoder network',\n","  'context': 'Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.',\n","  'question': ' What are encoder-decoder networks composed of?',\n","  'start_pos': 75},\n"," {'answer': 'Encoder-decoder networks',\n","  'context': 'Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.',\n","  'question': ' What is an encoder network that takes an input sequence and creates a contextualized representation of it?',\n","  'start_pos': 0},\n"," {'answer': 'a decoder',\n","  'context': 'Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.',\n","  'question': ' The context representation is then passed to what?',\n","  'start_pos': 233},\n"," {'answer': 'attention mechanism in RNNs',\n","  'context': 'The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder.',\n","  'question': ' What allows the decoder to view information from all the hidden states of the encoder?',\n","  'start_pos': 4},\n"," {'answer': 'choosing the single most probable token to generate at each step',\n","  'context': 'For the decoder, choosing the single most probable token to generate at each step is called greedy decoding.',\n","  'question': ' What is greedy decoding for a decoder?',\n","  'start_pos': 17},\n"," {'answer': 'beam width',\n","  'context': 'In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width.',\n","  'question': ' What is the name of the fixed-size memory footprint?',\n","  'start_pos': 170},\n"," {'answer': 'a parallel corpus',\n","  'context': 'Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages.',\n","  'question': ' Machine translation models are trained on what?',\n","  'start_pos': 42},\n"," {'answer': 'two',\n","  'context': 'Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages.',\n","  'question': ' A bitext is a text that appears in how many languages?',\n","  'start_pos': 111},\n"," {'answer': 'a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts',\n","  'context': 'Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.',\n","  'question': ' What is backtranslation?',\n","  'start_pos': 19},\n"," {'answer': 'Backtranslation',\n","  'context': 'Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.',\n","  'question': ' What is a way of making use of monolingual corpora in the target language?',\n","  'start_pos': 0},\n"," {'answer': 'Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards',\n","  'context': 'Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.',\n","  'question': ' How does a pilot MT engine create synthetic bitexts?',\n","  'start_pos': 0},\n"," {'answer': \"a translation's adequacy (how well it captures the meaning of the source sentence) and fluency\",\n","  'context': \"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.\",\n","  'question': ' What is MT evaluated by measuring?',\n","  'start_pos': 29},\n"," {'answer': 'Human evaluation',\n","  'context': \"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.\",\n","  'question': ' What is the gold standard?',\n","  'start_pos': 178},\n"," {'answer': 'character n-gram overlap with human translations',\n","  'context': \"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.\",\n","  'question': ' Automatic evaluation metrics measure what?',\n","  'start_pos': 275},\n"," {'answer': 'chrF',\n","  'context': \"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.\",\n","  'question': ' What measure character n-gram overlap with human translations?',\n","  'start_pos': 255},\n"," {'answer': 'chrF',\n","  'context': \"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.\",\n","  'question': ' What measure recent metrics based on embedding similarity?',\n","  'start_pos': 255},\n"," {'answer': 'The regular expression',\n","  'context': 'The regular expression is a fundamental tool in language processing, and this chapter showed how to perform basic text normalization tasks.',\n","  'question': ' What is a fundamental tool in language processing?',\n","  'start_pos': 0},\n"," {'answer': 'basic text normalization tasks',\n","  'context': 'The regular expression is a fundamental tool in language processing, and this chapter showed how to perform basic text normalization tasks.',\n","  'question': ' What did this chapter show how to perform?',\n","  'start_pos': 108},\n"," {'answer': 'minimum edit distance',\n","  'context': 'The minimum edit distance was introduced.',\n","  'question': ' What is the minimum edit distance?',\n","  'start_pos': 4},\n"," {'answer': 'Here',\n","  'context': 'Here is a summary of the main points we covered.',\n","  'question': ' What is a summary of the main points we covered?',\n","  'start_pos': 0},\n"," {'answer': 'regular expression',\n","  'context': 'The regular expression language can be used for pattern matching.',\n","  'question': ' What language can be used for pattern matching?',\n","  'start_pos': 4},\n"," {'answer': 'concatenation of symbols and disjunction of symbols',\n","  'context': 'Basic operations in regular expressions include concatenation of symbols and disjunction of symbols.',\n","  'question': ' What are two basic operations in regular expressions?',\n","  'start_pos': 48},\n"," {'answer': 'Basic operations in regular expressions',\n","  'context': 'Basic operations in regular expressions include concatenation of symbols and disjunction of symbols.',\n","  'question': ' Concatenation of symbols and disjunction of symbols are two examples of what?',\n","  'start_pos': 0},\n"," {'answer': 'Word tokenization and normalization',\n","  'context': 'Word tokenization and normalization can be done by cascades of simple regular expression substitution.',\n","  'question': ' What can be done by cascades of simple regular expression substitution?',\n","  'start_pos': 0},\n"," {'answer': 'Porter algorithm',\n","  'context': 'The Porter algorithm can be used to stemming, stripping off affixes.',\n","  'question': ' What algorithm can be used to stemming, stripping off affixes?',\n","  'start_pos': 4},\n"," {'answer': 'high',\n","  'context': \"It doesn't have high accuracy, but it is useful for some tasks.\",\n","  'question': ' What kind of accuracy does this tool not have?',\n","  'start_pos': 16},\n"," {'answer': 'minimum number',\n","  'context': 'The minimum number of operations it takes to edit one into the other is the minimum edit distance.',\n","  'question': ' How many operations does it take to edit one into the other?',\n","  'start_pos': 4},\n"," {'answer': 'The minimum number of operations',\n","  'context': 'The minimum number of operations it takes to edit one into the other is the minimum edit distance.',\n","  'question': ' What is the minimum edit distance?',\n","  'start_pos': 0},\n"," {'answer': 'Dynamic programming',\n","  'context': 'Dynamic programming can be used to calculate the minimum edit distance.',\n","  'question': ' What can be used to calculate the minimum edit distance?',\n","  'start_pos': 0},\n"," {'answer': 'this chapter',\n","  'context': 'Language modeling and the n-gram were introduced in this chapter.',\n","  'question': ' In what chapter were language modeling and the n-gram introduced?',\n","  'start_pos': 52},\n"," {'answer': 'a probability',\n","  'context': 'Language models can be used to assign a probability to a sentence or a sequence of words.',\n","  'question': ' Language models can be used to assign what to a sentence or a sequence of words?',\n","  'start_pos': 38},\n"," {'answer': 'n-grams',\n","  'context': 'n-grams are models that estimate words from a fixed window.',\n","  'question': ' What are models that estimate words from a fixed window?',\n","  'start_pos': 0},\n"," {'answer': 'n',\n","  'context': 'n',\n","  'question': ' What is the name of the neophyte?',\n","  'start_pos': 0},\n"," {'answer': 'n',\n","  'context': 'n',\n","  'question': ' What does naivete mean?',\n","  'start_pos': 0},\n"," {'answer': 'Gram probabilities',\n","  'context': 'Gram probabilities can be estimated by counting and normalizing.',\n","  'question': ' What can be estimated by counting and normalizing?',\n","  'start_pos': 0},\n"," {'answer': 'perplexity',\n","  'context': 'n-gram language models are evaluated using perplexity.',\n","  'question': ' What is used to evaluate n-gram language models?',\n","  'start_pos': 43},\n"," {'answer': 'the perplexity of a test set',\n","  'context': 'The geometric mean of the inverse test set probability computed by the model is the perplexity of a test set.',\n","  'question': ' What is the geometric mean of the inverse test set probability computed by the model?',\n","  'start_pos': 80},\n"," {'answer': 'There is a more sophisticated way to estimate the probability of n-grams.',\n","  'context': 'There is a more sophisticated way to estimate the probability of n-grams.',\n","  'question': ' What is a more sophisticated way to estimate the probability of n-grams?',\n","  'start_pos': 0},\n"," {'answer': 'smoothing',\n","  'context': 'Lower-order n-gram counts are used for smoothing.',\n","  'question': ' Lower-order n-gram counts are used for what?',\n","  'start_pos': 39},\n"," {'answer': 'discounting',\n","  'context': 'discounting is required to create a probability distribution.',\n","  'question': ' What is required to create a probability distribution?',\n","  'start_pos': 0},\n"," {'answer': 'probability of a word being a novel continuation is used.',\n","  'context': 'The probability of a word being a novel continuation is used.',\n","  'question': ' What is the probability of a word being a novel continuation used for?',\n","  'start_pos': 4},\n"," {'answer': 'A discounted probability',\n","  'context': 'A discounted probability is mixed with a lower-order continuation probability.',\n","  'question': ' What is mixed with a lower-order continuation probability?',\n","  'start_pos': 0},\n"," {'answer': 'naive Bayes model',\n","  'context': 'The naive Bayes model was applied to the text categorization task of sentiment analysis.',\n","  'question': ' What was applied to the text categorization task of sentiment analysis?',\n","  'start_pos': 4},\n"," {'answer': 'text categorization task of sentiment analysis',\n","  'context': 'The naive Bayes model was applied to the text categorization task of sentiment analysis.',\n","  'question': ' What was the naive Bayes model?',\n","  'start_pos': 41},\n"," {'answer': 'classification',\n","  'context': 'Language processing tasks can be seen as tasks of classification.',\n","  'question': ' Language processing tasks can be seen as tasks of what?',\n","  'start_pos': 50},\n"," {'answer': 'sentiment analysis',\n","  'context': 'Text categorization, in which an entire text is assigned a class from a finite set, includes tasks such as sentiment analysis.',\n","  'question': ' Text categorization assigns an entire text a class from a finite set of what?',\n","  'start_pos': 107},\n"," {'answer': 'Sentiment',\n","  'context': 'Sentiment analysis considers a text to be a reflection of the positive or negative orientation of the writer.',\n","  'question': ' What type of analysis considers a text to reflect the positive or negative orientation of the writer?',\n","  'start_pos': 0},\n"," {'answer': 'generative',\n","  'context': 'The bag of words assumption and the conditional independence assumption are made by a generative model.',\n","  'question': ' The bag of words assumption and the conditional independence assumption are made by what model?',\n","  'start_pos': 86},\n"," {'answer': 'text classification tasks',\n","  'context': 'Bayes with binarized features seem to work better for text classification tasks.',\n","  'question': ' Bayes with binarized features seem to work better for what?',\n","  'start_pos': 54},\n"," {'answer': 'recall and precision',\n","  'context': 'Classifiers are evaluated on their recall and precision.',\n","  'question': ' What are classifiers evaluated on?',\n","  'start_pos': 35},\n"," {'answer': 'their recall and precision',\n","  'context': 'Classifiers are evaluated on their recall and precision.',\n","  'question': ' Classifiers are evaluated on what?',\n","  'start_pos': 29},\n"," {'answer': 'The use of cross-validation',\n","  'context': 'The use of cross-validation in the training set is included in the training set.',\n","  'question': ' What is included in the training set?',\n","  'start_pos': 0},\n"," {'answer': 'a statistical significance test',\n","  'context': \"If we can't be sure that one version is better than the other, we should use a statistical significance test.\",\n","  'question': \" What should we use if we can't be sure that one version is better than the other?\",\n","  'start_pos': 77},\n"," {'answer': 'a model card',\n","  'context': 'The harms that may be caused by the model, including its training data and other components, should be reported in a model card.',\n","  'question': ' What should harms that may be caused by a model be reported in?',\n","  'start_pos': 115},\n"," {'answer': 'model of classification',\n","  'context': 'The model of classification was introduced in this chapter.',\n","  'question': ' What model of classification was introduced in this chapter?',\n","  'start_pos': 4},\n"," {'answer': 'Logistic regression',\n","  'context': 'Logistic regression is a supervised machine learning classification that extracts real-valued features from the input, sums them, and passes the sum through a sigmoid function to generate a probability.',\n","  'question': ' What is supervised machine learning classification?',\n","  'start_pos': 0},\n"," {'answer': 'real-valued features',\n","  'context': 'Logistic regression is a supervised machine learning classification that extracts real-valued features from the input, sums them, and passes the sum through a sigmoid function to generate a probability.',\n","  'question': ' What does logistic regression extract from input?',\n","  'start_pos': 82},\n"," {'answer': 'A threshold',\n","  'context': 'A threshold is used to make a decision.',\n","  'question': ' What is used to make a decision?',\n","  'start_pos': 0},\n"," {'answer': 'Logistic regression',\n","  'context': 'Logistic regression can be used with more than one class, for example for text classification and part-of-speech labeling.',\n","  'question': ' What can be used with more than one class?',\n","  'start_pos': 0},\n"," {'answer': 'text classification and part-of-speech labeling',\n","  'context': 'Logistic regression can be used with more than one class, for example for text classification and part-of-speech labeling.',\n","  'question': ' For example, what can logistic regression be used for?',\n","  'start_pos': 74},\n"," {'answer': 'softmax',\n","  'context': 'The softmax function is used to compute probabilities.',\n","  'question': ' What function is used to compute probabilities?',\n","  'start_pos': 4},\n"," {'answer': 'weights are learned from a loss function that must be minimized.',\n","  'context': 'The weights are learned from a loss function that must be minimized.',\n","  'question': ' The weights are learned from a loss function that must be minimized.',\n","  'start_pos': 4},\n"," {'answer': 'Minimizing the loss function',\n","  'context': 'Minimizing the loss function is a problem that can be solved using iterative techniques.',\n","  'question': ' What is a problem that can be solved using iterative techniques?',\n","  'start_pos': 0},\n"," {'answer': 'Regularization is used to avoid overfitting',\n","  'context': 'Regularization is used to avoid overfitting.',\n","  'question': ' Is regularization used to avoid overfitting?',\n","  'start_pos': 0},\n"," {'answer': 'Logistic regression',\n","  'context': 'Logistic regression is one of the most useful analytic tools because of its ability to study the importance of individual features.',\n","  'question': ' What is one of the most useful analytic tools?',\n","  'start_pos': 0},\n"," {'answer': 'individual features',\n","  'context': 'Logistic regression is one of the most useful analytic tools because of its ability to study the importance of individual features.',\n","  'question': ' Logistic regression studies the importance of what?',\n","  'start_pos': 111},\n"," {'answer': 'A word',\n","  'context': 'A word is modeled as a point in high-dimensional space.',\n","  'question': ' What is modeled as a point in high-dimensional space?',\n","  'start_pos': 0},\n"," {'answer': 'Each word',\n","  'context': 'Each word is mapped to a fixed embedded in the chapter.',\n","  'question': ' What is mapped to a fixed embedded in the chapter?',\n","  'start_pos': 0},\n"," {'answer': 'sparse and dense',\n","  'context': 'There are two classes of models: sparse and dense.',\n","  'question': ' What are the two classes of models?',\n","  'start_pos': 33},\n"," {'answer': 'co-occurrence counts',\n","  'context': 'Cells are functions of co-occurrence counts and each corresponds to a word in the vocabulary.',\n","  'question': ' Cells are functions of what?',\n","  'start_pos': 23},\n"," {'answer': 'Cells',\n","  'context': 'Cells are functions of co-occurrence counts and each corresponds to a word in the vocabulary.',\n","  'question': ' Each cell corresponds to a word in the vocabulary?',\n","  'start_pos': 0},\n"," {'answer': 'a row',\n","  'context': 'The matrix has a row for each word in the vocabulary and a column for each document.',\n","  'question': ' What does the matrix have for each word in the vocabulary?',\n","  'start_pos': 15},\n"," {'answer': 'matrix',\n","  'context': 'The matrix has a row for each word in the vocabulary and a column for each document.',\n","  'question': ' What is the column for each document?',\n","  'start_pos': 4},\n"," {'answer': 'term-term',\n","  'context': 'The word-context or term-term matrix has a row for each target word in the vocabulary and a column for each context term in the vocabulary.',\n","  'question': ' What is another name for the word-context matrix?',\n","  'start_pos': 20},\n"," {'answer': 'The word-context or term-term matrix',\n","  'context': 'The word-context or term-term matrix has a row for each target word in the vocabulary and a column for each context term in the vocabulary.',\n","  'question': ' What is the row for each target word in the vocabulary?',\n","  'start_pos': 0},\n"," {'answer': 'vocabulary',\n","  'context': 'The word-context or term-term matrix has a row for each target word in the vocabulary and a column for each context term in the vocabulary.',\n","  'question': ' The column for each context term is what?',\n","  'start_pos': 128},\n"," {'answer': 'tf-idf and PPMI',\n","  'context': 'The tf-idf and PPMI are the two most common sparse weights for word-context matrices.',\n","  'question': ' What are the two most common sparse weights for word-context matrices?',\n","  'start_pos': 4},\n"," {'answer': '50-1000',\n","  'context': 'The models have dimensions of 50-1000.',\n","  'question': ' What are the dimensions of the models?',\n","  'start_pos': 30},\n"," {'answer': 'Word2vec',\n","  'context': 'Word2vec is a popular way to compute dense embeddeds.',\n","  'question': ' What is a popular way to compute dense embeddeds?',\n","  'start_pos': 0},\n"," {'answer': 'There',\n","  'context': 'There is a chance that two words are likely to occur nearby in text.',\n","  'question': ' What is the chance that two words are likely to appear near a text?',\n","  'start_pos': 0},\n"," {'answer': 'dot product',\n","  'context': 'The dot product is used to calculate the probability.',\n","  'question': ' What product is used to calculate the probability?',\n","  'start_pos': 4},\n"," {'answer': 'stochastic gradient descent',\n","  'context': 'Skip-gram uses a method called stochastic gradient descent to train the classifier by learning embedded words that have a high dot product and a low dot product.',\n","  'question': ' What method does Skip-gram use to train the classifier?',\n","  'start_pos': 31},\n"," {'answer': 'stochastic gradient descent',\n","  'context': 'Skip-gram uses a method called stochastic gradient descent to train the classifier by learning embedded words that have a high dot product and a low dot product.',\n","  'question': ' What is the method called that skip-gram uses to train a classifier by learning embedded words that have a high and low dot product?',\n","  'start_pos': 31},\n"," {'answer': 'GloVe',\n","  'context': 'The method based on word co-occurrence probabilities is called GloVe.',\n","  'question': ' What is the name of the method based on word co-occurrence probabilities?',\n","  'start_pos': 63},\n"," {'answer': 'The method based on word co-occurrence probabilities',\n","  'context': 'The method based on word co-occurrence probabilities is called GloVe.',\n","  'question': ' What is GloVe?',\n","  'start_pos': 0},\n"," {'answer': 'the function of the dot product between the vectors',\n","  'context': 'Word and document similarities are determined by the function of the dot product between the vectors.',\n","  'question': ' What determines the similarities between Word and document?',\n","  'start_pos': 49},\n"," {'answer': 'normalized dot product',\n","  'context': 'The normalized dot product is the most popular metric.',\n","  'question': ' What is the most popular metric?',\n","  'start_pos': 4},\n"," {'answer': 'human neurons',\n","  'context': 'Neural networks were originally inspired by human neurons but are now just an abstract computational device.',\n","  'question': ' What were neural networks originally inspired by?',\n","  'start_pos': 44},\n"," {'answer': 'Neural networks were originally inspired by human neurons',\n","  'context': 'Neural networks were originally inspired by human neurons but are now just an abstract computational device.',\n","  'question': ' What are neural networks now just an abstract computational device?',\n","  'start_pos': 0},\n"," {'answer': 'sigmoid, tanh, or rectified linear unit',\n","  'context': 'Neural units add a bias and apply a non- linear activation function like sigmoid, tanh, or rectified linear unit.',\n","  'question': ' Neural units add a bias and apply a non- linear activation function like what?',\n","  'start_pos': 73},\n"," {'answer': 'each unit in layer i + 1',\n","  'context': 'Each unit in layer i is connected to each unit in layer i + 1 and there are no cycles.',\n","  'question': ' What is the name of the unit in layer i connected to?',\n","  'start_pos': 37},\n"," {'answer': 'there are no cycles',\n","  'context': 'Each unit in layer i is connected to each unit in layer i + 1 and there are no cycles.',\n","  'question': ' What happens when each unit is connected to each unit?',\n","  'start_pos': 66},\n"," {'answer': 'Neural networks',\n","  'context': 'Neural networks are powered by the ability of early layers to learn representations that can be used by later layers.',\n","  'question': ' What is powered by the ability of early layers to learn representations that can be used by later layers?',\n","  'start_pos': 0},\n"," {'answer': 'Neural networks are trained.',\n","  'context': 'Neural networks are trained.',\n","  'question': ' How are neural networks trained?',\n","  'start_pos': 0},\n"," {'answer': 'error backpropagation',\n","  'context': 'The error backpropagation is used to compute the loss function for a network.',\n","  'question': ' What is used to compute the loss function for a network?',\n","  'start_pos': 4},\n"," {'answer': 'a neural network',\n","  'context': 'Neural language models use a neural network to estimate the probability of the next word.',\n","  'question': ' What do neural language models use to estimate the probability of the next word?',\n","  'start_pos': 27},\n"," {'answer': 'pretrained',\n","  'context': 'Neural language models can learn embeddeds from scratch or pretrained them.',\n","  'question': ' Neural language models can learn embeddeds from scratch or what?',\n","  'start_pos': 59},\n"," {'answer': 'Part of speech and named entities',\n","  'context': 'Part of speech and named entities were introduced in this chapter.',\n","  'question': ' What were introduced in this chapter?',\n","  'start_pos': 0},\n"," {'answer': 'Part of speech',\n","  'context': 'Part of speech and named entities were introduced in this chapter.',\n","  'question': ' What was introduced in the chapter that dealt with speech and named entities?',\n","  'start_pos': 0},\n"," {'answer': 'small',\n","  'context': 'There is a small set of closed class words that are highly frequent, ambiguous, and act as function words.',\n","  'question': ' How many closed class words are there?',\n","  'start_pos': 11},\n"," {'answer': 'function words',\n","  'context': 'There is a small set of closed class words that are highly frequent, ambiguous, and act as function words.',\n","  'question': ' What do closed class terms act as?',\n","  'start_pos': 91},\n"," {'answer': 'between 40 and 200',\n","  'context': 'There are between 40 and 200 part-of-speech tags.',\n","  'question': ' How many part-of-speech tags are there?',\n","  'start_pos': 10},\n"," {'answer': 'A part-of-speech label',\n","  'context': 'A part-of-speech label is assigned to a sequence of words.',\n","  'question': ' What is assigned to a sequence of words?',\n","  'start_pos': 0},\n"," {'answer': 'not strictly entities or even proper nouns',\n","  'context': 'Many other types of named entities are not strictly entities or even proper nouns.',\n","  'question': ' What are many other types of named entities?',\n","  'start_pos': 39},\n"," {'answer': 'generative and discriminative',\n","  'context': 'Two approaches to sequence modeling are generative and discriminative.',\n","  'question': ' What are the two approaches to sequence modeling?',\n","  'start_pos': 40},\n"," {'answer': 'chapters will show a neural approach.',\n","  'context': 'The following chapters will show a neural approach.',\n","  'question': ' The following chapters will show a neural approach.',\n","  'start_pos': 14},\n"," {'answer': 'the probabilities in HMM taggers',\n","  'context': 'The maximum likelihood estimation on tag-labeled training corpora estimates the probabilities in HMM taggers.',\n","  'question': ' What does the maximum likelihood estimation estimate on tag-labeled training corpora?',\n","  'start_pos': 76},\n"," {'answer': 'Viterbi',\n","  'context': 'The most likely tag sequence is found using the Viterbi algorithm.',\n","  'question': ' What algorithm is used to find the most likely tag sequence?',\n","  'start_pos': 48},\n"," {'answer': 'The best tag sequence',\n","  'context': 'The best tag sequence can be chosen based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep.',\n","  'question': ' What can be chosen based on features that condition on the output tag?',\n","  'start_pos': 0},\n"," {'answer': 'output tag, the prior output tag, the entire input sequence, and the current timestep',\n","  'context': 'The best tag sequence can be chosen based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep.',\n","  'question': ' What is the best tag sequence?',\n","  'start_pos': 76},\n"," {'answer': 'The best sequence of tags and a version of the Forward- Backward algorithm',\n","  'context': 'The best sequence of tags and a version of the Forward- Backward algorithm are used for training.',\n","  'question': ' What is used for training?',\n","  'start_pos': 0},\n"," {'answer': 'Forward- Backward',\n","  'context': 'The best sequence of tags and a version of the Forward- Backward algorithm are used for training.',\n","  'question': ' What algorithm is used?',\n","  'start_pos': 47},\n"," {'answer': 'this chapter',\n","  'context': 'The concepts of recurrent neural networks and transformers have been introduced in this chapter.',\n","  'question': ' The concepts of recurrent neural networks and transformers have been introduced in what chapter?',\n","  'start_pos': 83},\n"," {'answer': 'Here',\n","  'context': 'Here is a summary of the main points we covered.',\n","  'question': ' What is a summary of the main points we covered?',\n","  'start_pos': 0},\n"," {'answer': 't',\n","  'context': 'The output of each neural unit is based on the current input at t and the hidden layer from time t  1.',\n","  'question': ' The output of each neural unit is based on the current input at what?',\n","  'start_pos': 64},\n"," {'answer': 'neural unit',\n","  'context': 'The output of each neural unit is based on the current input at t and the hidden layer from time t  1.',\n","  'question': ' What is the hidden layer from time t 1?',\n","  'start_pos': 19},\n"," {'answer': 'Backpropagation through time',\n","  'context': 'Backpropagation through time is a simple extension of the backpropagation algorithm.',\n","  'question': ' What is a simple extension of the backpropagation algorithm?',\n","  'start_pos': 0},\n"," {'answer': 'simple recurrent networks fail',\n","  'context': 'Modern systems use more complex gated architectures that explicitly decide what to remember and forget in their hidden and context layers, which is why simple recurrent networks fail.',\n","  'question': ' Modern systems use more complex gated architectures that explicitly decide what to remember and forget in their hidden and context layers?',\n","  'start_pos': 152},\n"," {'answer': 'gated architectures that explicitly decide what to remember and forget in their hidden and context layers',\n","  'context': 'Modern systems use more complex gated architectures that explicitly decide what to remember and forget in their hidden and context layers, which is why simple recurrent networks fail.',\n","  'question': ' Simple recurrent networks fail because of what?',\n","  'start_pos': 32},\n"," {'answer': 'transformer networks',\n","  'context': 'Non-recurrent networks are called transformer networks.',\n","  'question': ' What are non-recurrent networks called?',\n","  'start_pos': 34},\n"," {'answer': 'based on a set of attention heads',\n","  'context': 'A selfattention layer maps input sequence to output sequence of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word',\n","  'question': ' How does a selfattention layer map input sequence to output sequence of the same length?',\n","  'start_pos': 81},\n"," {'answer': 'selfattention layer maps input sequence to output sequence of the same length, based on a set of attention heads',\n","  'context': 'A selfattention layer maps input sequence to output sequence of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word',\n","  'question': ' What model how the surrounding words are relevant for the processing of the current word?',\n","  'start_pos': 2},\n"," {'answer': 'residual connections',\n","  'context': 'A transformer block consists of a single attention layer followed by a feed forward layer with residual connections.',\n","  'question': ' A transformer block consists of a single attention layer followed by a feed forward layer with what?',\n","  'start_pos': 95},\n"," {'answer': 'Transformer blocks',\n","  'context': 'Transformer blocks can be stacked to make more powerful networks.',\n","  'question': ' What can be stacked to make more powerful networks?',\n","  'start_pos': 0},\n"," {'answer': 'language-based',\n","  'context': 'There are common language-based applications for RNNs and transformers.',\n","  'question': ' What kind of applications are there for RNNs and transformers?',\n","  'start_pos': 17},\n"," {'answer': 'probabilistic',\n","  'context': 'Language modeling is probabilistic.',\n","  'question': ' What type of modeling is language modeling?',\n","  'start_pos': 21},\n"," {'answer': 'a probability',\n","  'context': 'The next element of a sequence can be assigned a probability.',\n","  'question': ' What can be assigned to the next element of a sequence?',\n","  'start_pos': 47},\n"," {'answer': 'trained language model',\n","  'context': 'A trained language model is used for auto-regressive generation.',\n","  'question': ' What is used for auto-regressive generation?',\n","  'start_pos': 2},\n"," {'answer': 'Sequence',\n","  'context': 'Sequence.',\n","  'question': ' What is the name of the sequence?',\n","  'start_pos': 0},\n"," {'answer': 'Sequence',\n","  'context': 'Sequence.',\n","  'question': ' What does Sequence do?',\n","  'start_pos': 0},\n"," {'answer': 'a label',\n","  'context': 'Each element of a sequence is assigned a label.',\n","  'question': ' What is assigned to each element of a sequence?',\n","  'start_pos': 39},\n"," {'answer': 'a category',\n","  'context': 'A sequence classification is where an entire text is assigned to a category.',\n","  'question': ' A sequence classification is where an entire text is assigned to what category?',\n","  'start_pos': 65},\n"," {'answer': 'machine translation',\n","  'context': 'One of the most widely used applications of NLP is machine translation, and the encoder-decoder model is a key tool that has applications throughout the system.',\n","  'question': ' What is one of the most widely used applications of NLP?',\n","  'start_pos': 51},\n"," {'answer': 'encoder-decoder model',\n","  'context': 'One of the most widely used applications of NLP is machine translation, and the encoder-decoder model is a key tool that has applications throughout the system.',\n","  'question': ' What is a key tool that has applications throughout the system?',\n","  'start_pos': 80},\n"," {'answer': 'Structural and lexical differences make translation difficult.',\n","  'context': 'Structural and lexical differences make translation difficult.',\n","  'question': ' Structural and lexical differences make translation difficult?',\n","  'start_pos': 0},\n"," {'answer': 'typology',\n","  'context': 'Some of the differences are investigated by the linguistic field of typology.',\n","  'question': ' What field investigates some of the differences?',\n","  'start_pos': 68},\n"," {'answer': 'an Encoder network',\n","  'context': 'A transformer is composed of an Encoder network that takes an input sequence and creates a contextualized representation of it.',\n","  'question': ' What is a transformer composed of?',\n","  'start_pos': 29},\n"," {'answer': 'Encoder network',\n","  'context': 'A transformer is composed of an Encoder network that takes an input sequence and creates a contextualized representation of it.',\n","  'question': ' What network takes an input sequence and creates a contextualized representation of it?',\n","  'start_pos': 32},\n"," {'answer': 'context representation',\n","  'context': 'A task-specific output sequence is generated from this context representation.',\n","  'question': ' A task-specific output sequence is generated from what context representation?',\n","  'start_pos': 55},\n"," {'answer': 'attention mechanism in RNNs and cross-attention in transformers',\n","  'context': 'The attention mechanism in RNNs and cross-attention in transformers allow the decoder to see all the hidden states of the encoder.',\n","  'question': ' What allows the decoder to see all the hidden states of the encoder?',\n","  'start_pos': 4},\n"," {'answer': 'greedy decoding',\n","  'context': 'The single most probable token to generate at each step is called greedy decoding.',\n","  'question': ' What is the most probable token to generate at each step?',\n","  'start_pos': 66},\n"," {'answer': 'The single most probable token to generate at each step',\n","  'context': 'The single most probable token to generate at each step is called greedy decoding.',\n","  'question': ' What is greedy decoding?',\n","  'start_pos': 0},\n"," {'answer': 'we keep possible token at each step',\n","  'context': 'Instead of choosing the best token to generate at each step, we keep possible token at each step.',\n","  'question': ' What do we do instead of choosing the best token to generate at each step?',\n","  'start_pos': 61},\n"," {'answer': 'The beam width',\n","  'context': 'The beam width is the fixed-size memory footprint k.',\n","  'question': ' What is the fixed-size memory footprint k?',\n","  'start_pos': 0},\n"," {'answer': 'bitext is a text that appears in more than one language.',\n","  'context': 'A bitext is a text that appears in more than one language.',\n","  'question': ' A bitext is a text that appears in more than one language?',\n","  'start_pos': 2},\n"," {'answer': 'Backtranslation',\n","  'context': 'Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot engine backwards.',\n","  'question': ' What is a way of making use of monolingual corpora in the target language?',\n","  'start_pos': 0},\n"," {'answer': 'a way of making use of monolingual corpora in the target language by running a pilot engine backwards',\n","  'context': 'Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot engine backwards.',\n","  'question': ' What is backtranslation?',\n","  'start_pos': 19},\n"," {'answer': 'how well it captures the meaning of the source sentence and how natural it is in the target language',\n","  'context': \"The translation's effectiveness is measured by how well it captures the meaning of the source sentence and how natural it is in the target language.\",\n","  'question': \" How is translation's effectiveness measured?\",\n","  'start_pos': 47},\n"," {'answer': 'effectiveness',\n","  'context': \"The translation's effectiveness is measured by how well it captures the meaning of the source sentence and how natural it is in the target language.\",\n","  'question': ' How well does translation capture the meaning of the source sentence and how natural it is in target language?',\n","  'start_pos': 18},\n"," {'answer': 'chrF',\n","  'context': 'Automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, and more recent metrics based on embedded similarity are commonly used.',\n","  'question': ' What measure character n-gram overlap with human translations?',\n","  'start_pos': 34},\n"," {'answer': 'Automatic evaluation metrics',\n","  'context': 'Automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, and more recent metrics based on embedded similarity are commonly used.',\n","  'question': ' What measure recent metrics based on embedded similarity are commonly used?',\n","  'start_pos': 0}]"]},"metadata":{},"execution_count":40}]}]}