question,answer,context,start_pos
 What is a fundamental tool in language processing?,the regular expression,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",67
 What did this chapter show how to perform?,basic text normalization tasks,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",117
 What is comparing strings?,minimum edit distance algorithm,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",265
 What are the main points we covered about these ideas?,a summary,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",327
 • The regular expression language is a powerful tool for what?,pattern-matching,• The regular expression language is a powerful tool for pattern-matching.,57
 What are some basic operations in regular expressions?,"concatenation of symbols, disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ, $) and precedence operators","• Basic operations in regular expressions include concatenation of symbols, disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ, $) and precedence operators ((,) ).",50
 • Word tokenization and normalization are generally done by cascades of what?,simple regular expression substitutions or finite automata,• Word tokenization and normalization are generally done by cascades of simple regular expression substitutions or finite automata.,72
" What is a simple and efficient way to do stemming, stripping off affixes?",The Porter algorithm,"• The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.",2
 What is the minimum edit distance between two strings?,the minimum number of operations it takes to edit one into the other,"• The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.",51
 What can be computed by dynamic programming?,Minimum edit distance,"• The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.",121
 What is one of the most widely used tools in language processing?,n-gram,"This chapter introduced language modeling and the n-gram, one of the most widely used tools in language processing.",50
 What are Markov models that estimate words from a fixed window of previous words?,n-grams,"• Language models offer a way to assign a probability to a sentence or other sequence of words, and to predict a word from preceding words. • n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate). • n-gram language models are evaluated extrinsically in some task, or intrinsically using perplexity. • The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model. • Smoothing algorithms provide a more sophisticated way to estimate the probability of n-grams. Commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through backoff or interpolation.",142
 How can n-gram probabilities be estimated?,by counting in a corpus and normalizing,"• Language models offer a way to assign a probability to a sentence or other sequence of words, and to predict a word from preceding words. • n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate). • n-gram language models are evaluated extrinsically in some task, or intrinsically using perplexity. • The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model. • Smoothing algorithms provide a more sophisticated way to estimate the probability of n-grams. Commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through backoff or interpolation.",265
 What is the maximum likelihood estimate?,normalizing,"• Language models offer a way to assign a probability to a sentence or other sequence of words, and to predict a word from preceding words. • n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate). • n-gram language models are evaluated extrinsically in some task, or intrinsically using perplexity. • The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model. • Smoothing algorithms provide a more sophisticated way to estimate the probability of n-grams. Commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through backoff or interpolation.",293
 What is the geometric mean of the inverse test set probability computed by the model?,The perplexity of a test set,"• Language models offer a way to assign a probability to a sentence or other sequence of words, and to predict a word from preceding words. • n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate). • n-gram language models are evaluated extrinsically in some task, or intrinsically using perplexity. • The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model. • Smoothing algorithms provide a more sophisticated way to estimate the probability of n-grams. Commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through backoff or interpolation.",444
 Smoothing algorithms provide a more sophisticated way to estimate the probability of what?,n-grams,"• Language models offer a way to assign a probability to a sentence or other sequence of words, and to predict a word from preceding words. • n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate). • n-gram language models are evaluated extrinsically in some task, or intrinsically using perplexity. • The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model. • Smoothing algorithms provide a more sophisticated way to estimate the probability of n-grams. Commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through backoff or interpolation.",671
 What rely on lower-order n-gram counts?,smoothing algorithms,"• Language models offer a way to assign a probability to a sentence or other sequence of words, and to predict a word from preceding words. • n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate). • n-gram language models are evaluated extrinsically in some task, or intrinsically using perplexity. • The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model. • Smoothing algorithms provide a more sophisticated way to estimate the probability of n-grams. Commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through backoff or interpolation.",694
 What do both backoff and interpolation require to create a probability distribution?,discounting,• Both backoff and interpolation require discounting to create a probability distribution. • Kneser-Ney smoothing makes use of the probability of a word being a novel continuation. The interpolated Kneser-Ney smoothing algorithm mixes a discounted probability with a lower-order continuation probability.,41
 What makes use of the probability of a word being a novel continuation?,Kneser-Ney smoothing,• Both backoff and interpolation require discounting to create a probability distribution. • Kneser-Ney smoothing makes use of the probability of a word being a novel continuation. The interpolated Kneser-Ney smoothing algorithm mixes a discounted probability with a lower-order continuation probability.,93
 Which algorithm mixes a discounted probability with a lower-order continuation probability?,Kneser-Ney smoothing,• Both backoff and interpolation require discounting to create a probability distribution. • Kneser-Ney smoothing makes use of the probability of a word being a novel continuation. The interpolated Kneser-Ney smoothing algorithm mixes a discounted probability with a lower-order continuation probability.,198
 What model was introduced in this chapter?,Bayes model for classification,This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.,34
 What task did this chapter apply the Bayes model to?,text categorization task of sentiment analysis,This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.,87
 • Many language processing tasks can be viewed as tasks of what?,classification,• Many language processing tasks can be viewed as tasks of classification.,59
 What does text categorization do?,an entire text is assigned a class from a finite set,"• Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution. • Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object. • Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class) • Naive Bayes with binarized features seems to work better for many text classification tasks. • Classifiers are evaluated based on precision and recall.",32
 What is a class assigned to an entire text from a finite set?,Text categorization,"• Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution. • Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object. • Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class) • Naive Bayes with binarized features seems to work better for many text classification tasks. • Classifiers are evaluated based on precision and recall.",2
 What is negative orientation?,sentiment) that a writer expresses toward some object,"• Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution. • Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object. • Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class) • Naive Bayes with binarized features seems to work better for many text classification tasks. • Classifiers are evaluated based on precision and recall.",289
 Naive Bayes is a generative model that makes the bag of words assumption and what else?,conditional independence assumption,"• Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution. • Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object. • Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class) • Naive Bayes with binarized features seems to work better for many text classification tasks. • Classifiers are evaluated based on precision and recall.",453
 What does Naive Bayes with binarized features seem to work better for?,many text classification tasks,"• Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution. • Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object. • Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class) • Naive Bayes with binarized features seems to work better for many text classification tasks. • Classifiers are evaluated based on precision and recall.",620
 Classifiers are evaluated based on what?,precision and recall,"• Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution. • Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object. • Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class) • Naive Bayes with binarized features seems to work better for many text classification tasks. • Classifiers are evaluated based on precision and recall.",689
 What are classifiers trained using?,"distinct training, dev, and test sets","• Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set. • Statistical significance tests should be used to determine whether we can be confident that one version of a classifier is better than another. • Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",32
 What should be used to determine if one version of a classifier is better than another?,Statistical significance tests,"• Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set. • Statistical significance tests should be used to determine whether we can be confident that one version of a classifier is better than another. • Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",132
 What should designers of classifiers carefully consider?,harms that may be caused by the model,"• Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set. • Statistical significance tests should be used to determine whether we can be confident that one version of a classifier is better than another. • Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",329
 What should classifier designers report in a model card?,model characteristics,"• Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set. • Statistical significance tests should be used to determine whether we can be confident that one version of a classifier is better than another. • Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",429
 What model of classification was introduced in this chapter?,logistic regression,This chapter introduced the logistic regression model of classification.,28
 What is supervised machine learning classifier?,Logistic regression,"• Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision. • Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.). • Multinomial logistic regression uses the softmax function to compute probabilities. • The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized. • Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights. • Regularization is used to avoid overfitting.",2
 What is used to make a decision?,A threshold,"• Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision. • Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.). • Multinomial logistic regression uses the softmax function to compute probabilities. • The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized. • Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights. • Regularization is used to avoid overfitting.",230
 How many classes can logistic regression be used with?,two,"• Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision. • Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.). • Multinomial logistic regression uses the softmax function to compute probabilities. • The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized. • Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights. • Regularization is used to avoid overfitting.",309
 What is used to compute probabilities in multinomial logistic regression?,softmax function,"• Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision. • Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.). • Multinomial logistic regression uses the softmax function to compute probabilities. • The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized. • Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights. • Regularization is used to avoid overfitting.",538
 How are weights learned from a labeled training set?,via a loss function,"• Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision. • Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.). • Multinomial logistic regression uses the softmax function to compute probabilities. • The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized. • Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights. • Regularization is used to avoid overfitting.",657
 What is a convex optimization problem?,Minimizing this loss function,"• Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision. • Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.). • Multinomial logistic regression uses the softmax function to compute probabilities. • The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized. • Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights. • Regularization is used to avoid overfitting.",736
 How are iterative algorithms like gradient descent used?,to find the optimal weights,"• Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision. • Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.). • Multinomial logistic regression uses the softmax function to compute probabilities. • The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized. • Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights. • Regularization is used to avoid overfitting.",856
 What is used to find the optimal weights?,iterative algorithms like gradient descent,"• Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision. • Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.). • Multinomial logistic regression uses the softmax function to compute probabilities. • The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized. • Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights. • Regularization is used to avoid overfitting.",804
 What is regularization used to avoid?,overfitting,"• Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision. • Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.). • Multinomial logistic regression uses the softmax function to compute probabilities. • The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized. • Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights. • Regularization is used to avoid overfitting.",919
 What is one of the most useful analytic tools?,Logistic regression,"• Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features.",2
 Logistic regression studies the importance of what?,individual features,"• Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features.",133
 What is modeled as a vector-a point in high-dimensional space?,a word,"• In vector semantics, a word is modeled as a vector-a point in high-dimensional space, also called an embedding. In this chapter we focus on static embeddings, in each each word is mapped to a fixed embedding.",23
 What is also called an embedding?,a word is modeled as a vector-a point in high-dimensional space,"• In vector semantics, a word is modeled as a vector-a point in high-dimensional space, also called an embedding. In this chapter we focus on static embeddings, in each each word is mapped to a fixed embedding.",23
" In vector semantics, what is a word modelled as?",a vector,"• In vector semantics, a word is modeled as a vector-a point in high-dimensional space, also called an embedding. In this chapter we focus on static embeddings, in each each word is mapped to a fixed embedding.",44
 • Vector semantic models fall into what two classes?,sparse and dense,"• Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",48
 In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of what?,co-occurrence counts,"• Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",167
 The term-document matrix has a row for each word (term) in what vocabulary?,V,"• Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",138
 How many sparse weightings are common?,Two,"• Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",437
 What does the tf-idf weighting weight each cell by?,its term frequency and inverse document frequency,"• Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",519
 What weighting weights each cell by its term frequency and inverse document frequency?,tf-idf,"• Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",475
 What is most common for word-context matrices?,PPMI (pointwise positive mutual information,"• Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",574
 What do dense vector models have?,dimensionality 50-1000,"• Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words. • Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words. • Other important embedding algorithms include GloVe, a method based on ratios of word co-occurrence probabilities. • Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",27
 What is a popular way to compute dense embeddings?,Word2vec algorithms like skip-gram,"• Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words. • Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words. • Other important embedding algorithms include GloVe, a method based on ratios of word co-occurrence probabilities. • Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",51
 Skip-gram trains a logistic regression classifier to compute the probability that two words are likely to occur nearby in text?,dot product between the embeddings for the two words,"• Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words. • Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words. • Other important embedding algorithms include GloVe, a method based on ratios of word co-occurrence probabilities. • Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",301
 What is the probability computed from?,the dot product between the embeddings for the two words,"• Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words. • Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words. • Other important embedding algorithms include GloVe, a method based on ratios of word co-occurrence probabilities. • Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",297
 What does Skip-gram use to train the classifier?,stochastic gradient descent,"• Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words. • Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words. • Other important embedding algorithms include GloVe, a method based on ratios of word co-occurrence probabilities. • Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",372
 What is the GloVe method based on?,ratios of word co-occurrence probabilities,"• Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words. • Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words. • Other important embedding algorithms include GloVe, a method based on ratios of word co-occurrence probabilities. • Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",632
 Word and document similarities are computed by some function of what?,the dot product between vectors,"• Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words. • Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words. • Other important embedding algorithms include GloVe, a method based on ratios of word co-occurrence probabilities. • Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",781
 What is the most popular metric of the dot product between vectors?,The cosine of two vectors,"• Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words. • Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words. • Other important embedding algorithms include GloVe, a method based on ratios of word co-occurrence probabilities. • Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",814
 The cosine of two vectors is what?,a normalized dot product,"• Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words. • Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words. • Other important embedding algorithms include GloVe, a method based on ratios of word co-occurrence probabilities. • Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",840
 What are neural networks built out of?,neural units,"• Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device. • Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit. • In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles. • The power of neural networks comes from the ability of early layers to learn representations that can be utilized by later layers in the network. • Neural networks are trained by optimization algorithms like gradient descent. • Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network. • Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words. • Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",35
 What was the original inspiration for neural networks?,human neurons,"• Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device. • Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit. • In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles. • The power of neural networks comes from the ability of early layers to learn representations that can be utilized by later layers in the network. • Neural networks are trained by optimization algorithms like gradient descent. • Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network. • Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words. • Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",72
" What activation function is like sigmoid, tanh, or rectified linear unit?",non-linear,"• Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device. • Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit. • In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles. • The power of neural networks comes from the ability of early layers to learn representations that can be utilized by later layers in the network. • Neural networks are trained by optimization algorithms like gradient descent. • Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network. • Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words. • Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",230
" In a fully connected, feedforward network, what is each unit in layer i connected to?",each unit in layer i + 1,"• Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device. • Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit. • In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles. • The power of neural networks comes from the ability of early layers to learn representations that can be utilized by later layers in the network. • Neural networks are trained by optimization algorithms like gradient descent. • Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network. • Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words. • Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",389
 What is the power of neural networks?,ability of early layers to learn representations that can be utilized by later layers in the network,"• Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device. • Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit. • In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles. • The power of neural networks comes from the ability of early layers to learn representations that can be utilized by later layers in the network. • Neural networks are trained by optimization algorithms like gradient descent. • Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network. • Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words. • Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",486
 How are neural networks trained?,optimization algorithms like gradient descent,"• Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device. • Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit. • In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles. • The power of neural networks comes from the ability of early layers to learn representations that can be utilized by later layers in the network. • Neural networks are trained by optimization algorithms like gradient descent. • Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network. • Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words. • Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",621
 What is the term for backward differentiation on a computation graph?,Error backpropagation,"• Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device. • Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit. • In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles. • The power of neural networks comes from the ability of early layers to learn representations that can be utilized by later layers in the network. • Neural networks are trained by optimization algorithms like gradient descent. • Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network. • Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words. • Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",670
 What is used to compute the gradients of the loss function for a network?,Error backpropagation,"• Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device. • Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit. • In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles. • The power of neural networks comes from the ability of early layers to learn representations that can be utilized by later layers in the network. • Neural networks are trained by optimization algorithms like gradient descent. • Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network. • Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words. • Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",670
 What do neural language models use a neural network as a probabilistic classifier?,to compute the probability of the next word given the previous n words,"• Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device. • Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit. • In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles. • The power of neural networks comes from the ability of early layers to learn representations that can be utilized by later layers in the network. • Neural networks are trained by optimization algorithms like gradient descent. • Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network. • Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words. • Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",888
 Pretrained embeddings can be used by what type of model?,Neural language models,"• Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device. • Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit. • In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles. • The power of neural networks comes from the ability of early layers to learn representations that can be utilized by later layers in the network. • Neural networks are trained by optimization algorithms like gradient descent. • Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network. • Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words. • Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",962
 Neural language models can learn what from scratch?,embeddings,"• Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device. • Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit. • In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles. • The power of neural networks comes from the ability of early layers to learn representations that can be utilized by later layers in the network. • Neural networks are trained by optimization algorithms like gradient descent. • Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network. • Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words. • Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",1029
 What did this chapter introduce?,parts of speech and named entities,"This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:",24
 What tasks were introduced in this chapter?,partof-speech tagging and named entity recognition,"This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:",77
 What is the process of assigning a part-of-speech label to a word?,Part-of-speech tagging,"• Languages generally have a small set of closed class words that are highly frequent, ambiguous, and act as function words, and open-class words like nouns, verbs, adjectives. Various part-of-speech tagsets exist, of between 40 and 200 tags. • Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words. • Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.",245
" What are closed class words that are highly frequent, ambiguous, and act as function words?",small set,"• Languages generally have a small set of closed class words that are highly frequent, ambiguous, and act as function words, and open-class words like nouns, verbs, adjectives. Various part-of-speech tagsets exist, of between 40 and 200 tags. • Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words. • Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.",29
 What is the process of assigning a part-of-speech label to each of a sequence of words?,Part-of-speech tagging,"• Languages generally have a small set of closed class words that are highly frequent, ambiguous, and act as function words, and open-class words like nouns, verbs, adjectives. Various part-of-speech tagsets exist, of between 40 and 200 tags. • Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words. • Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.",245
" What are words for proper nouns referring mostly to people, places, and organizations?",Named entities,"• Languages generally have a small set of closed class words that are highly frequent, ambiguous, and act as function words, and open-class words like nouns, verbs, adjectives. Various part-of-speech tagsets exist, of between 40 and 200 tags. • Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words. • Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.",353
 What are two common approaches to sequence modeling?,"generative approach, HMM tagging, and a discriminative approach, CRF tagging","• Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters. • The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence • Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",51
 What is a generative approach called?,HMM tagging,"• Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters. • The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence • Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",72
 How are the probabilities in HMM taggers estimated?,maximum likelihood estimation on tag-labeled training corpora,"• Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters. • The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence • Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",234
 Which algorithm is used for the estimation of probabilities?,Viterbi algorithm,"• Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters. • The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence • Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",301
" What is used for decoding, finding the most likely tag sequence?",Viterbi algorithm,"• Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters. • The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence • Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",301
 What is another name for Conditional Random Fields?,CRF taggers,"• Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters. • The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence • Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",409
 How do CRF taggers train a log-linear model?,"based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep","• Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters. • The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence • Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",515
 What is the Viterbi algorithm used for?,"decoding, finding the most likely tag sequence","• Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters. • The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence • Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",331
 What is a version of the Forward-Backward algorithm?,training,"• Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters. • The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence • Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",794
 What are recurrent neural networks and transformers?,language problems,This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:,118
 What are transformers used for?,language problems,This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:,118
 How are Recurrent Neural Networks sequences processed?,one element at a time,"• In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1. • RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT). • Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers. • Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word. • A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks. • Common language-based applications for RNNs and transformers include:",62
 What is the output of each neural unit at time t based on?,the current input at t and the hidden layer from time t − 1,"• In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1. • RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT). • Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers. • Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word. • A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks. • Common language-based applications for RNNs and transformers include:",145
 How can RNNs be trained?,with a straightforward extension of the backpropagation algorithm,"• In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1. • RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT). • Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers. • Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word. • A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks. • Common language-based applications for RNNs and transformers include:",228
 What is BPTT?,backpropagation through time,"• In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1. • RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT). • Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers. • Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word. • A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks. • Common language-based applications for RNNs and transformers include:",304
 Why do simple recurrent networks fail on long inputs?,problems like vanishing gradients,"• In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1. • RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT). • Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers. • Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word. • A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks. • Common language-based applications for RNNs and transformers include:",400
 Modern systems use more complex gated architectures such as what?,LSTMs,"• In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1. • RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT). • Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers. • Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word. • A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks. • Common language-based applications for RNNs and transformers include:",503
 What type of architectures decide what to remember and forget?,complex gated,"• In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1. • RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT). • Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers. • Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word. • A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks. • Common language-based applications for RNNs and transformers include:",467
 Transformers are non-recurrent networks based on what?,self-attention,"• In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1. • RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT). • Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers. • Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word. • A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks. • Common language-based applications for RNNs and transformers include:",647
 A selfattention layer maps input sequences to output sequences of the same length?,based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word,"• In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1. • RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT). • Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers. • Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word. • A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks. • Common language-based applications for RNNs and transformers include:",746
 What is based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word?,A selfattention layer,"• In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1. • RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT). • Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers. • Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word. • A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks. • Common language-based applications for RNNs and transformers include:",663
 A transformer block consists of a single attention layer followed by what?,a feedforward layer,"• In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1. • RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT). • Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers. • Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word. • A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks. • Common language-based applications for RNNs and transformers include:",946
 What can be stacked to make deeper and more powerful networks?,Transformer blocks,"• In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1. • RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT). • Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers. • Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word. • A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks. • Common language-based applications for RNNs and transformers include:",1033
 What are common language-based applications for RNNs?,transformers,"• In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1. • RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT). • Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers. • Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word. • A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks. • Common language-based applications for RNNs and transformers include:",1160
 What is a term for probabilistic language modeling?,"assigning a probability to a sequence, or to the next element of a sequence given the preceding words","-Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",34
 What is an example of part-of-speech tagging?,Sequence labeling,"-Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",198
 What is assigned a label?,each element of a sequence,"-Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",251
 What classification is used when an entire text is assigned to a category?,Sequence,"-Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",300
 What is one of the most widely used applications of NLP?,Machine translation,"Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.",0
 The encoder-decoder model was first developed for what?,MT,"Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.",123
 What makes translation difficult?,Languages have divergences,"• Languages have divergences, both structural and lexical, that make translation difficult. • The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects. • Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence. • The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder. • For the decoder, choosing the single most probable token to generate at each step is called greedy decoding. • In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width. • Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. • Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts. • MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",2
 What investigates some of these differences?,The linguistic field of typology,"• Languages have divergences, both structural and lexical, that make translation difficult. • The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects. • Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence. • The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder. • For the decoder, choosing the single most probable token to generate at each step is called greedy decoding. • In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width. • Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. • Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts. • MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",94
 Languages can be classified by their position along what dimensions?,typological,"• Languages have divergences, both structural and lexical, that make translation difficult. • The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects. • Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence. • The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder. • For the decoder, choosing the single most probable token to generate at each step is called greedy decoding. • In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width. • Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. • Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts. • MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",219
 What are encoder-decoder networks composed of?,"an encoder network that takes an input sequence and creates a contextualized representation of it, the context","• Languages have divergences, both structural and lexical, that make translation difficult. • The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects. • Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence. • The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder. • For the decoder, choosing the single most probable token to generate at each step is called greedy decoding. • In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width. • Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. • Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts. • MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",361
 What is an encoder network that takes an input sequence and creates a contextualized representation of it?,Encoder-decoder networks,"• Languages have divergences, both structural and lexical, that make translation difficult. • The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects. • Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence. • The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder. • For the decoder, choosing the single most probable token to generate at each step is called greedy decoding. • In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width. • Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. • Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts. • MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",286
 The context representation is then passed to a decoder which generates what?,a task-specific output sequence,"• Languages have divergences, both structural and lexical, that make translation difficult. • The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects. • Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence. • The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder. • For the decoder, choosing the single most probable token to generate at each step is called greedy decoding. • In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width. • Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. • Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts. • MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",545
 What mechanism allows the decoder to view information from all hidden states of the encoder?,attention mechanism,"• Languages have divergences, both structural and lexical, that make translation difficult. • The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects. • Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence. • The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder. • For the decoder, choosing the single most probable token to generate at each step is called greedy decoding. • In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width. • Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. • Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts. • MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",584
 What is greedy decoding?,choosing the single most probable token to generate at each step,"• Languages have divergences, both structural and lexical, that make translation difficult. • The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects. • Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence. • The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder. • For the decoder, choosing the single most probable token to generate at each step is called greedy decoding. • In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width. • Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. • Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts. • MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",751
 What is this fixed-size memory footprint called?,the beam width,"• Languages have divergences, both structural and lexical, that make translation difficult. • The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects. • Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence. • The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder. • For the decoder, choosing the single most probable token to generate at each step is called greedy decoding. • In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width. • Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. • Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts. • MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",1011
 Machine translation models are trained on what?,a parallel corpus,"• Languages have divergences, both structural and lexical, that make translation difficult. • The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects. • Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence. • The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder. • For the decoder, choosing the single most probable token to generate at each step is called greedy decoding. • In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width. • Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. • Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts. • MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",1071
 What are translation models trained on?,a parallel corpus,"• Languages have divergences, both structural and lexical, that make translation difficult. • The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects. • Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence. • The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder. • For the decoder, choosing the single most probable token to generate at each step is called greedy decoding. • In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width. • Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. • Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts. • MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",1071
 What is a bitext?,a text that appears in two (or more) languages,"• Languages have divergences, both structural and lexical, that make translation difficult. • The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects. • Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence. • The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder. • For the decoder, choosing the single most probable token to generate at each step is called greedy decoding. • In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width. • Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. • Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts. • MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",1117
 Backtranslation uses monolingual corpora in what language?,target language,"• Languages have divergences, both structural and lexical, that make translation difficult. • The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects. • Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence. • The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder. • For the decoder, choosing the single most probable token to generate at each step is called greedy decoding. • In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width. • Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. • Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts. • MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",1236
 What does MT engine backwards create?,synthetic bitexts,"• Languages have divergences, both structural and lexical, that make translation difficult. • The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects. • Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence. • The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder. • For the decoder, choosing the single most probable token to generate at each step is called greedy decoding. • In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width. • Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. • Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts. • MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",1301
 What is MT evaluated by?,measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language).,"• Languages have divergences, both structural and lexical, that make translation difficult. • The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects. • Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence. • The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder. • For the decoder, choosing the single most probable token to generate at each step is called greedy decoding. • In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width. • Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. • Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts. • MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",1341
 Who is the gold standard?,Human evaluation,"• Languages have divergences, both structural and lexical, that make translation difficult. • The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects. • Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence. • The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder. • For the decoder, choosing the single most probable token to generate at each step is called greedy decoding. • In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width. • Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. • Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts. • MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",1500
 What measure character n-gram overlap with human translations?,chrF,"• Languages have divergences, both structural and lexical, that make translation difficult. • The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects. • Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence. • The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder. • For the decoder, choosing the single most probable token to generate at each step is called greedy decoding. • In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width. • Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. • Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts. • MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",1577
 What measure more recent metrics based on embedding similarity?,chrF,"• Languages have divergences, both structural and lexical, that make translation difficult. • The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects. • Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence. • The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder. • For the decoder, choosing the single most probable token to generate at each step is called greedy decoding. • In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width. • Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. • Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts. • MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",1577
 What is a fundamental tool in language processing?,The regular expression,"The regular expression is a fundamental tool in language processing, and this chapter showed how to perform basic text normalization tasks.",0
 What did this chapter show how to perform?,basic text normalization tasks,"The regular expression is a fundamental tool in language processing, and this chapter showed how to perform basic text normalization tasks.",108
 What is the minimum edit distance?,minimum edit distance,The minimum edit distance was introduced.,4
 What is a summary of the main points we covered?,Here,Here is a summary of the main points we covered.,0
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 What language can be used for pattern matching?,regular expression,The regular expression language can be used for pattern matching.,4
 What are two basic operations in regular expressions?,concatenation of symbols and disjunction of symbols,Basic operations in regular expressions include concatenation of symbols and disjunction of symbols.,48
 Concatenation of symbols and disjunction of symbols are two examples of what?,Basic operations in regular expressions,Basic operations in regular expressions include concatenation of symbols and disjunction of symbols.,0
 What can be done by cascades of simple regular expression substitution?,Word tokenization and normalization,Word tokenization and normalization can be done by cascades of simple regular expression substitution.,0
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
" What algorithm can be used to stemming, stripping off affixes?",Porter algorithm,"The Porter algorithm can be used to stemming, stripping off affixes.",4
 What kind of accuracy does this tool not have?,high,"It doesn't have high accuracy, but it is useful for some tasks.",16
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 How many operations does it take to edit one into the other?,minimum number,The minimum number of operations it takes to edit one into the other is the minimum edit distance.,4
 What is the minimum edit distance?,The minimum number of operations,The minimum number of operations it takes to edit one into the other is the minimum edit distance.,0
 What can be used to calculate the minimum edit distance?,Dynamic programming,Dynamic programming can be used to calculate the minimum edit distance.,0
 In what chapter were language modeling and the n-gram introduced?,this chapter,Language modeling and the n-gram were introduced in this chapter.,52
 Language models can be used to assign what to a sentence or a sequence of words?,a probability,Language models can be used to assign a probability to a sentence or a sequence of words.,38
 What are models that estimate words from a fixed window?,n-grams,n-grams are models that estimate words from a fixed window.,0
 What is the name of the neophyte?,n,n,0
 What does naivete mean?,n,n,0
 What can be estimated by counting in a corpus?,The maximum likelihood estimate,The maximum likelihood estimate can be estimated by counting in a corpus.,0
 What is used to evaluate n-gram language models?,perplexity,n-gram language models are evaluated using perplexity.,43
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 What is the geometric mean of the inverse test set probability computed by the model?,the perplexity of a test set,The geometric mean of the inverse test set probability computed by the model is the perplexity of a test set.,80
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 What is a more sophisticated way to estimate the probability of n-grams?,There is a more sophisticated way to estimate the probability of n-grams.,There is a more sophisticated way to estimate the probability of n-grams.,0
 Lower-order n-gram counts are used for what?,smoothing,Lower-order n-gram counts are used for smoothing.,39
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 What is required to create a probability distribution?,discounting,discounting is required to create a probability distribution.,0
 Kneser-Ney smoothing uses the probability of a word being what?,a novel continuation,The probability of a word being a novel continuation is used by Kneser-Ney smoothing.,32
 What is mixed with a lower-order continuation probability?,A discounted probability,A discounted probability is mixed with a lower-order continuation probability.,0
 What was applied to the text categorization task of sentiment analysis?,naive Bayes model,The naive Bayes model was applied to the text categorization task of sentiment analysis.,4
 What was the naive Bayes model?,text categorization task of sentiment analysis,The naive Bayes model was applied to the text categorization task of sentiment analysis.,41
 Language processing tasks can be seen as tasks of what?,classification,Language processing tasks can be seen as tasks of classification.,50
 What is the term for when an entire text is assigned a class from a finite set?,Text categorization,Text categorization is when an entire text is assigned a class from a finite set.,0
 What type of analysis considers a text to be a reflection of the positive or negative orientation a writer expresses toward an object?,Sentiment,Sentiment analysis considers a text to be a reflection of the positive or negative orientation that a writer expresses toward an object.,0
 The bag of words assumption and the conditional independence assumption are made by what model?,generative,The bag of words assumption and the conditional independence assumption are made by a generative model.,86
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 Bayes with binarized features seem to work better for what?,text classification tasks,Bayes with binarized features seem to work better for text classification tasks.,54
 What are classifiers evaluated based on?,recall and precision,Classifiers are evaluated based on recall and precision.,35
 What is included in the training set?,The use of cross-validation,The use of cross-validation in the training set is included in the training set.,0
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 What should we use if we can't be sure that one version is better than the other?,a statistical significance test,"If we can't be sure that one version is better than the other, we should use a statistical significance test.",77
 What should harms that may be caused by a model be reported in?,a model card,"The harms that may be caused by the model, including its training data and other components, should be reported in a model card.",115
 What model of classification was introduced in this chapter?,model of classification,The model of classification was introduced in this chapter.,4
 What is supervised machine learning classification?,Logistic regression,"Logistic regression is a supervised machine learning classification that extracts real-valued features from the input, sums them, and passes the sum through a sigmoid function to generate a probability.",0
 What does logistic regression extract from input?,real-valued features,"Logistic regression is a supervised machine learning classification that extracts real-valued features from the input, sums them, and passes the sum through a sigmoid function to generate a probability.",82
 What is used to make a decision?,A threshold,A threshold is used to make a decision.,0
 What can be used with more than one class?,Logistic regression,"Logistic regression can be used with more than one class, for example for text classification and part-of-speech labeling.",0
" For example, what can logistic regression be used for?",text classification and part-of-speech labeling,"Logistic regression can be used with more than one class, for example for text classification and part-of-speech labeling.",74
 What function is used to compute probabilities?,softmax,The softmax function is used to compute probabilities.,4
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 What is a problem that can be solved using iterative techniques?,Minimizing the loss function,Minimizing the loss function is a problem that can be solved using iterative techniques.,0
 Is regularization used to avoid overfitting?,Regularization is used to avoid overfitting,Regularization is used to avoid overfitting.,0
 What is one of the most useful analytic tools?,Logistic regression,Logistic regression is one of the most useful analytic tools because of its ability to study the importance of individual features.,0
 Logistic regression studies the importance of what?,individual features,Logistic regression is one of the most useful analytic tools because of its ability to study the importance of individual features.,111
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 What is modeled as a point in high-dimensional space?,A word,A word is modeled as a point in high-dimensional space.,0
 What is mapped to a fixed embedded in the chapter?,Each word,Each word is mapped to a fixed embedded in the chapter.,0
 What are the two classes of semantic models?,sparse and dense,There are two classes of semantic models: sparse and dense.,42
 Cells are functions of what?,co-occurrence counts,Cells are functions of co-occurrence counts and each corresponds to a word in the vocabulary.,23
 Each cell corresponds to a word in the vocabulary?,Cells,Cells are functions of co-occurrence counts and each corresponds to a word in the vocabulary.,0
 What does the matrix have for each word in the vocabulary?,a row,The matrix has a row for each word in the vocabulary and a column for each document.,15
 What is the column for each document?,matrix,The matrix has a row for each word in the vocabulary and a column for each document.,4
 What is another name for the word-context matrix?,term-term,The word-context or term-term matrix has a row for each target word in the vocabulary and a column for each context term in the vocabulary.,20
 What is the row for each target word in the vocabulary?,The word-context or term-term matrix,The word-context or term-term matrix has a row for each target word in the vocabulary and a column for each context term in the vocabulary.,0
 The column for each context term is what?,vocabulary,The word-context or term-term matrix has a row for each target word in the vocabulary and a column for each context term in the vocabulary.,128
 What are the two most common sparse weights for word-context matrices?,tf-idf and PPMI,The tf-idf and PPMI are the two most common sparse weights for word-context matrices.,4
 What is the density of the models?,50-1000,The density of the models is 50-1000.,29
 What is a popular way to compute dense embeddeds?,Word2vec,Word2vec is a popular way to compute dense embeddeds.,0
 What is the chance that two words are likely to appear near a text?,There,There is a chance that two words are likely to occur nearby in text.,0
 What product is used to calculate the probability?,dot product,The dot product is used to calculate the probability.,4
 How does Skip-gram learn from embeddings that have a high and low dot product?,a method of training,Skip-gram uses a method of training that learns from the embeddings that have a high dot product and low dot product.,15
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 What is the name of the method based on word co-occurrence probabilities?,GloVe,The method based on word co-occurrence probabilities is called GloVe.,63
 What is GloVe?,The method based on word co-occurrence probabilities,The method based on word co-occurrence probabilities is called GloVe.,0
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 What determines the similarities between Word and document?,the function of the dot product between the vectors,Word and document similarities are determined by the function of the dot product between the vectors.,49
 What is the most popular metric?,normalized dot product,The normalized dot product is the most popular metric.,4
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 What were neural networks originally inspired by?,human neurons,Neural networks were originally inspired by human neurons but are now just an abstract computational device.,44
 What are neural networks now just an abstract computational device?,Neural networks were originally inspired by human neurons,Neural networks were originally inspired by human neurons but are now just an abstract computational device.,0
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 Neural units add a bias and apply a non- linear activation function like what?,"sigmoid, tanh, or rectified linear unit","Neural units add a bias and apply a non- linear activation function like sigmoid, tanh, or rectified linear unit.",73
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 What is the name of the unit in layer i connected to?,each unit in layer i + 1,Each unit in layer i is connected to each unit in layer i + 1 and there are no cycles.,37
 What happens when each unit is connected to each unit?,there are no cycles,Each unit in layer i is connected to each unit in layer i + 1 and there are no cycles.,66
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 What is powered by the ability of early layers to learn representations that can be used by later layers?,Neural networks,Neural networks are powered by the ability of early layers to learn representations that can be used by later layers.,0
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 How are neural networks trained?,Neural networks are trained.,Neural networks are trained.,0
 What is used to compute the loss function for a network?,error backpropagation,The error backpropagation is used to compute the loss function for a network.,4
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 What do neural language models use to estimate the probability of the next word?,a neural network,Neural language models use a neural network to estimate the probability of the next word.,27
 Neural language models can learn embeddeds from scratch or what?,pretrained,Neural language models can learn embeddeds from scratch or pretrained them.,59
 What were introduced in this chapter?,Part of speech and named entities,Part of speech and named entities were introduced in this chapter.,0
 What was introduced in the chapter that dealt with speech and named entities?,Part of speech,Part of speech and named entities were introduced in this chapter.,0
 How many closed class words are there?,small,"There is a small set of closed class words that are highly frequent, ambiguous, and act as function words.",11
 What do closed class terms act as?,function words,"There is a small set of closed class words that are highly frequent, ambiguous, and act as function words.",91
 How many part-of-speech tags are there?,between 40 and 200,There are between 40 and 200 part-of-speech tags.,10
 What is assigned to each of a sequence of words?,A part-of-speech label,A part-of-speech label is assigned to each of a sequence of words.,0
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 What are many other types of named entities?,not strictly entities or even proper nouns,Many other types of named entities are not strictly entities or even proper nouns.,39
 What are the two approaches to sequence modeling?,generative and discriminative,Two approaches to sequence modeling are generative and discriminative.,40
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 What does the maximum likelihood estimation estimate on tag-labeled training corpora?,the probabilities in HMM taggers,The maximum likelihood estimation on tag-labeled training corpora estimates the probabilities in HMM taggers.,76
 What can be used to find the most likely tag sequence?,log-linear model,The most likely tag sequence can be found with the help of a log-linear model that can choose the best tag sequence based on features that condition on the output tag.,61
 What model can choose the best tag sequence based on features that condition on the output tag?,log-linear,The most likely tag sequence can be found with the help of a log-linear model that can choose the best tag sequence based on features that condition on the output tag.,61
 What is used for training?,The best sequence of tags and a version of the Forward- Backward algorithm,The best sequence of tags and a version of the Forward- Backward algorithm are used for training.,0
 What algorithm is used?,Forward- Backward,The best sequence of tags and a version of the Forward- Backward algorithm are used for training.,47
 The concepts of recurrent neural networks and transformers have been introduced in what chapter?,this chapter,The concepts of recurrent neural networks and transformers have been introduced in this chapter.,83
 What is a summary of the main points we covered?,Here,Here is a summary of the main points we covered.,0
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 What is the output of each neural unit based on?,the current input and the hidden layer from time,"The output of each neural unit is based on the current input and the hidden layer from time, as well as the one element at a time.",43
 What is the value of t1?,t  1,t  1.,0
 What is a simple extension of the backpropagation algorithm?,Backpropagation through time,Backpropagation through time is a simple extension of the backpropagation algorithm.,0
 Modern systems use more complex gated architectures that explicitly decide what to remember and forget in their hidden and context layers?,simple recurrent networks fail,"Modern systems use more complex gated architectures that explicitly decide what to remember and forget in their hidden and context layers, which is why simple recurrent networks fail.",152
 Simple recurrent networks fail because of what?,gated architectures that explicitly decide what to remember and forget in their hidden and context layers,"Modern systems use more complex gated architectures that explicitly decide what to remember and forget in their hidden and context layers, which is why simple recurrent networks fail.",32
 Non-recurrent networks are based on what?,self-attention,Non-recurrent networks are based on self-attention.,36
 How does a selfattention layer map input sequence to output sequence of the same length?,based on a set of attention heads,"A selfattention layer maps input sequence to output sequence of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word",81
 What model how the surrounding words are relevant for the processing of the current word?,"selfattention layer maps input sequence to output sequence of the same length, based on a set of attention heads","A selfattention layer maps input sequence to output sequence of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word",2
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 A transformer block consists of a single attention layer followed by a feed forward layer with what?,residual connections,A transformer block consists of a single attention layer followed by a feed forward layer with residual connections.,95
 What can be stacked to make more powerful networks?,Transformer blocks,Transformer blocks can be stacked to make more powerful networks.,0
 What kind of applications are included for RNNs and transformers?,Common language-based,Common language-based applications for RNNs and transformers are included.,0
 What is the term for probabilistic?,probabilistic,It is probabilistic.,6
 What type of modeling is used for language modeling?,Language modeling.,Language modeling.,0
 What can be assigned to the next element of a sequence?,a probability,The next element of a sequence can be assigned a probability.,47
 What is used to generate auto-regressive generation?,trained language model,A trained language model is used to generate auto-regressive generation.,2
 What does equence stand for?,equence,equence,0
 What is assigned to each element of a sequence?,a label,Each element of a sequence is assigned a label.,39
 What is the term for when an entire text is assigned to a category?,equence classification,equence classification is when an entire text is assigned to a category.,0
 What is one of the most widely used applications of NLP?,machine translation,"One of the most widely used applications of NLP is machine translation, and the encoder-decoder model is a key tool that has applications throughout the system.",51
 What is a key tool that has applications throughout the system?,encoder-decoder model,"One of the most widely used applications of NLP is machine translation, and the encoder-decoder model is a key tool that has applications throughout the system.",80
 Structural and lexical differences make translation difficult?,Structural and lexical differences make translation difficult.,Structural and lexical differences make translation difficult.,0
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 What field investigates some of the differences?,typology,Some of the differences are investigated by the linguistic field of typology.,68
 What is a transformer composed of?,an Encoder network,A transformer is composed of an Encoder network that takes an input sequence and creates a contextualized representation of it.,29
 What network takes an input sequence and creates a contextualized representation of it?,Encoder network,A transformer is composed of an Encoder network that takes an input sequence and creates a contextualized representation of it.,32
 A task-specific output sequence is generated from what context representation?,context representation,A task-specific output sequence is generated from this context representation.,55
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 What allows the decoder to see all the hidden states of the encoder?,attention mechanism in RNNs and cross-attention in transformers,The attention mechanism in RNNs and cross-attention in transformers allow the decoder to see all the hidden states of the encoder.,4
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 What is the most probable token to generate at each step?,greedy decoding,The single most probable token to generate at each step is called greedy decoding.,66
 What is greedy decoding?,The single most probable token to generate at each step,The single most probable token to generate at each step is called greedy decoding.,0
 What is the name of the person who has a job?,a,There is a,9
 What type of job is there?,a,There is a,9
 Where is there a place to go?,a,There is a,9
 What do we do instead of choosing the best token to generate at each step?,we keep possible token at each step,"Instead of choosing the best token to generate at each step, we keep possible token at each step.",61
 What is the fixed-size memory footprint k?,The beam width,The beam width is the fixed-size memory footprint k.,0
 A bitext is a text that appears in two or more languages when trained on what model?,machine translation,A bitext is a text that appears in two or more languages when trained on a machine translation model.,75
 What is a way of making use of monolingual corpora in the target language?,Backtranslation,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot engine backwards.,0
 What is backtranslation?,a way of making use of monolingual corpora in the target language by running a pilot engine backwards,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot engine backwards.,19
 How is translation's effectiveness measured?,how well it captures the meaning of the source sentence and how natural it is in the target language,The translation's effectiveness is measured by how well it captures the meaning of the source sentence and how natural it is in the target language.,47
 How well does translation capture the meaning of the source sentence and how natural it is in target language?,effectiveness,The translation's effectiveness is measured by how well it captures the meaning of the source sentence and how natural it is in the target language.,18
 What measure character n-gram overlap with human translations?,chrF,"Automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, and more recent metrics based on embedded similarity are commonly used.",34
 What measure recent metrics based on embedded similarity are commonly used?,Automatic evaluation metrics,"Automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, and more recent metrics based on embedded similarity are commonly used.",0
