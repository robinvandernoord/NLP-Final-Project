question,answer,context,start_pos
 What is a fundamental tool in language processing?,the regular expression,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",67
 What did this chapter show how to perform?,basic text normalization tasks,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",117
 What is comparing strings?,minimum edit distance algorithm,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",265
 What are the main points we covered about these ideas?,a summary,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",327
 What is a powerful tool for pattern-matching?,The regular expression language,The regular expression language is a powerful tool for pattern-matching.,0
 What are some basic operations in regular expressions?,"concatenation of symbols, disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ, $) and precedence operators","Basic operations in regular expressions include concatenation of symbols, disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ, $) and precedence operators ((,) ).",48
 Word tokenization and normalization are usually done by cascades of what?,simple regular expression substitutions or finite automata,Word tokenization and normalization are generally done by cascades of simple regular expression substitutions or finite automata.,70
 What is a simple and efficient way to do stemming?,The Porter algorithm,"The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.",0
 What does the Porter algorithm strip off?,affixes,"The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.",81
 What is the minimum edit distance between two strings?,the minimum number of operations it takes to edit one into the other,"The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.",49
 What can be computed by dynamic programming?,Minimum edit distance,"The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.",119
 What is one of the most widely used tools in language processing?,n-gram,"This chapter introduced language modeling and the n-gram, one of the most widely used tools in language processing.",50
 Language models offer a way to assign a probability to a sentence or other sequence of words?,to predict a word from preceding words,"Language models offer a way to assign a probability to a sentence or other sequence of words, and to predict a word from preceding words.",98
 What are Markov models that estimate words from a fixed window of previous words?,n-grams,n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).,0
 How can n-gram probabilities be estimated?,by counting in a corpus and normalizing,n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).,123
 What are n-gram language models evaluated extrinsically in some task?,intrinsically using perplexity,"n-gram language models are evaluated extrinsically in some task, or intrinsically using perplexity.",68
 What is the perplexity of a test set according to a language model?,the geometric mean of the inverse test set probability computed by the model,The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.,62
 The geometric mean of the inverse test set probability computed by the model is what?,The perplexity,The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.,0
 Smoothing algorithms provide a more sophisticated way to estimate the probability of what?,n-grams,Smoothing algorithms provide a more sophisticated way to estimate the probability of n-grams. Commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through backoff or interpolation.,85
 Commonly used smoothing algorithms rely on lower-order n-gram counts through backoff or what else?,interpolation,Smoothing algorithms provide a more sophisticated way to estimate the probability of n-grams. Commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through backoff or interpolation.,194
 What do both backoff and interpolation require to create a probability distribution?,discounting,Both backoff and interpolation require discounting to create a probability distribution.,39
 What mix a discounted probability with a lower-order continuation probability?,interpolated Kneser-Ney smoothing algorithm,Kneser-Ney smoothing makes use of the probability of a word being a novel continuation. The interpolated Kneser-Ney smoothing algorithm mixes a discounted probability with a lower-order continuation probability.,92
 What model was introduced in this chapter?,Bayes model for classification,This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.,34
 What task did this chapter apply the Bayes model to?,text categorization task of sentiment analysis,This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.,87
 Many language processing tasks can be viewed as tasks of what?,classification,Many language processing tasks can be viewed as tasks of classification.,57
 Text categorization assigns an entire text a class from a finite set of what?,"sentiment analysis, spam detection, language identification, and authorship attribution","Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.",107
 What are some of the tasks that text categorisation includes?,"sentiment analysis, spam detection, language identification, and authorship attribution","Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.",107
 What classifies a text as reflecting the positive or negative orientation that a writer expresses toward some object?,Sentiment analysis,Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object.,0
 What is Naive Bayes a generative model that makes the bag of words assumption?,position doesn't matter,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),74
 What is the conditional independence assumption made?,words are conditionally independent of each other given the class,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),144
 How are words conditionally independent of each other?,given the class,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),194
 Naive Bayes with binarized features seems to work better for what?,many text classification tasks,Naive Bayes with binarized features seems to work better for many text classification tasks.,61
 What are classifiers evaluated based on?,precision and recall,Classifiers are evaluated based on precision and recall.,35
 How are classifiers trained?,"using distinct training, dev, and test sets","Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.",24
 What type of training is included in the training set?,cross-validation,"Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.",90
 What should be used to determine if one version of a classifier is better than another?,Statistical significance tests,Statistical significance tests should be used to determine whether we can be confident that one version of a classifier is better than another.,0
 What should designers of classifiers carefully consider?,harms that may be caused by the model,"Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",51
 What should classifier designers report in a model card?,model characteristics,"Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",151
 What model of classification was introduced in this chapter?,logistic regression,This chapter introduced the logistic regression model of classification.,28
 What is supervised machine learning classifier?,Logistic regression,"Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",0
 What is used to make a decision?,A threshold,"Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",228
 How does logistic regression extract real-valued features from input?,"multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability","Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",115
 How many classes can logistic regression be used with?,two,"Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).",37
 What is a good example of a multinomial logistic regression?,n-ary text classification,"Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).",164
 What function does multinomial logistic regression use to compute probabilities?,softmax,Multinomial logistic regression uses the softmax function to compute probabilities.,41
 What are the weights learned from a labeled training set via?,a loss function,"The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.",78
 What must be minimized?,cross-entropy loss,"The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.",107
 What is a convex optimization problem?,Minimizing this loss function,"Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.",0
 What is used to find the optimal weights?,iterative algorithms like gradient descent,"Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.",68
 Is regularization used to avoid overfitting?,Regularization is used to avoid overfitting,Regularization is used to avoid overfitting.,0
 What is one of the most useful analytic tools?,Logistic regression,"Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features.",0
 Logistic regression studies the importance of what?,individual features,"Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features.",131
" In vector semantics, a word is modeled as what?",a vector,"In vector semantics, a word is modeled as a vector-a point in high-dimensional space, also called an embedding. In this chapter we focus on static embeddings, in each each word is mapped to a fixed embedding.",42
 What is a point in high-dimensional space also called?,an embedding,"In vector semantics, a word is modeled as a vector-a point in high-dimensional space, also called an embedding. In this chapter we focus on static embeddings, in each each word is mapped to a fixed embedding.",98
 What are the two classes of vector semantic models?,sparse and dense,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",46
 What are cells in sparse models functions of?,co-occurrence counts,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",165
 How many sparse weightings are common?,Two,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",435
 What weighting weights each cell by its term frequency?,tf-idf,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",473
 What weighting weights each cell by its term frequency and inverse document frequency?,tf-idf,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",473
 What is most common for for word-context matrices?,PPMI (pointwise positive mutual information,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",572
 What is the dimensionality of dense vector models?,50-1000,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,40
 What is a popular way to compute dense embeddings?,Word2vec algorithms like skip-gram,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,49
 Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'?,Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,131
 What is computed from the dot product between the embeddings for two words?,probability that two words are 'likely to occur nearby in text'. This probability,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,196
 What does Skip-gram use to train the classifier?,stochastic gradient descent,"Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.",15
 What is a high dot product with embeddings of words that occur nearby?,embeddings,"Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.",80
 GloVe is a method based on what kind of probabilities?,word co-occurrence,"Other important embedding algorithms include GloVe, a method based on ratios of word co-occurrence probabilities.",80
 What are word and document similarities computed by?,some function of the dot product between vectors,"Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",86
 What is the most popular such metric?,The cosine of two vectors,"Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",136
 What are neural networks made out of?,neural units,"Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device.",33
 What were neural networks originally inspired by?,human neurons,"Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device.",70
" Each neural unit multiplies input values by a weight vector, adds a bias, and then applies what type of activation function?",non-linear,"Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit.",93
" In a fully connected, feedforward network, what is connected to each unit in layer i + 1?",each unit in layer i,"In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles.",43
 There are no cycles in what network?,feedforward network,"In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles.",22
 The power of neural networks comes from the ability of early layers to learn what?,representations,The power of neural networks comes from the ability of early layers to learn representations that can be utilized by later layers in the network.,77
 What are neural networks trained by?,optimization algorithms like gradient descent,Neural networks are trained by optimization algorithms like gradient descent.,31
 What is error backpropagation?,backward differentiation on a computation graph,"Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.",23
 What is backward differentiation on a computation graph used for?,compute the gradients of the loss function for a network,"Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.",83
 What do neural language models use as a probabilistic classifier?,a neural network,"Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words.",27
 What does a neural network compute?,the probability of the next word given the previous n words,"Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words.",86
 Pretrained embeddings can be used by what type of model?,Neural language models,"Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",0
 Neural language models can learn what from scratch?,embeddings,"Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",67
 What did this chapter introduce?,parts of speech and named entities,"This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:",24
 What tasks were introduced in this chapter?,partof-speech tagging and named entity recognition,"This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:",77
 How many part-of-speech tagsets exist?,between 40 and 200,"Languages generally have a small set of closed class words that are highly frequent, ambiguous, and act as function words, and open-class words like nouns, verbs, adjectives. Various part-of-speech tagsets exist, of between 40 and 200 tags.",216
 What is the process of assigning a part-of-speech label to a sequence of words?,Part-of-speech tagging,Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words.,0
 What are words for proper nouns?,Named entities,"Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.",0
" Named entities refer to people, places, and what else?",organizations,"Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.",82
 What are two common approaches to sequence modeling?,"generative approach, HMM tagging, and a discriminative approach, CRF tagging","Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",49
 What is a generative approach called?,HMM tagging,"Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",70
 How is CRF tagging defined?,discriminative,"Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",89
 How are the probabilities in HMM taggers estimated?,maximum likelihood estimation on tag-labeled training corpora,"The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence",50
" What algorithm is used for decoding, finding the most likely tag sequence?",Viterbi,"The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence",117
 What is another name for Conditional Random Fields?,CRF taggers,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",29
 What do CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence?,Conditional Random Fields,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",0
 What is the Viterbi algorithm used for?,inference,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",297
 What is another name for the Forward-Backward algorithm?,Viterbi algorithm,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",275
 What are recurrent neural networks and transformers?,language problems,This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:,118
 What are transformers used for?,language problems,This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:,118
 How are sequences processed in Recurrent Neural Networks?,one element at a time,"In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1.",60
 What is the output of each neural unit at time t based on?,the current input at t and the hidden layer from time t − 1,"In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1.",143
 How can RNNs be trained?,with a straightforward extension of the backpropagation algorithm,"RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).",20
 What is backpropagation through time?,BPTT,"RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).",126
 Why do simple recurrent networks fail on long inputs?,problems like vanishing gradients,Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers.,57
 Modern systems use more complex gated architectures like what?,LSTMs,Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers.,160
 LSTMs explicitly decide what to remember and forget in their hidden and context layers?,complex gated architectures,Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers.,124
 Transformers are non-recurrent networks based on what?,self-attention,"Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word.",49
 A selfattention layer maps input sequences to output sequences of the same length?,attention heads that each model how the surrounding words are relevant for the processing of the current word,"Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word.",166
 What is a transformer block composed of?,a single attention layer,A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks.,32
 What can be stacked to make deeper and more powerful networks?,Transformer blocks,A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks.,156
 What are some common language-based applications for RNNs and transformers?,:,Common language-based applications for RNNs and transformers include:,68
 What is Probabilistic language modeling?,"assigning a probability to a sequence, or to the next element of a sequence given the preceding words","Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",33
 What is automatic regressive generation using?,a trained language model,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",170
 What is assigned a label?,each element of a sequence,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",250
 What classification is used when an entire text is assigned to a category?,Sequence,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",299
 What is one of the most widely used applications of NLP?,Machine translation,"Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.",0
 The encoder-decoder model was first developed for what?,MT,"Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.",123
" Languages have divergences, both structural and what else?",lexical,"Languages have divergences, both structural and lexical, that make translation difficult.",48
 What field investigates some of these differences?,linguistic field of typology,The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects.,4
 Languages can be classified by their position along typological dimensions like whether verbs precede what?,their objects,The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects.,175
 What are encoder-decoder networks composed of?,an encoder network,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",75
 What is an encoder network that takes an input sequence and creates a contextualized representation of it?,Encoder-decoder networks,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",0
 The context representation is then passed to what?,a decoder,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",233
 What allows the decoder to view information from all the hidden states of the encoder?,attention mechanism in RNNs,"The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder.",4
 What is greedy decoding for a decoder?,choosing the single most probable token to generate at each step,"For the decoder, choosing the single most probable token to generate at each step is called greedy decoding.",17
 What is the name of the fixed-size memory footprint?,beam width,"In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width.",170
 Machine translation models are trained on what?,a parallel corpus,"Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages.",42
 A bitext is a text that appears in how many languages?,two,"Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages.",111
 What is backtranslation?,a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,19
 What is a way of making use of monolingual corpora in the target language?,Backtranslation,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,0
 How does a pilot MT engine create synthetic bitexts?,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,0
 What is MT evaluated by measuring?,a translation's adequacy (how well it captures the meaning of the source sentence) and fluency,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",29
 What is the gold standard?,Human evaluation,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",178
 Automatic evaluation metrics measure what?,character n-gram overlap with human translations,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",275
 What measure character n-gram overlap with human translations?,chrF,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",255
 What measure recent metrics based on embedding similarity?,chrF,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",255
