question,answer,context,start_pos
What is a fundamental tool in language processing?,the regular expression,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",67
What is the most important tool in language processing?,the regular expression,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",67
What is the most fundamental tool in language processing?,the regular expression,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",67
What is the most important tool for language processing?,the regular expression,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",67
What did this chapter show how to perform?,basic text normalization tasks,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",117
How did this chapter show how to perform?,basic text normalization tasks,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",117
What did the chapter show us how to do?,basic text normalization tasks,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",117
What is comparing strings?,minimum edit distance algorithm,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",265
What is the difference between strings?,minimum edit distance algorithm,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",265
What are the differences between strings?,minimum edit distance algorithm,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",265
What is the purpose of comparing strings?,minimum edit distance algorithm,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",265
What's the difference between strings?,minimum edit distance algorithm,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",265
What are the main points we covered about these ideas?,a summary,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",327
What are the main points we covered?,a summary,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",327
What are the main points of these ideas?,a summary,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",327
What are the main points about these ideas?,a summary,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",327
What are some of the main points we covered?,a summary,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",327
What is a powerful tool for pattern-matching?,The regular expression language,The regular expression language is a powerful tool for pattern-matching.,0
What is the most powerful tool for pattern matching?,The regular expression language,The regular expression language is a powerful tool for pattern-matching.,0
What is the most powerful tool for pattern-matching?,The regular expression language,The regular expression language is a powerful tool for pattern-matching.,0
What are some basic operations in regular expressions?,"concatenation of symbols, disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ, $) and precedence operators","Basic operations in regular expressions include concatenation of symbols, disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ, $) and precedence operators ((,) ).",48
What are some basic operations?,"concatenation of symbols, disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ, $) and precedence operators","Basic operations in regular expressions include concatenation of symbols, disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ, $) and precedence operators ((,) ).",48
What are the basic operations in regular expressions?,"concatenation of symbols, disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ, $) and precedence operators","Basic operations in regular expressions include concatenation of symbols, disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ, $) and precedence operators ((,) ).",48
What are some basic operations in regular expressions?,"concatenation of symbols, disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ, $) and precedence operators","Basic operations in regular expressions include concatenation of symbols, disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ, $) and precedence operators ((,) ).",48
Word tokenization and normalization are usually done by cascades of what?,simple regular expression substitutions or finite automata,Word tokenization and normalization are generally done by cascades of simple regular expression substitutions or finite automata.,70
What is a simple and efficient way to do stemming?,The Porter algorithm,"The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.",0
What is an efficient way to stemming?,The Porter algorithm,"The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.",0
What is a simple and efficient way to stemming?,The Porter algorithm,"The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.",0
What is a simple and efficient way of stemming?,The Porter algorithm,"The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.",0
What does the Porter algorithm strip off?,affixes,"The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.",81
What does the Porter algorithm do?,affixes,"The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.",81
What does the Porter algorithm not do?,affixes,"The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.",81
What is the Porter algorithm?,affixes,"The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.",81
What is the Porter algorithm used for?,affixes,"The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.",81
What is the minimum edit distance between two strings?,the minimum number of operations it takes to edit one into the other,"The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.",49
What is the minimum edit distance?,the minimum number of operations it takes to edit one into the other,"The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.",49
What is the minimum edit distance between strings?,the minimum number of operations it takes to edit one into the other,"The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.",49
What can be computed by dynamic programming?,Minimum edit distance,"The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.",119
What can be computed with dynamic programming?,Minimum edit distance,"The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.",119
What can be computed using dynamic programming?,Minimum edit distance,"The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.",119
What can be computed by dynamic programming?,Minimum edit distance,"The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.",119
What can be done with dynamic programming?,Minimum edit distance,"The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.",119
What is one of the most widely used tools in language processing?,n-gram,"This chapter introduced language modeling and the n-gram, one of the most widely used tools in language processing.",50
What is the most widely used language processing tool?,n-gram,"This chapter introduced language modeling and the n-gram, one of the most widely used tools in language processing.",50
What is the most used language processing tool?,n-gram,"This chapter introduced language modeling and the n-gram, one of the most widely used tools in language processing.",50
What is one of the most widely used language processing tools?,n-gram,"This chapter introduced language modeling and the n-gram, one of the most widely used tools in language processing.",50
One of the most widely used tools in language processing is what?,n-gram,"This chapter introduced language modeling and the n-gram, one of the most widely used tools in language processing.",50
Language models offer a way to assign a probability to a sentence or other sequence of words?,to predict a word from preceding words,"Language models offer a way to assign a probability to a sentence or other sequence of words, and to predict a word from preceding words.",98
What are Markov models that estimate words from a fixed window of previous words?,n-grams,n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).,0
What are the models that estimate words from a fixed window?,n-grams,n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).,0
What are the models that estimate the words from a fixed window?,n-grams,n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).,0
What are the models that estimate words from a window?,n-grams,n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).,0
What are the models that estimate the words from a window?,n-grams,n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).,0
How can n-gram probabilities be estimated?,by counting in a corpus and normalizing,n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).,123
How can the probabilities be calculated?,by counting in a corpus and normalizing,n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).,123
How can the probabilities be estimated?,by counting in a corpus and normalizing,n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).,123
How can the probabilities be determined?,by counting in a corpus and normalizing,n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).,123
How can we estimate the probabilities?,by counting in a corpus and normalizing,n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).,123
What are n-gram language models evaluated extrinsically in some task?,intrinsically using perplexity,"n-gram language models are evaluated extrinsically in some task, or intrinsically using perplexity.",68
What are the results of the evaluation of the n-gram language models?,intrinsically using perplexity,"n-gram language models are evaluated extrinsically in some task, or intrinsically using perplexity.",68
Is it possible to evaluate n-gram language models in a task?,intrinsically using perplexity,"n-gram language models are evaluated extrinsically in some task, or intrinsically using perplexity.",68
What are the consequences of evaluating n-gram language models in a task?,intrinsically using perplexity,"n-gram language models are evaluated extrinsically in some task, or intrinsically using perplexity.",68
What are n-gram language models evaluated for?,intrinsically using perplexity,"n-gram language models are evaluated extrinsically in some task, or intrinsically using perplexity.",68
What is the perplexity of a test set according to a language model?,the geometric mean of the inverse test set probability computed by the model,The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.,62
What is the perplexity of a test set?,the geometric mean of the inverse test set probability computed by the model,The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.,62
What is the perplexity of the test set?,the geometric mean of the inverse test set probability computed by the model,The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.,62
What is the perplexity of a test?,the geometric mean of the inverse test set probability computed by the model,The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.,62
What is the perplexity of a test set in a language?,the geometric mean of the inverse test set probability computed by the model,The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.,62
The geometric mean of the inverse test set probability computed by the model is what?,The perplexity,The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.,0
What is the geometric mean of the inverse test set probability?,The perplexity,The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.,0
What is the mean of the inverse test set probability?,The perplexity,The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.,0
What is the geometric mean of the inverse test set probabilities?,The perplexity,The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.,0
What is the geometric mean of inverse test set probability?,The perplexity,The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.,0
Smoothing algorithms provide a more sophisticated way to estimate the probability of what?,n-grams,Smoothing algorithms provide a more sophisticated way to estimate the probability of n-grams. Commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through backoff or interpolation.,85
Commonly used smoothing algorithms rely on lower-order n-gram counts through backoff or what else?,interpolation,Smoothing algorithms provide a more sophisticated way to estimate the probability of n-grams. Commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through backoff or interpolation.,194
What do both backoff and interpolation require to create a probability distribution?,discounting,Both backoff and interpolation require discounting to create a probability distribution.,39
What does it take to create a probability distribution?,discounting,Both backoff and interpolation require discounting to create a probability distribution.,39
What are the requirements to create a probability distribution?,discounting,Both backoff and interpolation require discounting to create a probability distribution.,39
What do you need to create a probability distribution?,discounting,Both backoff and interpolation require discounting to create a probability distribution.,39
What are the requirements for backoff and interpolation to create a probability distribution?,discounting,Both backoff and interpolation require discounting to create a probability distribution.,39
What mix a discounted probability with a lower-order continuation probability?,interpolated Kneser-Ney smoothing algorithm,Kneser-Ney smoothing makes use of the probability of a word being a novel continuation. The interpolated Kneser-Ney smoothing algorithm mixes a discounted probability with a lower-order continuation probability.,92
What is the mix of a discounted probability and a continuation probability?,interpolated Kneser-Ney smoothing algorithm,Kneser-Ney smoothing makes use of the probability of a word being a novel continuation. The interpolated Kneser-Ney smoothing algorithm mixes a discounted probability with a lower-order continuation probability.,92
What do you mean by a discounted probability and a lower-order continuation probability?,interpolated Kneser-Ney smoothing algorithm,Kneser-Ney smoothing makes use of the probability of a word being a novel continuation. The interpolated Kneser-Ney smoothing algorithm mixes a discounted probability with a lower-order continuation probability.,92
What is the mix of a discounted probability and a lower-order continuation probability?,interpolated Kneser-Ney smoothing algorithm,Kneser-Ney smoothing makes use of the probability of a word being a novel continuation. The interpolated Kneser-Ney smoothing algorithm mixes a discounted probability with a lower-order continuation probability.,92
What does it mean to mix a discounted probability with a lower-order continuation probability?,interpolated Kneser-Ney smoothing algorithm,Kneser-Ney smoothing makes use of the probability of a word being a novel continuation. The interpolated Kneser-Ney smoothing algorithm mixes a discounted probability with a lower-order continuation probability.,92
What model was introduced in this chapter?,Bayes model for classification,This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.,34
Which model was introduced in this chapter?,Bayes model for classification,This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.,34
What model was introduced in this chapter?,Bayes model for classification,This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.,34
What model was introduced?,Bayes model for classification,This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.,34
What is the model that was introduced in this chapter?,Bayes model for classification,This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.,34
What task did this chapter apply the Bayes model to?,text categorization task of sentiment analysis,This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.,87
What was the purpose of applying the Bayes model to this chapter?,text categorization task of sentiment analysis,This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.,87
What was the purpose of applying the Bayes model to?,text categorization task of sentiment analysis,This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.,87
Many language processing tasks can be viewed as tasks of what?,classification,Many language processing tasks can be viewed as tasks of classification.,57
Text categorization assigns an entire text a class from a finite set of what?,"sentiment analysis, spam detection, language identification, and authorship attribution","Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.",107
What are some of the tasks that text categorisation includes?,"sentiment analysis, spam detection, language identification, and authorship attribution","Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.",107
What tasks are included in text categorization?,"sentiment analysis, spam detection, language identification, and authorship attribution","Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.",107
What tasks are included in text categorisation?,"sentiment analysis, spam detection, language identification, and authorship attribution","Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.",107
What are some of the tasks that are included in text categorization?,"sentiment analysis, spam detection, language identification, and authorship attribution","Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.",107
What are some of the tasks that are included?,"sentiment analysis, spam detection, language identification, and authorship attribution","Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.",107
What classifies a text as reflecting the positive or negative orientation that a writer expresses toward some object?,Sentiment analysis,Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object.,0
What is the meaning of a text reflecting a positive or negative orientation?,Sentiment analysis,Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object.,0
What is the meaning of a text reflecting the positive or negative orientation that a writer expresses toward an object?,Sentiment analysis,Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object.,0
What is the meaning of a text reflecting the positive or negative orientation of an object?,Sentiment analysis,Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object.,0
What is Naive Bayes a generative model that makes the bag of words assumption?,position doesn't matter,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),74
What is the model that makes the bag of words assumption?,position doesn't matter,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),74
What is the generative model that makes the bag of words assumption?,position doesn't matter,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),74
What is the model that makes the bag of words assume?,position doesn't matter,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),74
What is the conditional independence assumption made?,words are conditionally independent of each other given the class,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),144
What is the independence assumption made?,words are conditionally independent of each other given the class,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),144
What is the independence assumption?,words are conditionally independent of each other given the class,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),144
Is the independence assumption made?,words are conditionally independent of each other given the class,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),144
How are words conditionally independent of each other?,given the class,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),194
How are words independent of each other?,given the class,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),194
How are words different?,given the class,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),194
Is it possible for words to be independent of each other?,given the class,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),194
Naive Bayes with binarized features seems to work better for what?,many text classification tasks,Naive Bayes with binarized features seems to work better for many text classification tasks.,61
Bayes with binarized features seem to work better for what?,many text classification tasks,Naive Bayes with binarized features seems to work better for many text classification tasks.,61
What are classifiers evaluated based on?,precision and recall,Classifiers are evaluated based on precision and recall.,35
What are the results of the evaluation?,precision and recall,Classifiers are evaluated based on precision and recall.,35
What are the results of the evaluation of classifiers?,precision and recall,Classifiers are evaluated based on precision and recall.,35
What are the results of the evaluation of the classifiers?,precision and recall,Classifiers are evaluated based on precision and recall.,35
What are the criteria for evaluating classifiers?,precision and recall,Classifiers are evaluated based on precision and recall.,35
How are classifiers trained?,"using distinct training, dev, and test sets","Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.",24
How are they trained?,"using distinct training, dev, and test sets","Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.",24
How are trained?,"using distinct training, dev, and test sets","Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.",24
What are the training methods for classifiers?,"using distinct training, dev, and test sets","Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.",24
How are the trained?,"using distinct training, dev, and test sets","Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.",24
What type of training is included in the training set?,cross-validation,"Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.",90
What type of training is included in the set?,cross-validation,"Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.",90
What training is included in the set?,cross-validation,"Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.",90
What kind of training is included in the set?,cross-validation,"Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.",90
What types of training are included in the set?,cross-validation,"Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.",90
What should be used to determine if one version of a classifier is better than another?,Statistical significance tests,Statistical significance tests should be used to determine whether we can be confident that one version of a classifier is better than another.,0
What should be used to determine if a version is better than another?,Statistical significance tests,Statistical significance tests should be used to determine whether we can be confident that one version of a classifier is better than another.,0
"If one version is better than the other, what should be used to determine it?",Statistical significance tests,Statistical significance tests should be used to determine whether we can be confident that one version of a classifier is better than another.,0
What should be used to determine if a version is better than the other version?,Statistical significance tests,Statistical significance tests should be used to determine whether we can be confident that one version of a classifier is better than another.,0
"If one version is better than the other, what should be done?",Statistical significance tests,Statistical significance tests should be used to determine whether we can be confident that one version of a classifier is better than another.,0
What should designers of classifiers carefully consider?,harms that may be caused by the model,"Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",51
What should be considered by the designers of classifiers?,harms that may be caused by the model,"Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",51
What should be considered by designers?,harms that may be caused by the model,"Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",51
What should be considered when designing a classifier?,harms that may be caused by the model,"Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",51
What should be considered by designers of classifiers?,harms that may be caused by the model,"Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",51
What should classifier designers report in a model card?,model characteristics,"Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",151
What should be reported in a model card?,model characteristics,"Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",151
What should the designers report in the card?,model characteristics,"Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",151
What should designers report in a model card?,model characteristics,"Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",151
What should the designers report in the model card?,model characteristics,"Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",151
What model of classification was introduced in this chapter?,logistic regression,This chapter introduced the logistic regression model of classification.,28
Which model of classification was introduced in this chapter?,logistic regression,This chapter introduced the logistic regression model of classification.,28
Which model of classification was introduced?,logistic regression,This chapter introduced the logistic regression model of classification.,28
What model of classification was introduced?,logistic regression,This chapter introduced the logistic regression model of classification.,28
What is the model of classification that was introduced in this chapter?,logistic regression,This chapter introduced the logistic regression model of classification.,28
What is supervised machine learning classifier?,Logistic regression,"Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",0
What is supervised machine learning?,Logistic regression,"Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",0
What is the difference between supervised and untrained machine learning?,Logistic regression,"Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",0
What is machine learning?,Logistic regression,"Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",0
What about machine learning?,Logistic regression,"Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",0
What is used to make a decision?,A threshold,"Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",228
What is used to make a decision?,A threshold,"Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",228
What is used when making a decision?,A threshold,"Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",228
What's used to make a decision?,A threshold,"Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",228
What is used in making a decision?,A threshold,"Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",228
How does logistic regression extract real-valued features from input?,"multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability","Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",115
How many classes can logistic regression be used with?,two,"Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).",37
What is a good example of a multinomial logistic regression?,n-ary text classification,"Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).",164
What is a good example of a regression?,n-ary text classification,"Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).",164
What is a good example of a multinomial regression?,n-ary text classification,"Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).",164
What is the best example of a multinomial regression?,n-ary text classification,"Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).",164
What is an example of a multinomial regression?,n-ary text classification,"Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).",164
What function does multinomial logistic regression use to compute probabilities?,softmax,Multinomial logistic regression uses the softmax function to compute probabilities.,41
What is the function that multinomial logistic regression uses to compute probabilities?,softmax,Multinomial logistic regression uses the softmax function to compute probabilities.,41
What is the function multinomial logistic regression uses to compute probabilities?,softmax,Multinomial logistic regression uses the softmax function to compute probabilities.,41
What does multinomial logistic regression do?,softmax,Multinomial logistic regression uses the softmax function to compute probabilities.,41
What is the function that multinomial Logistic Regression uses to compute probabilities?,softmax,Multinomial logistic regression uses the softmax function to compute probabilities.,41
What are the weights learned from a labeled training set via?,a loss function,"The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.",78
What are the weights learned from a training set?,a loss function,"The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.",78
What are the weights learned from training?,a loss function,"The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.",78
What are the weights learned from the training set?,a loss function,"The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.",78
What are the weights learned from a training regimen?,a loss function,"The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.",78
What must be minimized?,cross-entropy loss,"The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.",107
What should be minimized?,cross-entropy loss,"The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.",107
What must be minimized?,cross-entropy loss,"The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.",107
What needs to be minimized?,cross-entropy loss,"The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.",107
What can be minimized?,cross-entropy loss,"The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.",107
What is a convex optimization problem?,Minimizing this loss function,"Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.",0
What is the problem?,Minimizing this loss function,"Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.",0
Do you know what aconvex optimization problem is?,Minimizing this loss function,"Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.",0
What is used to find the optimal weights?,iterative algorithms like gradient descent,"Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.",68
How is it possible to find the optimal weights?,iterative algorithms like gradient descent,"Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.",68
What is used to find the best weights?,iterative algorithms like gradient descent,"Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.",68
What is used to find the optimal weights?,iterative algorithms like gradient descent,"Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.",68
What is used to find the right weights?,iterative algorithms like gradient descent,"Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.",68
Is regularization used to avoid overfitting?,Regularization is used to avoid overfitting,Regularization is used to avoid overfitting.,0
Is regularization used to avoid overfitting?,Regularization is used to avoid overfitting,Regularization is used to avoid overfitting.,0
Is regularization used to prevent overfitting?,Regularization is used to avoid overfitting,Regularization is used to avoid overfitting.,0
Is regularization done to avoid overfitting?,Regularization is used to avoid overfitting,Regularization is used to avoid overfitting.,0
What is one of the most useful analytic tools?,Logistic regression,"Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features.",0
What is the most useful analytic tool?,Logistic regression,"Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features.",0
One of the most useful analytic tools is what?,Logistic regression,"Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features.",0
One of the most useful analytic tools?,Logistic regression,"Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features.",0
What are the most useful analytic tools?,Logistic regression,"Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features.",0
Logistic regression studies the importance of what?,individual features,"Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features.",131
"In vector semantics, a word is modeled as what?",a vector,"In vector semantics, a word is modeled as a vector-a point in high-dimensional space, also called an embedding. In this chapter we focus on static embeddings, in each each word is mapped to a fixed embedding.",42
What is a point in high-dimensional space also called?,an embedding,"In vector semantics, a word is modeled as a vector-a point in high-dimensional space, also called an embedding. In this chapter we focus on static embeddings, in each each word is mapped to a fixed embedding.",98
What is a point in high-dimensional space?,an embedding,"In vector semantics, a word is modeled as a vector-a point in high-dimensional space, also called an embedding. In this chapter we focus on static embeddings, in each each word is mapped to a fixed embedding.",98
What is a point in space?,an embedding,"In vector semantics, a word is modeled as a vector-a point in high-dimensional space, also called an embedding. In this chapter we focus on static embeddings, in each each word is mapped to a fixed embedding.",98
What are the two classes of vector semantic models?,sparse and dense,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",46
What are the two classes of models?,sparse and dense,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",46
What are the two different classes of models?,sparse and dense,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",46
What are the classes of models?,sparse and dense,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",46
What are cells in sparse models functions of?,co-occurrence counts,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",165
What are the functions of cells in sparse models?,co-occurrence counts,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",165
What are the sparse models functions?,co-occurrence counts,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",165
What are the sparse models' functions?,co-occurrence counts,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",165
What are the sparse models functions of cells?,co-occurrence counts,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",165
How many sparse weightings are common?,Two,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",435
How many of them are common?,Two,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",435
What number of sparse weightings are common?,Two,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",435
What weighting weights each cell by its term frequency?,tf-idf,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",473
What is the weight of each cell by its term Frequency?,tf-idf,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",473
What weight is given to each cell by its term Frequency?,tf-idf,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",473
What weight is given to each cell's term Frequency?,tf-idf,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",473
What weight is given to each cell's term frequency?,tf-idf,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",473
What weighting weights each cell by its term frequency and inverse document frequency?,tf-idf,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",473
What is the weight of each cell by its term Frequency and inverse document Frequency?,tf-idf,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",473
What is the weight of each cell by its term and inverse document Frequency?,tf-idf,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",473
What is the weight of each cell by its term and inverse document frequencies?,tf-idf,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",473
What is the weight of each cell by its inverse document Frequency?,tf-idf,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",473
What is most common for for word-context matrices?,PPMI (pointwise positive mutual information,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",572
What are the most common word-context matrices?,PPMI (pointwise positive mutual information,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",572
What is the most common word-context matrix?,PPMI (pointwise positive mutual information,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",572
What is the average for word-context matrices?,PPMI (pointwise positive mutual information,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",572
What's the most common for word-context matrices?,PPMI (pointwise positive mutual information,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",572
What is the dimensionality of dense vector models?,50-1000,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,40
What is the density of the models?,50-1000,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,40
What is the density of models?,50-1000,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,40
What is the density of a model?,50-1000,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,40
What is the relative density of models?,50-1000,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,40
What is a popular way to compute dense embeddings?,Word2vec algorithms like skip-gram,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,49
What is the most popular way to compute dense embeddings?,Word2vec algorithms like skip-gram,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,49
Is there a way to compute dense embeddeds?,Word2vec algorithms like skip-gram,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,49
What is the most popular way to compute dense embeddeds?,Word2vec algorithms like skip-gram,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,49
What is the best way to compute dense embeddings?,Word2vec algorithms like skip-gram,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,49
Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'?,Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,131
What is computed from the dot product between the embeddings for two words?,probability that two words are 'likely to occur nearby in text'. This probability,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,196
What is computed from the dot product between the two words?,probability that two words are 'likely to occur nearby in text'. This probability,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,196
What is computed from the dot product when there are two words?,probability that two words are 'likely to occur nearby in text'. This probability,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,196
What is computed from the dot product between the words?,probability that two words are 'likely to occur nearby in text'. This probability,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,196
What is computed from the dot product?,probability that two words are 'likely to occur nearby in text'. This probability,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,196
What does Skip-gram use to train the classifier?,stochastic gradient descent,"Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.",15
What is Skip-gram used for?,stochastic gradient descent,"Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.",15
What does Skip-gram do to train?,stochastic gradient descent,"Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.",15
What does Skip-Gram do to train?,stochastic gradient descent,"Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.",15
What does Skip-gram do to train the class?,stochastic gradient descent,"Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.",15
What is a high dot product with embeddings of words that occur nearby?,embeddings,"Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.",80
What is a high dot product that has embedded words nearby?,embeddings,"Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.",80
What is a high dot product?,embeddings,"Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.",80
What is a high dot product that has words embedded in it?,embeddings,"Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.",80
What is a high dot product with embedded words?,embeddings,"Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.",80
GloVe is a method based on what kind of probabilities?,word co-occurrence,"Other important embedding algorithms include GloVe, a method based on ratios of word co-occurrence probabilities.",80
What kind of probabilities are used for GloVe?,word co-occurrence,"Other important embedding algorithms include GloVe, a method based on ratios of word co-occurrence probabilities.",80
What kind of probabilities do you use for GloVe?,word co-occurrence,"Other important embedding algorithms include GloVe, a method based on ratios of word co-occurrence probabilities.",80
What are word and document similarities computed by?,some function of the dot product between vectors,"Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",86
Word and document similarities are computed by what?,some function of the dot product between vectors,"Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",86
What is the most popular such metric?,The cosine of two vectors,"Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",136
What is the most popular metric?,The cosine of two vectors,"Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",136
Which is the most popular metric?,The cosine of two vectors,"Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",136
Which one is the most popular?,The cosine of two vectors,"Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",136
What's the most popular metric?,The cosine of two vectors,"Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",136
What are neural networks made out of?,neural units,"Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device.",33
What are neural networks made of?,neural units,"Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device.",33
What are neural networks made out of?,neural units,"Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device.",33
What were neural networks originally inspired by?,human neurons,"Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device.",70
What are the neural networks that inspired them?,human neurons,"Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device.",70
What are the original neural networks that inspired them?,human neurons,"Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device.",70
What were the original neural networks that inspired them?,human neurons,"Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device.",70
"Each neural unit multiplies input values by a weight vector, adds a bias, and then applies what type of activation function?",non-linear,"Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit.",93
What type of activation function is applied by each neural unit?,non-linear,"Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit.",93
What type of activation function is applied by each neural unit when it adds a bias?,non-linear,"Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit.",93
What type of activation function is applied by each neural unit when it adds a bias and multiplies input values?,non-linear,"Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit.",93
What type of activation function is applied by each neural unit when they add a bias?,non-linear,"Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit.",93
"In a fully connected, feedforward network, what is connected to each unit in layer i + 1?",each unit in layer i,"In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles.",43
What is connected to each unit in the feed forward network?,each unit in layer i,"In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles.",43
What is connected to each unit in the feedforward network?,each unit in layer i,"In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles.",43
What is connected to each unit in layer i + 1 in a feedforward network?,each unit in layer i,"In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles.",43
What is connected to each unit in layer i + 1 in a feed forward network?,each unit in layer i,"In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles.",43
There are no cycles in what network?,feedforward network,"In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles.",22
What network has no cycles?,feedforward network,"In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles.",22
What network have no cycles?,feedforward network,"In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles.",22
The power of neural networks comes from the ability of early layers to learn what?,representations,The power of neural networks comes from the ability of early layers to learn representations that can be utilized by later layers in the network.,77
What are neural networks trained by?,optimization algorithms like gradient descent,Neural networks are trained by optimization algorithms like gradient descent.,31
Neural networks are trained by what?,optimization algorithms like gradient descent,Neural networks are trained by optimization algorithms like gradient descent.,31
What is error backpropagation?,backward differentiation on a computation graph,"Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.",23
What is error backpropagation?,backward differentiation on a computation graph,"Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.",23
What does error backpropagation mean?,backward differentiation on a computation graph,"Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.",23
What is error backpagation?,backward differentiation on a computation graph,"Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.",23
Is error backpropagation a thing?,backward differentiation on a computation graph,"Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.",23
What is backward differentiation on a computation graph used for?,compute the gradients of the loss function for a network,"Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.",83
What is the difference between a graph and computation?,compute the gradients of the loss function for a network,"Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.",83
What is the difference between a graph and a computation?,compute the gradients of the loss function for a network,"Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.",83
What is the difference between a graph and a computation graph?,compute the gradients of the loss function for a network,"Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.",83
What do neural language models use as a probabilistic classifier?,a neural network,"Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words.",27
What are neural language models used for?,a neural network,"Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words.",27
What do neural language models do?,a neural network,"Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words.",27
What are neural language models used to do?,a neural network,"Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words.",27
What does a neural network compute?,the probability of the next word given the previous n words,"Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words.",86
What does a neural network do?,the probability of the next word given the previous n words,"Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words.",86
What does a network do?,the probability of the next word given the previous n words,"Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words.",86
How does a neural network work?,the probability of the next word given the previous n words,"Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words.",86
What do a neural network do?,the probability of the next word given the previous n words,"Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words.",86
Pretrained embeddings can be used by what type of model?,Neural language models,"Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",0
What type of model can pretrained embeddeds be used in?,Neural language models,"Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",0
What type of model can pretrained embeddings be used in?,Neural language models,"Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",0
What type of model can pretrained embeddeds be used for?,Neural language models,"Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",0
What type of model can trained embeddeds be used in?,Neural language models,"Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",0
Neural language models can learn what from scratch?,embeddings,"Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",67
What did this chapter introduce?,parts of speech and named entities,"This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:",24
What was introduced in this chapter?,parts of speech and named entities,"This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:",24
What did this chapter do?,parts of speech and named entities,"This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:",24
What happened in this chapter?,parts of speech and named entities,"This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:",24
What tasks were introduced in this chapter?,partof-speech tagging and named entity recognition,"This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:",77
Which tasks were introduced in this chapter?,partof-speech tagging and named entity recognition,"This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:",77
What are the new tasks in this chapter?,partof-speech tagging and named entity recognition,"This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:",77
What new tasks were introduced in this chapter?,partof-speech tagging and named entity recognition,"This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:",77
What are the tasks that were introduced in this chapter?,partof-speech tagging and named entity recognition,"This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:",77
How many part-of-speech tagsets exist?,between 40 and 200,"Languages generally have a small set of closed class words that are highly frequent, ambiguous, and act as function words, and open-class words like nouns, verbs, adjectives. Various part-of-speech tagsets exist, of between 40 and 200 tags.",216
What is the process of assigning a part-of-speech label to a sequence of words?,Part-of-speech tagging,Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words.,0
How do you assign a part-of-speech label to a sequence of words?,Part-of-speech tagging,Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words.,0
Is it possible to assign a part-of-speech label to a sequence of words?,Part-of-speech tagging,Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words.,0
What is the process of assigning a part-of-speech label to a sequence of words?,Part-of-speech tagging,Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words.,0
Is there a process for assigning a part-of-speech label to a sequence of words?,Part-of-speech tagging,Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words.,0
What are words for proper nouns?,Named entities,"Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.",0
What words are used for proper nouns?,Named entities,"Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.",0
What are the words for proper nouns?,Named entities,"Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.",0
"Named entities refer to people, places, and what else?",organizations,"Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.",82
"What about people, places, and what not?",organizations,"Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.",82
What are named entities?,organizations,"Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.",82
What are named entities and what do they mean?,organizations,"Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.",82
What else are named entities?,organizations,"Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.",82
What are two common approaches to sequence modeling?,"generative approach, HMM tagging, and a discriminative approach, CRF tagging","Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",49
What are the most common approaches to sequence modeling?,"generative approach, HMM tagging, and a discriminative approach, CRF tagging","Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",49
What are some common approaches to sequence modeling?,"generative approach, HMM tagging, and a discriminative approach, CRF tagging","Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",49
What is a generative approach called?,HMM tagging,"Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",70
What is a generative approach?,HMM tagging,"Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",70
What is agenerative approach?,HMM tagging,"Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",70
How is CRF tagging defined?,discriminative,"Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",89
How is it defined?,discriminative,"Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",89
What is the definition of CRF tagging?,discriminative,"Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",89
How is this defined?,discriminative,"Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",89
How is the tag defined?,discriminative,"Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",89
How are the probabilities in HMM taggers estimated?,maximum likelihood estimation on tag-labeled training corpora,"The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence",50
What are the probabilities in taggers?,maximum likelihood estimation on tag-labeled training corpora,"The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence",50
What are the probabilities in HMM taggers?,maximum likelihood estimation on tag-labeled training corpora,"The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence",50
How are the probabilities calculated?,maximum likelihood estimation on tag-labeled training corpora,"The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence",50
How are the probabilities for taggers estimated?,maximum likelihood estimation on tag-labeled training corpora,"The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence",50
"What algorithm is used for decoding, finding the most likely tag sequence?",Viterbi,"The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence",117
How do you find the most likely tag sequence?,Viterbi,"The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence",117
What is the best way to find the most likely tag sequence?,Viterbi,"The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence",117
What is the most likely sequence for decoding?,Viterbi,"The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence",117
What is used to find the most likely tag sequence?,Viterbi,"The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence",117
What is another name for Conditional Random Fields?,CRF taggers,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",29
What is the name of the field?,CRF taggers,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",29
What is another name for the field?,CRF taggers,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",29
What is another name for Random Fields?,CRF taggers,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",29
What is another name for random fields?,CRF taggers,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",29
What do CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence?,Conditional Random Fields,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",0
What does a log-linear model do to choose the best tag sequence?,Conditional Random Fields,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",0
What is the Viterbi algorithm used for?,inference,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",297
What is the purpose of the Viterbi algorithm?,inference,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",297
What is it used for?,inference,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",297
What is the use of the Viterbi algorithm?,inference,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",297
What is the purpose of the Viterbi Algorithm?,inference,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",297
What is another name for the Forward-Backward algorithm?,Viterbi algorithm,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",275
Is there another name for the Forward- Backward algorithm?,Viterbi algorithm,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",275
What is another name for the Forward- Backward algorithm?,Viterbi algorithm,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",275
Is there another name for the forward-backward algorithm?,Viterbi algorithm,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",275
What are recurrent neural networks and transformers?,language problems,This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:,118
What are neural networks?,language problems,This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:,118
What are neural networks and transformers?,language problems,This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:,118
What are recurrent neural networks?,language problems,This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:,118
What are transformers used for?,language problems,This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:,118
What are they used for?,language problems,This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:,118
What are the uses of transformers?,language problems,This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:,118
What are the uses of transformer?,language problems,This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:,118
What do transformers do?,language problems,This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:,118
How are sequences processed in Recurrent Neural Networks?,one element at a time,"In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1.",60
What is the output of each neural unit at time t based on?,the current input at t and the hidden layer from time t − 1,"In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1.",143
What is the output of each neural unit?,the current input at t and the hidden layer from time t − 1,"In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1.",143
What is the output of the neural units?,the current input at t and the hidden layer from time t − 1,"In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1.",143
What is the output of each neural unit at that time?,the current input at t and the hidden layer from time t − 1,"In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1.",143
What is the output of each unit?,the current input at t and the hidden layer from time t − 1,"In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1.",143
How can RNNs be trained?,with a straightforward extension of the backpropagation algorithm,"RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).",20
How can they be trained?,with a straightforward extension of the backpropagation algorithm,"RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).",20
How can RNNs be trained?,with a straightforward extension of the backpropagation algorithm,"RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).",20
How can we train RNNs?,with a straightforward extension of the backpropagation algorithm,"RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).",20
What is backpropagation through time?,BPTT,"RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).",126
What is backpropagation?,BPTT,"RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).",126
"Backpropagation through time, what is it?",BPTT,"RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).",126
What is the history of backpropagation through time?,BPTT,"RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).",126
What is the nature of backpropagation through time?,BPTT,"RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).",126
Why do simple recurrent networks fail on long inputs?,problems like vanishing gradients,Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers.,57
Modern systems use more complex gated architectures like what?,LSTMs,Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers.,160
LSTMs explicitly decide what to remember and forget in their hidden and context layers?,complex gated architectures,Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers.,124
"In their hidden and context layers, what to remember and forget?",complex gated architectures,Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers.,124
What to remember and forget in the hidden and context layers?,complex gated architectures,Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers.,124
"In their hidden and context layers, what to remember and what to forget?",complex gated architectures,Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers.,124
Transformers are non-recurrent networks based on what?,self-attention,"Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word.",49
What is the basis for non-recurrent networks?,self-attention,"Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word.",49
A selfattention layer maps input sequences to output sequences of the same length?,attention heads that each model how the surrounding words are relevant for the processing of the current word,"Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word.",166
What is a transformer block composed of?,a single attention layer,A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks.,32
What is a transformer block made of?,a single attention layer,A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks.,32
What is the composition of a transformer block?,a single attention layer,A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks.,32
What is the structure of a transformer block?,a single attention layer,A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks.,32
What can be stacked to make deeper and more powerful networks?,Transformer blocks,A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks.,156
What can be done to make networks more powerful?,Transformer blocks,A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks.,156
What can be stacked to make networks more powerful?,Transformer blocks,A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks.,156
What are some common language-based applications for RNNs and transformers?,:,Common language-based applications for RNNs and transformers include:,68
What are some language-based applications for transformers?,:,Common language-based applications for RNNs and transformers include:,68
What are some common language-based applications for transformers?,:,Common language-based applications for RNNs and transformers include:,68
What are some common language-based applications?,:,Common language-based applications for RNNs and transformers include:,68
What are some common language applications for transformers?,:,Common language-based applications for RNNs and transformers include:,68
What is Probabilistic language modeling?,"assigning a probability to a sequence, or to the next element of a sequence given the preceding words","Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",33
What is probabilistic language modeling?,"assigning a probability to a sequence, or to the next element of a sequence given the preceding words","Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",33
What does probabilistic language modeling do?,"assigning a probability to a sequence, or to the next element of a sequence given the preceding words","Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",33
What is Probabilistic language modeling?,"assigning a probability to a sequence, or to the next element of a sequence given the preceding words","Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",33
What is automatic regressive generation using?,a trained language model,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",170
What is the use of automatic regressive generation?,a trained language model,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",170
What is automatic regressive generation used for?,a trained language model,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",170
What is the automatic regressive generation used for?,a trained language model,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",170
What is the automatic generation used for?,a trained language model,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",170
What is assigned a label?,each element of a sequence,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",250
What is written on a label?,each element of a sequence,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",250
What is the assigned label?,each element of a sequence,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",250
What is written on the label?,each element of a sequence,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",250
What classification is used when an entire text is assigned to a category?,Sequence,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",299
"When a text is assigned to a category, what classification is used?",Sequence,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",299
"When an entire text is assigned to a category, what classification is used?",Sequence,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",299
"When a text is assigned to a category, what is the classification used?",Sequence,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",299
"When an entire text is assigned to a category, what is the classification used?",Sequence,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",299
What is one of the most widely used applications of NLP?,Machine translation,"Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.",0
What is one of the most used applications?,Machine translation,"Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.",0
What is one of the most widely used applications?,Machine translation,"Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.",0
What is one of the most popular applications?,Machine translation,"Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.",0
One of the most widely used applications?,Machine translation,"Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.",0
The encoder-decoder model was first developed for what?,MT,"Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.",123
What is the first thing the model was developed for?,MT,"Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.",123
What was the first thing the model was developed for?,MT,"Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.",123
What was the first thing that was done with the model?,MT,"Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.",123
"Languages have divergences, both structural and what else?",lexical,"Languages have divergences, both structural and lexical, that make translation difficult.",48
What field investigates some of these differences?,linguistic field of typology,The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects.,4
Languages can be classified by their position along typological dimensions like whether verbs precede what?,their objects,The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects.,175
What are encoder-decoder networks composed of?,an encoder network,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",75
What are the networks composed of?,an encoder network,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",75
What are the networks made of?,an encoder network,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",75
What are the networks made up of?,an encoder network,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",75
What are encoder-decoder networks made of?,an encoder network,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",75
What is an encoder network that takes an input sequence and creates a contextualized representation of it?,Encoder-decoder networks,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",0
What is a network that takes an input sequence and creates a representation of it?,Encoder-decoder networks,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",0
What is a network that takes an input sequence and makes a representation of it?,Encoder-decoder networks,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",0
What is an input sequence that is used to create a representation of it?,Encoder-decoder networks,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",0
What is a network that takes an input sequence and makes a representation out of it?,Encoder-decoder networks,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",0
The context representation is then passed to what?,a decoder,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",233
What is then passed to the context representation?,a decoder,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",233
What is the context representation passed to?,a decoder,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",233
What allows the decoder to view information from all the hidden states of the encoder?,attention mechanism in RNNs,"The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder.",4
What can the decoder see from the hidden states of the encoder?,attention mechanism in RNNs,"The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder.",4
What can the decoder see from the hidden states of the encoder?,attention mechanism in RNNs,"The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder.",4
What makes it possible for the decoder to see all the hidden states of the encoder?,attention mechanism in RNNs,"The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder.",4
What makes it possible for the decoder to see all the hidden states of the encoder?,attention mechanism in RNNs,"The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder.",4
What is greedy decoding for a decoder?,choosing the single most probable token to generate at each step,"For the decoder, choosing the single most probable token to generate at each step is called greedy decoding.",17
What is greedy decoding?,choosing the single most probable token to generate at each step,"For the decoder, choosing the single most probable token to generate at each step is called greedy decoding.",17
What is the purpose of decoding?,choosing the single most probable token to generate at each step,"For the decoder, choosing the single most probable token to generate at each step is called greedy decoding.",17
What does greedy decoding mean?,choosing the single most probable token to generate at each step,"For the decoder, choosing the single most probable token to generate at each step is called greedy decoding.",17
What is the reason for greedy decoding?,choosing the single most probable token to generate at each step,"For the decoder, choosing the single most probable token to generate at each step is called greedy decoding.",17
What is the name of the fixed-size memory footprint?,beam width,"In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width.",170
What is the name of the memory footprint?,beam width,"In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width.",170
What is the name of the footprint?,beam width,"In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width.",170
What is the name of the fixed-size memory footprint?,beam width,"In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width.",170
Machine translation models are trained on what?,a parallel corpus,"Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages.",42
What are the machine translation models trained on?,a parallel corpus,"Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages.",42
What are the machine translation models trained to do?,a parallel corpus,"Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages.",42
A bitext is a text that appears in how many languages?,two,"Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages.",111
How many languages does a bitext appear in?,two,"Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages.",111
How many languages has a bitext appeared in?,two,"Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages.",111
What is backtranslation?,a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,19
What is backtranslation?,a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,19
"Backtranslation, what is it?",a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,19
What does backtranslation mean?,a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,19
What is a way of making use of monolingual corpora in the target language?,Backtranslation,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,0
How can monolingual corpora be used in the target language?,Backtranslation,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,0
Is there a way to use monolingual corpora in the target language?,Backtranslation,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,0
What is the best way to use monolingual corpora in a language?,Backtranslation,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,0
What is the best way to use monolingual corpora in the target language?,Backtranslation,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,0
How does a pilot MT engine create synthetic bitexts?,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,0
How does a pilot engine work?,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,0
How does a pilot engine create something?,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,0
How does a pilot engine create synthetic bits?,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,0
How does a pilot engine make synthetic bits?,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,0
What is MT evaluated by measuring?,a translation's adequacy (how well it captures the meaning of the source sentence) and fluency,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",29
What is evaluated by measuring?,a translation's adequacy (how well it captures the meaning of the source sentence) and fluency,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",29
What is assessed by measuring?,a translation's adequacy (how well it captures the meaning of the source sentence) and fluency,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",29
What is the evaluation of MT done by measuring?,a translation's adequacy (how well it captures the meaning of the source sentence) and fluency,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",29
What is the purpose of measuring?,a translation's adequacy (how well it captures the meaning of the source sentence) and fluency,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",29
What is the gold standard?,Human evaluation,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",178
What is the gold standard?,Human evaluation,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",178
What is the standard for gold?,Human evaluation,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",178
"The gold standard, what is it?",Human evaluation,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",178
What's the gold standard?,Human evaluation,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",178
Automatic evaluation metrics measure what?,character n-gram overlap with human translations,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",275
Automatic evaluation metrics measure what?,character n-gram overlap with human translations,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",275
What metrics measure what?,character n-gram overlap with human translations,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",275
Automatic evaluation metrics measure?,character n-gram overlap with human translations,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",275
What measure character n-gram overlap with human translations?,chrF,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",255
What is the correlation between character n-gram and human translations?,chrF,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",255
What is the difference between character n-gram and human translations?,chrF,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",255
What measure recent metrics based on embedding similarity?,chrF,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",255
What are the recent metrics based on similarity?,chrF,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",255
What are the recent metrics that are based on similarity?,chrF,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",255
What are the metrics based on similarity?,chrF,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",255
Is there a measure for recent metrics based on similarity?,chrF,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",255
