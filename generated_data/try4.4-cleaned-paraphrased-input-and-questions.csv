question,answer,context,start_pos
What is a fundamental tool in language processing?,the regular expression,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",67
What is the most important tool in language processing?,the regular expression,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",67
What is the most fundamental tool in language processing?,the regular expression,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",67
What is the most important tool for language processing?,the regular expression,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",67
What did this chapter show how to perform?,basic text normalization tasks,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",117
How did this chapter show how to perform?,basic text normalization tasks,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",117
What did the chapter show us how to do?,basic text normalization tasks,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",117
What is comparing strings?,minimum edit distance algorithm,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",265
What is the difference between strings?,minimum edit distance algorithm,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",265
What are the differences between strings?,minimum edit distance algorithm,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",265
What is the purpose of comparing strings?,minimum edit distance algorithm,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",265
What's the difference between strings?,minimum edit distance algorithm,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",265
What are the main points we covered about these ideas?,a summary,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",327
What are the main points we covered?,a summary,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",327
What are the main points of these ideas?,a summary,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",327
What are the main points about these ideas?,a summary,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",327
What are some of the main points we covered?,a summary,"This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here's a summary of the main points we covered about these ideas:",327
What is a powerful tool for pattern-matching?,The regular expression language,The regular expression language is a powerful tool for pattern-matching.,0
What is the most powerful tool for pattern matching?,The regular expression language,The regular expression language is a powerful tool for pattern-matching.,0
What is the most powerful tool for pattern-matching?,The regular expression language,The regular expression language is a powerful tool for pattern-matching.,0
What are some basic operations in regular expressions?,"concatenation of symbols, disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ, $) and precedence operators","Basic operations in regular expressions include concatenation of symbols, disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ, $) and precedence operators ((,) ).",48
What are some basic operations?,"concatenation of symbols, disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ, $) and precedence operators","Basic operations in regular expressions include concatenation of symbols, disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ, $) and precedence operators ((,) ).",48
What are the basic operations in regular expressions?,"concatenation of symbols, disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ, $) and precedence operators","Basic operations in regular expressions include concatenation of symbols, disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ, $) and precedence operators ((,) ).",48
What are some basic operations in regular expressions?,"concatenation of symbols, disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ, $) and precedence operators","Basic operations in regular expressions include concatenation of symbols, disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ, $) and precedence operators ((,) ).",48
Word tokenization and normalization are usually done by cascades of what?,simple regular expression substitutions or finite automata,Word tokenization and normalization are generally done by cascades of simple regular expression substitutions or finite automata.,70
What is a simple and efficient way to do stemming?,The Porter algorithm,"The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.",0
What is an efficient way to stemming?,The Porter algorithm,"The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.",0
What is a simple and efficient way to stemming?,The Porter algorithm,"The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.",0
What is a simple and efficient way of stemming?,The Porter algorithm,"The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.",0
What does the Porter algorithm strip off?,affixes,"The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.",81
What does the Porter algorithm do?,affixes,"The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.",81
What does the Porter algorithm not do?,affixes,"The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.",81
What is the Porter algorithm?,affixes,"The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.",81
What is the Porter algorithm used for?,affixes,"The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.",81
What is the minimum edit distance between two strings?,the minimum number of operations it takes to edit one into the other,"The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.",49
What is the minimum edit distance?,the minimum number of operations it takes to edit one into the other,"The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.",49
What is the minimum edit distance between strings?,the minimum number of operations it takes to edit one into the other,"The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.",49
What can be computed by dynamic programming?,Minimum edit distance,"The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.",119
What can be computed with dynamic programming?,Minimum edit distance,"The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.",119
What can be computed using dynamic programming?,Minimum edit distance,"The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.",119
What can be computed by dynamic programming?,Minimum edit distance,"The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.",119
What can be done with dynamic programming?,Minimum edit distance,"The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.",119
What is one of the most widely used tools in language processing?,n-gram,"This chapter introduced language modeling and the n-gram, one of the most widely used tools in language processing.",50
What is the most widely used language processing tool?,n-gram,"This chapter introduced language modeling and the n-gram, one of the most widely used tools in language processing.",50
What is the most used language processing tool?,n-gram,"This chapter introduced language modeling and the n-gram, one of the most widely used tools in language processing.",50
What is one of the most widely used language processing tools?,n-gram,"This chapter introduced language modeling and the n-gram, one of the most widely used tools in language processing.",50
One of the most widely used tools in language processing is what?,n-gram,"This chapter introduced language modeling and the n-gram, one of the most widely used tools in language processing.",50
Language models offer a way to assign a probability to a sentence or other sequence of words?,to predict a word from preceding words,"Language models offer a way to assign a probability to a sentence or other sequence of words, and to predict a word from preceding words.",98
What are Markov models that estimate words from a fixed window of previous words?,n-grams,n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).,0
What are the models that estimate words from a fixed window?,n-grams,n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).,0
What are the models that estimate the words from a fixed window?,n-grams,n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).,0
What are the models that estimate words from a window?,n-grams,n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).,0
What are the models that estimate the words from a window?,n-grams,n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).,0
How can n-gram probabilities be estimated?,by counting in a corpus and normalizing,n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).,123
How can the probabilities be calculated?,by counting in a corpus and normalizing,n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).,123
How can the probabilities be estimated?,by counting in a corpus and normalizing,n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).,123
How can the probabilities be determined?,by counting in a corpus and normalizing,n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).,123
How can we estimate the probabilities?,by counting in a corpus and normalizing,n-grams are Markov models that estimate words from a fixed window of previous words. n-gram probabilities can be estimated by counting in a corpus and normalizing (the maximum likelihood estimate).,123
What are n-gram language models evaluated extrinsically in some task?,intrinsically using perplexity,"n-gram language models are evaluated extrinsically in some task, or intrinsically using perplexity.",68
What are the results of the evaluation of the n-gram language models?,intrinsically using perplexity,"n-gram language models are evaluated extrinsically in some task, or intrinsically using perplexity.",68
Is it possible to evaluate n-gram language models in a task?,intrinsically using perplexity,"n-gram language models are evaluated extrinsically in some task, or intrinsically using perplexity.",68
What are the consequences of evaluating n-gram language models in a task?,intrinsically using perplexity,"n-gram language models are evaluated extrinsically in some task, or intrinsically using perplexity.",68
What are n-gram language models evaluated for?,intrinsically using perplexity,"n-gram language models are evaluated extrinsically in some task, or intrinsically using perplexity.",68
What is the perplexity of a test set according to a language model?,the geometric mean of the inverse test set probability computed by the model,The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.,62
What is the perplexity of a test set?,the geometric mean of the inverse test set probability computed by the model,The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.,62
What is the perplexity of the test set?,the geometric mean of the inverse test set probability computed by the model,The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.,62
What is the perplexity of a test?,the geometric mean of the inverse test set probability computed by the model,The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.,62
What is the perplexity of a test set in a language?,the geometric mean of the inverse test set probability computed by the model,The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.,62
The geometric mean of the inverse test set probability computed by the model is what?,The perplexity,The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.,0
What is the geometric mean of the inverse test set probability?,The perplexity,The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.,0
What is the mean of the inverse test set probability?,The perplexity,The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.,0
What is the geometric mean of the inverse test set probabilities?,The perplexity,The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.,0
What is the geometric mean of inverse test set probability?,The perplexity,The perplexity of a test set according to a language model is the geometric mean of the inverse test set probability computed by the model.,0
Smoothing algorithms provide a more sophisticated way to estimate the probability of what?,n-grams,Smoothing algorithms provide a more sophisticated way to estimate the probability of n-grams. Commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through backoff or interpolation.,85
Commonly used smoothing algorithms rely on lower-order n-gram counts through backoff or what else?,interpolation,Smoothing algorithms provide a more sophisticated way to estimate the probability of n-grams. Commonly used smoothing algorithms for n-grams rely on lower-order n-gram counts through backoff or interpolation.,194
What do both backoff and interpolation require to create a probability distribution?,discounting,Both backoff and interpolation require discounting to create a probability distribution.,39
What does it take to create a probability distribution?,discounting,Both backoff and interpolation require discounting to create a probability distribution.,39
What are the requirements to create a probability distribution?,discounting,Both backoff and interpolation require discounting to create a probability distribution.,39
What do you need to create a probability distribution?,discounting,Both backoff and interpolation require discounting to create a probability distribution.,39
What are the requirements for backoff and interpolation to create a probability distribution?,discounting,Both backoff and interpolation require discounting to create a probability distribution.,39
What mix a discounted probability with a lower-order continuation probability?,interpolated Kneser-Ney smoothing algorithm,Kneser-Ney smoothing makes use of the probability of a word being a novel continuation. The interpolated Kneser-Ney smoothing algorithm mixes a discounted probability with a lower-order continuation probability.,92
What is the mix of a discounted probability and a continuation probability?,interpolated Kneser-Ney smoothing algorithm,Kneser-Ney smoothing makes use of the probability of a word being a novel continuation. The interpolated Kneser-Ney smoothing algorithm mixes a discounted probability with a lower-order continuation probability.,92
What do you mean by a discounted probability and a lower-order continuation probability?,interpolated Kneser-Ney smoothing algorithm,Kneser-Ney smoothing makes use of the probability of a word being a novel continuation. The interpolated Kneser-Ney smoothing algorithm mixes a discounted probability with a lower-order continuation probability.,92
What is the mix of a discounted probability and a lower-order continuation probability?,interpolated Kneser-Ney smoothing algorithm,Kneser-Ney smoothing makes use of the probability of a word being a novel continuation. The interpolated Kneser-Ney smoothing algorithm mixes a discounted probability with a lower-order continuation probability.,92
What does it mean to mix a discounted probability with a lower-order continuation probability?,interpolated Kneser-Ney smoothing algorithm,Kneser-Ney smoothing makes use of the probability of a word being a novel continuation. The interpolated Kneser-Ney smoothing algorithm mixes a discounted probability with a lower-order continuation probability.,92
What model was introduced in this chapter?,Bayes model for classification,This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.,34
Which model was introduced in this chapter?,Bayes model for classification,This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.,34
What model was introduced in this chapter?,Bayes model for classification,This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.,34
What model was introduced?,Bayes model for classification,This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.,34
What is the model that was introduced in this chapter?,Bayes model for classification,This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.,34
What task did this chapter apply the Bayes model to?,text categorization task of sentiment analysis,This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.,87
What was the purpose of applying the Bayes model to this chapter?,text categorization task of sentiment analysis,This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.,87
What was the purpose of applying the Bayes model to?,text categorization task of sentiment analysis,This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.,87
Many language processing tasks can be viewed as tasks of what?,classification,Many language processing tasks can be viewed as tasks of classification.,57
Text categorization assigns an entire text a class from a finite set of what?,"sentiment analysis, spam detection, language identification, and authorship attribution","Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.",107
What are some of the tasks that text categorisation includes?,"sentiment analysis, spam detection, language identification, and authorship attribution","Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.",107
What tasks are included in text categorization?,"sentiment analysis, spam detection, language identification, and authorship attribution","Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.",107
What tasks are included in text categorisation?,"sentiment analysis, spam detection, language identification, and authorship attribution","Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.",107
What are some of the tasks that are included in text categorization?,"sentiment analysis, spam detection, language identification, and authorship attribution","Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.",107
What are some of the tasks that are included?,"sentiment analysis, spam detection, language identification, and authorship attribution","Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.",107
What classifies a text as reflecting the positive or negative orientation that a writer expresses toward some object?,Sentiment analysis,Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object.,0
What is the meaning of a text reflecting a positive or negative orientation?,Sentiment analysis,Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object.,0
What is the meaning of a text reflecting the positive or negative orientation that a writer expresses toward an object?,Sentiment analysis,Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object.,0
What is the meaning of a text reflecting the positive or negative orientation of an object?,Sentiment analysis,Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object.,0
What is Naive Bayes a generative model that makes the bag of words assumption?,position doesn't matter,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),74
What is the model that makes the bag of words assumption?,position doesn't matter,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),74
What is the generative model that makes the bag of words assumption?,position doesn't matter,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),74
What is the model that makes the bag of words assume?,position doesn't matter,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),74
What is the conditional independence assumption made?,words are conditionally independent of each other given the class,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),144
What is the independence assumption made?,words are conditionally independent of each other given the class,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),144
What is the independence assumption?,words are conditionally independent of each other given the class,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),144
Is the independence assumption made?,words are conditionally independent of each other given the class,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),144
How are words conditionally independent of each other?,given the class,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),194
How are words independent of each other?,given the class,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),194
How are words different?,given the class,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),194
Is it possible for words to be independent of each other?,given the class,Naive Bayes is a generative model that makes the bag of words assumption (position doesn't matter) and the conditional independence assumption (words are conditionally independent of each other given the class),194
Naive Bayes with binarized features seems to work better for what?,many text classification tasks,Naive Bayes with binarized features seems to work better for many text classification tasks.,61
Bayes with binarized features seem to work better for what?,many text classification tasks,Naive Bayes with binarized features seems to work better for many text classification tasks.,61
What are classifiers evaluated based on?,precision and recall,Classifiers are evaluated based on precision and recall.,35
What are the results of the evaluation?,precision and recall,Classifiers are evaluated based on precision and recall.,35
What are the results of the evaluation of classifiers?,precision and recall,Classifiers are evaluated based on precision and recall.,35
What are the results of the evaluation of the classifiers?,precision and recall,Classifiers are evaluated based on precision and recall.,35
What are the criteria for evaluating classifiers?,precision and recall,Classifiers are evaluated based on precision and recall.,35
How are classifiers trained?,"using distinct training, dev, and test sets","Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.",24
How are they trained?,"using distinct training, dev, and test sets","Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.",24
How are trained?,"using distinct training, dev, and test sets","Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.",24
What are the training methods for classifiers?,"using distinct training, dev, and test sets","Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.",24
How are the trained?,"using distinct training, dev, and test sets","Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.",24
What type of training is included in the training set?,cross-validation,"Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.",90
What type of training is included in the set?,cross-validation,"Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.",90
What training is included in the set?,cross-validation,"Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.",90
What kind of training is included in the set?,cross-validation,"Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.",90
What types of training are included in the set?,cross-validation,"Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.",90
What should be used to determine if one version of a classifier is better than another?,Statistical significance tests,Statistical significance tests should be used to determine whether we can be confident that one version of a classifier is better than another.,0
What should be used to determine if a version is better than another?,Statistical significance tests,Statistical significance tests should be used to determine whether we can be confident that one version of a classifier is better than another.,0
"If one version is better than the other, what should be used to determine it?",Statistical significance tests,Statistical significance tests should be used to determine whether we can be confident that one version of a classifier is better than another.,0
What should be used to determine if a version is better than the other version?,Statistical significance tests,Statistical significance tests should be used to determine whether we can be confident that one version of a classifier is better than another.,0
"If one version is better than the other, what should be done?",Statistical significance tests,Statistical significance tests should be used to determine whether we can be confident that one version of a classifier is better than another.,0
What should designers of classifiers carefully consider?,harms that may be caused by the model,"Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",51
What should be considered by the designers of classifiers?,harms that may be caused by the model,"Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",51
What should be considered by designers?,harms that may be caused by the model,"Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",51
What should be considered when designing a classifier?,harms that may be caused by the model,"Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",51
What should be considered by designers of classifiers?,harms that may be caused by the model,"Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",51
What should classifier designers report in a model card?,model characteristics,"Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",151
What should be reported in a model card?,model characteristics,"Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",151
What should the designers report in the card?,model characteristics,"Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",151
What should designers report in a model card?,model characteristics,"Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",151
What should the designers report in the model card?,model characteristics,"Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.",151
What model of classification was introduced in this chapter?,logistic regression,This chapter introduced the logistic regression model of classification.,28
Which model of classification was introduced in this chapter?,logistic regression,This chapter introduced the logistic regression model of classification.,28
Which model of classification was introduced?,logistic regression,This chapter introduced the logistic regression model of classification.,28
What model of classification was introduced?,logistic regression,This chapter introduced the logistic regression model of classification.,28
What is the model of classification that was introduced in this chapter?,logistic regression,This chapter introduced the logistic regression model of classification.,28
What is supervised machine learning classifier?,Logistic regression,"Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",0
What is supervised machine learning?,Logistic regression,"Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",0
What is the difference between supervised and untrained machine learning?,Logistic regression,"Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",0
What is machine learning?,Logistic regression,"Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",0
What about machine learning?,Logistic regression,"Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",0
What is used to make a decision?,A threshold,"Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",228
What is used to make a decision?,A threshold,"Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",228
What is used when making a decision?,A threshold,"Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",228
What's used to make a decision?,A threshold,"Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",228
What is used in making a decision?,A threshold,"Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",228
How does logistic regression extract real-valued features from input?,"multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability","Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.",115
How many classes can logistic regression be used with?,two,"Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).",37
What is a good example of a multinomial logistic regression?,n-ary text classification,"Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).",164
What is a good example of a regression?,n-ary text classification,"Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).",164
What is a good example of a multinomial regression?,n-ary text classification,"Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).",164
What is the best example of a multinomial regression?,n-ary text classification,"Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).",164
What is an example of a multinomial regression?,n-ary text classification,"Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).",164
What function does multinomial logistic regression use to compute probabilities?,softmax,Multinomial logistic regression uses the softmax function to compute probabilities.,41
What is the function that multinomial logistic regression uses to compute probabilities?,softmax,Multinomial logistic regression uses the softmax function to compute probabilities.,41
What is the function multinomial logistic regression uses to compute probabilities?,softmax,Multinomial logistic regression uses the softmax function to compute probabilities.,41
What does multinomial logistic regression do?,softmax,Multinomial logistic regression uses the softmax function to compute probabilities.,41
What is the function that multinomial Logistic Regression uses to compute probabilities?,softmax,Multinomial logistic regression uses the softmax function to compute probabilities.,41
What are the weights learned from a labeled training set via?,a loss function,"The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.",78
What are the weights learned from a training set?,a loss function,"The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.",78
What are the weights learned from training?,a loss function,"The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.",78
What are the weights learned from the training set?,a loss function,"The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.",78
What are the weights learned from a training regimen?,a loss function,"The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.",78
What must be minimized?,cross-entropy loss,"The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.",107
What should be minimized?,cross-entropy loss,"The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.",107
What must be minimized?,cross-entropy loss,"The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.",107
What needs to be minimized?,cross-entropy loss,"The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.",107
What can be minimized?,cross-entropy loss,"The weights (vector w and bias b) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.",107
What is a convex optimization problem?,Minimizing this loss function,"Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.",0
What is the problem?,Minimizing this loss function,"Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.",0
Do you know what aconvex optimization problem is?,Minimizing this loss function,"Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.",0
What is used to find the optimal weights?,iterative algorithms like gradient descent,"Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.",68
How is it possible to find the optimal weights?,iterative algorithms like gradient descent,"Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.",68
What is used to find the best weights?,iterative algorithms like gradient descent,"Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.",68
What is used to find the optimal weights?,iterative algorithms like gradient descent,"Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.",68
What is used to find the right weights?,iterative algorithms like gradient descent,"Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.",68
Is regularization used to avoid overfitting?,Regularization is used to avoid overfitting,Regularization is used to avoid overfitting.,0
Is regularization used to avoid overfitting?,Regularization is used to avoid overfitting,Regularization is used to avoid overfitting.,0
Is regularization used to prevent overfitting?,Regularization is used to avoid overfitting,Regularization is used to avoid overfitting.,0
Is regularization done to avoid overfitting?,Regularization is used to avoid overfitting,Regularization is used to avoid overfitting.,0
What is one of the most useful analytic tools?,Logistic regression,"Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features.",0
What is the most useful analytic tool?,Logistic regression,"Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features.",0
One of the most useful analytic tools is what?,Logistic regression,"Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features.",0
One of the most useful analytic tools?,Logistic regression,"Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features.",0
What are the most useful analytic tools?,Logistic regression,"Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features.",0
Logistic regression studies the importance of what?,individual features,"Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features.",131
"In vector semantics, a word is modeled as what?",a vector,"In vector semantics, a word is modeled as a vector-a point in high-dimensional space, also called an embedding. In this chapter we focus on static embeddings, in each each word is mapped to a fixed embedding.",42
What is a point in high-dimensional space also called?,an embedding,"In vector semantics, a word is modeled as a vector-a point in high-dimensional space, also called an embedding. In this chapter we focus on static embeddings, in each each word is mapped to a fixed embedding.",98
What is a point in high-dimensional space?,an embedding,"In vector semantics, a word is modeled as a vector-a point in high-dimensional space, also called an embedding. In this chapter we focus on static embeddings, in each each word is mapped to a fixed embedding.",98
What is a point in space?,an embedding,"In vector semantics, a word is modeled as a vector-a point in high-dimensional space, also called an embedding. In this chapter we focus on static embeddings, in each each word is mapped to a fixed embedding.",98
What are the two classes of vector semantic models?,sparse and dense,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",46
What are the two classes of models?,sparse and dense,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",46
What are the two different classes of models?,sparse and dense,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",46
What are the classes of models?,sparse and dense,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",46
What are cells in sparse models functions of?,co-occurrence counts,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",165
What are the functions of cells in sparse models?,co-occurrence counts,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",165
What are the sparse models functions?,co-occurrence counts,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",165
What are the sparse models' functions?,co-occurrence counts,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",165
What are the sparse models functions of cells?,co-occurrence counts,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",165
How many sparse weightings are common?,Two,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",435
How many of them are common?,Two,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",435
What number of sparse weightings are common?,Two,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",435
What weighting weights each cell by its term frequency?,tf-idf,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",473
What is the weight of each cell by its term Frequency?,tf-idf,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",473
What weight is given to each cell by its term Frequency?,tf-idf,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",473
What weight is given to each cell's term Frequency?,tf-idf,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",473
What weight is given to each cell's term frequency?,tf-idf,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",473
What weighting weights each cell by its term frequency and inverse document frequency?,tf-idf,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",473
What is the weight of each cell by its term Frequency and inverse document Frequency?,tf-idf,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",473
What is the weight of each cell by its term and inverse document Frequency?,tf-idf,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",473
What is the weight of each cell by its term and inverse document frequencies?,tf-idf,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",473
What is the weight of each cell by its inverse document Frequency?,tf-idf,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",473
What is most common for for word-context matrices?,PPMI (pointwise positive mutual information,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",572
What are the most common word-context matrices?,PPMI (pointwise positive mutual information,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",572
What is the most common word-context matrix?,PPMI (pointwise positive mutual information,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",572
What is the average for word-context matrices?,PPMI (pointwise positive mutual information,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",572
What's the most common for word-context matrices?,PPMI (pointwise positive mutual information,"Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information) most common for for word-context matrices.",572
What is the dimensionality of dense vector models?,50-1000,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,40
What is the density of the models?,50-1000,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,40
What is the density of models?,50-1000,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,40
What is the density of a model?,50-1000,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,40
What is the relative density of models?,50-1000,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,40
What is a popular way to compute dense embeddings?,Word2vec algorithms like skip-gram,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,49
What is the most popular way to compute dense embeddings?,Word2vec algorithms like skip-gram,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,49
Is there a way to compute dense embeddeds?,Word2vec algorithms like skip-gram,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,49
What is the most popular way to compute dense embeddeds?,Word2vec algorithms like skip-gram,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,49
What is the best way to compute dense embeddings?,Word2vec algorithms like skip-gram,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,49
Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'?,Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,131
What is computed from the dot product between the embeddings for two words?,probability that two words are 'likely to occur nearby in text'. This probability,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,196
What is computed from the dot product between the two words?,probability that two words are 'likely to occur nearby in text'. This probability,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,196
What is computed from the dot product when there are two words?,probability that two words are 'likely to occur nearby in text'. This probability,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,196
What is computed from the dot product between the words?,probability that two words are 'likely to occur nearby in text'. This probability,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,196
What is computed from the dot product?,probability that two words are 'likely to occur nearby in text'. This probability,Dense vector models have dimensionality 50-1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are 'likely to occur nearby in text'. This probability is computed from the dot product between the embeddings for the two words.,196
What does Skip-gram use to train the classifier?,stochastic gradient descent,"Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.",15
What is Skip-gram used for?,stochastic gradient descent,"Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.",15
What does Skip-gram do to train?,stochastic gradient descent,"Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.",15
What does Skip-Gram do to train?,stochastic gradient descent,"Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.",15
What does Skip-gram do to train the class?,stochastic gradient descent,"Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.",15
What is a high dot product with embeddings of words that occur nearby?,embeddings,"Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.",80
What is a high dot product that has embedded words nearby?,embeddings,"Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.",80
What is a high dot product?,embeddings,"Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.",80
What is a high dot product that has words embedded in it?,embeddings,"Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.",80
What is a high dot product with embedded words?,embeddings,"Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.",80
GloVe is a method based on what kind of probabilities?,word co-occurrence,"Other important embedding algorithms include GloVe, a method based on ratios of word co-occurrence probabilities.",80
What kind of probabilities are used for GloVe?,word co-occurrence,"Other important embedding algorithms include GloVe, a method based on ratios of word co-occurrence probabilities.",80
What kind of probabilities do you use for GloVe?,word co-occurrence,"Other important embedding algorithms include GloVe, a method based on ratios of word co-occurrence probabilities.",80
What are word and document similarities computed by?,some function of the dot product between vectors,"Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",86
Word and document similarities are computed by what?,some function of the dot product between vectors,"Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",86
What is the most popular such metric?,The cosine of two vectors,"Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",136
What is the most popular metric?,The cosine of two vectors,"Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",136
Which is the most popular metric?,The cosine of two vectors,"Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",136
Which one is the most popular?,The cosine of two vectors,"Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",136
What's the most popular metric?,The cosine of two vectors,"Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors-a normalized dot product-is the most popular such metric.",136
What are neural networks made out of?,neural units,"Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device.",33
What are neural networks made of?,neural units,"Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device.",33
What are neural networks made out of?,neural units,"Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device.",33
What were neural networks originally inspired by?,human neurons,"Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device.",70
What are the neural networks that inspired them?,human neurons,"Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device.",70
What are the original neural networks that inspired them?,human neurons,"Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device.",70
What were the original neural networks that inspired them?,human neurons,"Neural networks are built out of neural units, originally inspired by human neurons but now simply an abstract computational device.",70
"Each neural unit multiplies input values by a weight vector, adds a bias, and then applies what type of activation function?",non-linear,"Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit.",93
What type of activation function is applied by each neural unit?,non-linear,"Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit.",93
What type of activation function is applied by each neural unit when it adds a bias?,non-linear,"Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit.",93
What type of activation function is applied by each neural unit when it adds a bias and multiplies input values?,non-linear,"Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit.",93
What type of activation function is applied by each neural unit when they add a bias?,non-linear,"Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit.",93
"In a fully connected, feedforward network, what is connected to each unit in layer i + 1?",each unit in layer i,"In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles.",43
What is connected to each unit in the feed forward network?,each unit in layer i,"In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles.",43
What is connected to each unit in the feedforward network?,each unit in layer i,"In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles.",43
What is connected to each unit in layer i + 1 in a feedforward network?,each unit in layer i,"In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles.",43
What is connected to each unit in layer i + 1 in a feed forward network?,each unit in layer i,"In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles.",43
There are no cycles in what network?,feedforward network,"In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles.",22
What network has no cycles?,feedforward network,"In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles.",22
What network have no cycles?,feedforward network,"In a fully-connected, feedforward network, each unit in layer i is connected to each unit in layer i + 1, and there are no cycles.",22
The power of neural networks comes from the ability of early layers to learn what?,representations,The power of neural networks comes from the ability of early layers to learn representations that can be utilized by later layers in the network.,77
What are neural networks trained by?,optimization algorithms like gradient descent,Neural networks are trained by optimization algorithms like gradient descent.,31
Neural networks are trained by what?,optimization algorithms like gradient descent,Neural networks are trained by optimization algorithms like gradient descent.,31
What is error backpropagation?,backward differentiation on a computation graph,"Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.",23
What is error backpropagation?,backward differentiation on a computation graph,"Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.",23
What does error backpropagation mean?,backward differentiation on a computation graph,"Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.",23
What is error backpagation?,backward differentiation on a computation graph,"Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.",23
Is error backpropagation a thing?,backward differentiation on a computation graph,"Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.",23
What is backward differentiation on a computation graph used for?,compute the gradients of the loss function for a network,"Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.",83
What is the difference between a graph and computation?,compute the gradients of the loss function for a network,"Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.",83
What is the difference between a graph and a computation?,compute the gradients of the loss function for a network,"Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.",83
What is the difference between a graph and a computation graph?,compute the gradients of the loss function for a network,"Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.",83
What do neural language models use as a probabilistic classifier?,a neural network,"Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words.",27
What are neural language models used for?,a neural network,"Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words.",27
What do neural language models do?,a neural network,"Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words.",27
What are neural language models used to do?,a neural network,"Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words.",27
What does a neural network compute?,the probability of the next word given the previous n words,"Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words.",86
What does a neural network do?,the probability of the next word given the previous n words,"Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words.",86
What does a network do?,the probability of the next word given the previous n words,"Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words.",86
How does a neural network work?,the probability of the next word given the previous n words,"Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words.",86
What do a neural network do?,the probability of the next word given the previous n words,"Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous n words.",86
Pretrained embeddings can be used by what type of model?,Neural language models,"Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",0
What type of model can pretrained embeddeds be used in?,Neural language models,"Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",0
What type of model can pretrained embeddings be used in?,Neural language models,"Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",0
What type of model can pretrained embeddeds be used for?,Neural language models,"Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",0
What type of model can trained embeddeds be used in?,Neural language models,"Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",0
Neural language models can learn what from scratch?,embeddings,"Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.",67
What did this chapter introduce?,parts of speech and named entities,"This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:",24
What was introduced in this chapter?,parts of speech and named entities,"This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:",24
What did this chapter do?,parts of speech and named entities,"This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:",24
What happened in this chapter?,parts of speech and named entities,"This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:",24
What tasks were introduced in this chapter?,partof-speech tagging and named entity recognition,"This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:",77
Which tasks were introduced in this chapter?,partof-speech tagging and named entity recognition,"This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:",77
What are the new tasks in this chapter?,partof-speech tagging and named entity recognition,"This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:",77
What new tasks were introduced in this chapter?,partof-speech tagging and named entity recognition,"This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:",77
What are the tasks that were introduced in this chapter?,partof-speech tagging and named entity recognition,"This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:",77
How many part-of-speech tagsets exist?,between 40 and 200,"Languages generally have a small set of closed class words that are highly frequent, ambiguous, and act as function words, and open-class words like nouns, verbs, adjectives. Various part-of-speech tagsets exist, of between 40 and 200 tags.",216
What is the process of assigning a part-of-speech label to a sequence of words?,Part-of-speech tagging,Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words.,0
How do you assign a part-of-speech label to a sequence of words?,Part-of-speech tagging,Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words.,0
Is it possible to assign a part-of-speech label to a sequence of words?,Part-of-speech tagging,Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words.,0
What is the process of assigning a part-of-speech label to a sequence of words?,Part-of-speech tagging,Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words.,0
Is there a process for assigning a part-of-speech label to a sequence of words?,Part-of-speech tagging,Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words.,0
What are words for proper nouns?,Named entities,"Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.",0
What words are used for proper nouns?,Named entities,"Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.",0
What are the words for proper nouns?,Named entities,"Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.",0
"Named entities refer to people, places, and what else?",organizations,"Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.",82
"What about people, places, and what not?",organizations,"Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.",82
What are named entities?,organizations,"Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.",82
What are named entities and what do they mean?,organizations,"Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.",82
What else are named entities?,organizations,"Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren't strictly entities or even proper nouns.",82
What are two common approaches to sequence modeling?,"generative approach, HMM tagging, and a discriminative approach, CRF tagging","Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",49
What are the most common approaches to sequence modeling?,"generative approach, HMM tagging, and a discriminative approach, CRF tagging","Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",49
What are some common approaches to sequence modeling?,"generative approach, HMM tagging, and a discriminative approach, CRF tagging","Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",49
What is a generative approach called?,HMM tagging,"Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",70
What is a generative approach?,HMM tagging,"Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",70
What is agenerative approach?,HMM tagging,"Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",70
How is CRF tagging defined?,discriminative,"Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",89
How is it defined?,discriminative,"Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",89
What is the definition of CRF tagging?,discriminative,"Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",89
How is this defined?,discriminative,"Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",89
How is the tag defined?,discriminative,"Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.",89
How are the probabilities in HMM taggers estimated?,maximum likelihood estimation on tag-labeled training corpora,"The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence",50
What are the probabilities in taggers?,maximum likelihood estimation on tag-labeled training corpora,"The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence",50
What are the probabilities in HMM taggers?,maximum likelihood estimation on tag-labeled training corpora,"The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence",50
How are the probabilities calculated?,maximum likelihood estimation on tag-labeled training corpora,"The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence",50
How are the probabilities for taggers estimated?,maximum likelihood estimation on tag-labeled training corpora,"The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence",50
"What algorithm is used for decoding, finding the most likely tag sequence?",Viterbi,"The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence",117
How do you find the most likely tag sequence?,Viterbi,"The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence",117
What is the best way to find the most likely tag sequence?,Viterbi,"The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence",117
What is the most likely sequence for decoding?,Viterbi,"The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence",117
What is used to find the most likely tag sequence?,Viterbi,"The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence",117
What is another name for Conditional Random Fields?,CRF taggers,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",29
What is the name of the field?,CRF taggers,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",29
What is another name for the field?,CRF taggers,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",29
What is another name for Random Fields?,CRF taggers,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",29
What is another name for random fields?,CRF taggers,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",29
What do CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence?,Conditional Random Fields,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",0
What does a log-linear model do to choose the best tag sequence?,Conditional Random Fields,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",0
What is the Viterbi algorithm used for?,inference,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",297
What is the purpose of the Viterbi algorithm?,inference,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",297
What is it used for?,inference,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",297
What is the use of the Viterbi algorithm?,inference,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",297
What is the purpose of the Viterbi Algorithm?,inference,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",297
What is another name for the Forward-Backward algorithm?,Viterbi algorithm,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",275
Is there another name for the Forward- Backward algorithm?,Viterbi algorithm,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",275
What is another name for the Forward- Backward algorithm?,Viterbi algorithm,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",275
Is there another name for the forward-backward algorithm?,Viterbi algorithm,"Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training.",275
What are recurrent neural networks and transformers?,language problems,This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:,118
What are neural networks?,language problems,This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:,118
What are neural networks and transformers?,language problems,This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:,118
What are recurrent neural networks?,language problems,This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:,118
What are transformers used for?,language problems,This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:,118
What are they used for?,language problems,This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:,118
What are the uses of transformers?,language problems,This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:,118
What are the uses of transformer?,language problems,This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:,118
What do transformers do?,language problems,This chapter has introduced the concepts of recurrent neural networks and transformers and how they can be applied to language problems. Here’s a summary of the main points that we covered:,118
How are sequences processed in Recurrent Neural Networks?,one element at a time,"In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1.",60
What is the output of each neural unit at time t based on?,the current input at t and the hidden layer from time t − 1,"In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1.",143
What is the output of each neural unit?,the current input at t and the hidden layer from time t − 1,"In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1.",143
What is the output of the neural units?,the current input at t and the hidden layer from time t − 1,"In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1.",143
What is the output of each neural unit at that time?,the current input at t and the hidden layer from time t − 1,"In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1.",143
What is the output of each unit?,the current input at t and the hidden layer from time t − 1,"In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time t based both on the current input at t and the hidden layer from time t − 1.",143
How can RNNs be trained?,with a straightforward extension of the backpropagation algorithm,"RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).",20
How can they be trained?,with a straightforward extension of the backpropagation algorithm,"RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).",20
How can RNNs be trained?,with a straightforward extension of the backpropagation algorithm,"RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).",20
How can we train RNNs?,with a straightforward extension of the backpropagation algorithm,"RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).",20
What is backpropagation through time?,BPTT,"RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).",126
What is backpropagation?,BPTT,"RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).",126
"Backpropagation through time, what is it?",BPTT,"RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).",126
What is the history of backpropagation through time?,BPTT,"RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).",126
What is the nature of backpropagation through time?,BPTT,"RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).",126
Why do simple recurrent networks fail on long inputs?,problems like vanishing gradients,Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers.,57
Modern systems use more complex gated architectures like what?,LSTMs,Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers.,160
LSTMs explicitly decide what to remember and forget in their hidden and context layers?,complex gated architectures,Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers.,124
"In their hidden and context layers, what to remember and forget?",complex gated architectures,Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers.,124
What to remember and forget in the hidden and context layers?,complex gated architectures,Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers.,124
"In their hidden and context layers, what to remember and what to forget?",complex gated architectures,Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers.,124
Transformers are non-recurrent networks based on what?,self-attention,"Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word.",49
What is the basis for non-recurrent networks?,self-attention,"Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word.",49
A selfattention layer maps input sequences to output sequences of the same length?,attention heads that each model how the surrounding words are relevant for the processing of the current word,"Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word.",166
What is a transformer block composed of?,a single attention layer,A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks.,32
What is a transformer block made of?,a single attention layer,A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks.,32
What is the composition of a transformer block?,a single attention layer,A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks.,32
What is the structure of a transformer block?,a single attention layer,A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks.,32
What can be stacked to make deeper and more powerful networks?,Transformer blocks,A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks.,156
What can be done to make networks more powerful?,Transformer blocks,A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks.,156
What can be stacked to make networks more powerful?,Transformer blocks,A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following each. Transformer blocks can be stacked to make deeper and more powerful networks.,156
What are some common language-based applications for RNNs and transformers?,:,Common language-based applications for RNNs and transformers include:,68
What are some language-based applications for transformers?,:,Common language-based applications for RNNs and transformers include:,68
What are some common language-based applications for transformers?,:,Common language-based applications for RNNs and transformers include:,68
What are some common language-based applications?,:,Common language-based applications for RNNs and transformers include:,68
What are some common language applications for transformers?,:,Common language-based applications for RNNs and transformers include:,68
What is Probabilistic language modeling?,"assigning a probability to a sequence, or to the next element of a sequence given the preceding words","Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",33
What is probabilistic language modeling?,"assigning a probability to a sequence, or to the next element of a sequence given the preceding words","Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",33
What does probabilistic language modeling do?,"assigning a probability to a sequence, or to the next element of a sequence given the preceding words","Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",33
What is Probabilistic language modeling?,"assigning a probability to a sequence, or to the next element of a sequence given the preceding words","Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",33
What is automatic regressive generation using?,a trained language model,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",170
What is the use of automatic regressive generation?,a trained language model,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",170
What is automatic regressive generation used for?,a trained language model,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",170
What is the automatic regressive generation used for?,a trained language model,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",170
What is the automatic generation used for?,a trained language model,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",170
What is assigned a label?,each element of a sequence,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",250
What is written on a label?,each element of a sequence,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",250
What is the assigned label?,each element of a sequence,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",250
What is written on the label?,each element of a sequence,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",250
What classification is used when an entire text is assigned to a category?,Sequence,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",299
"When a text is assigned to a category, what classification is used?",Sequence,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",299
"When an entire text is assigned to a category, what classification is used?",Sequence,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",299
"When a text is assigned to a category, what is the classification used?",Sequence,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",299
"When an entire text is assigned to a category, what is the classification used?",Sequence,"Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words. -Auto-regressive generation using a trained language model. -Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label. -Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.",299
What is one of the most widely used applications of NLP?,Machine translation,"Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.",0
What is one of the most used applications?,Machine translation,"Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.",0
What is one of the most widely used applications?,Machine translation,"Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.",0
What is one of the most popular applications?,Machine translation,"Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.",0
One of the most widely used applications?,Machine translation,"Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.",0
The encoder-decoder model was first developed for what?,MT,"Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.",123
What is the first thing the model was developed for?,MT,"Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.",123
What was the first thing the model was developed for?,MT,"Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.",123
What was the first thing that was done with the model?,MT,"Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.",123
"Languages have divergences, both structural and what else?",lexical,"Languages have divergences, both structural and lexical, that make translation difficult.",48
What field investigates some of these differences?,linguistic field of typology,The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects.,4
Languages can be classified by their position along typological dimensions like whether verbs precede what?,their objects,The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects.,175
What are encoder-decoder networks composed of?,an encoder network,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",75
What are the networks composed of?,an encoder network,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",75
What are the networks made of?,an encoder network,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",75
What are the networks made up of?,an encoder network,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",75
What are encoder-decoder networks made of?,an encoder network,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",75
What is an encoder network that takes an input sequence and creates a contextualized representation of it?,Encoder-decoder networks,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",0
What is a network that takes an input sequence and creates a representation of it?,Encoder-decoder networks,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",0
What is a network that takes an input sequence and makes a representation of it?,Encoder-decoder networks,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",0
What is an input sequence that is used to create a representation of it?,Encoder-decoder networks,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",0
What is a network that takes an input sequence and makes a representation out of it?,Encoder-decoder networks,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",0
The context representation is then passed to what?,a decoder,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",233
What is then passed to the context representation?,a decoder,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",233
What is the context representation passed to?,a decoder,"Encoder-decoder networks (either for RNNs or transformers) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.",233
What allows the decoder to view information from all the hidden states of the encoder?,attention mechanism in RNNs,"The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder.",4
What can the decoder see from the hidden states of the encoder?,attention mechanism in RNNs,"The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder.",4
What can the decoder see from the hidden states of the encoder?,attention mechanism in RNNs,"The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder.",4
What makes it possible for the decoder to see all the hidden states of the encoder?,attention mechanism in RNNs,"The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder.",4
What makes it possible for the decoder to see all the hidden states of the encoder?,attention mechanism in RNNs,"The attention mechanism in RNNs, and cross-attention in transformers, allows the decoder to view information from all the hidden states of the encoder.",4
What is greedy decoding for a decoder?,choosing the single most probable token to generate at each step,"For the decoder, choosing the single most probable token to generate at each step is called greedy decoding.",17
What is greedy decoding?,choosing the single most probable token to generate at each step,"For the decoder, choosing the single most probable token to generate at each step is called greedy decoding.",17
What is the purpose of decoding?,choosing the single most probable token to generate at each step,"For the decoder, choosing the single most probable token to generate at each step is called greedy decoding.",17
What does greedy decoding mean?,choosing the single most probable token to generate at each step,"For the decoder, choosing the single most probable token to generate at each step is called greedy decoding.",17
What is the reason for greedy decoding?,choosing the single most probable token to generate at each step,"For the decoder, choosing the single most probable token to generate at each step is called greedy decoding.",17
What is the name of the fixed-size memory footprint?,beam width,"In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width.",170
What is the name of the memory footprint?,beam width,"In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width.",170
What is the name of the footprint?,beam width,"In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width.",170
What is the name of the fixed-size memory footprint?,beam width,"In beam search, instead of choosing the best token to generate at each timestep, we keep k possible tokens at each step. This fixed-size memory footprint k is called the beam width.",170
Machine translation models are trained on what?,a parallel corpus,"Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages.",42
What are the machine translation models trained on?,a parallel corpus,"Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages.",42
What are the machine translation models trained to do?,a parallel corpus,"Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages.",42
A bitext is a text that appears in how many languages?,two,"Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages.",111
How many languages does a bitext appear in?,two,"Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages.",111
How many languages has a bitext appeared in?,two,"Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages.",111
What is backtranslation?,a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,19
What is backtranslation?,a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,19
"Backtranslation, what is it?",a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,19
What does backtranslation mean?,a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,19
What is a way of making use of monolingual corpora in the target language?,Backtranslation,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,0
How can monolingual corpora be used in the target language?,Backtranslation,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,0
Is there a way to use monolingual corpora in the target language?,Backtranslation,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,0
What is the best way to use monolingual corpora in a language?,Backtranslation,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,0
What is the best way to use monolingual corpora in the target language?,Backtranslation,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,0
How does a pilot MT engine create synthetic bitexts?,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,0
How does a pilot engine work?,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,0
How does a pilot engine create something?,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,0
How does a pilot engine create synthetic bits?,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,0
How does a pilot engine make synthetic bits?,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts.,0
What is MT evaluated by measuring?,a translation's adequacy (how well it captures the meaning of the source sentence) and fluency,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",29
What is evaluated by measuring?,a translation's adequacy (how well it captures the meaning of the source sentence) and fluency,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",29
What is assessed by measuring?,a translation's adequacy (how well it captures the meaning of the source sentence) and fluency,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",29
What is the evaluation of MT done by measuring?,a translation's adequacy (how well it captures the meaning of the source sentence) and fluency,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",29
What is the purpose of measuring?,a translation's adequacy (how well it captures the meaning of the source sentence) and fluency,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",29
What is the gold standard?,Human evaluation,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",178
What is the gold standard?,Human evaluation,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",178
What is the standard for gold?,Human evaluation,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",178
"The gold standard, what is it?",Human evaluation,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",178
What's the gold standard?,Human evaluation,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",178
Automatic evaluation metrics measure what?,character n-gram overlap with human translations,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",275
Automatic evaluation metrics measure what?,character n-gram overlap with human translations,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",275
What metrics measure what?,character n-gram overlap with human translations,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",275
Automatic evaluation metrics measure?,character n-gram overlap with human translations,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",275
What measure character n-gram overlap with human translations?,chrF,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",255
What is the correlation between character n-gram and human translations?,chrF,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",255
What is the difference between character n-gram and human translations?,chrF,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",255
What measure recent metrics based on embedding similarity?,chrF,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",255
What are the recent metrics based on similarity?,chrF,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",255
What are the recent metrics that are based on similarity?,chrF,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",255
What are the metrics based on similarity?,chrF,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",255
Is there a measure for recent metrics based on similarity?,chrF,"MT is evaluated by measuring a translation's adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.",255
What is a fundamental tool in language processing?,The regular expression,"The regular expression is a fundamental tool in language processing, and this chapter showed how to perform basic text normalization tasks.",0
What is the most important tool in language processing?,The regular expression,"The regular expression is a fundamental tool in language processing, and this chapter showed how to perform basic text normalization tasks.",0
What is the most fundamental tool in language processing?,The regular expression,"The regular expression is a fundamental tool in language processing, and this chapter showed how to perform basic text normalization tasks.",0
What is the most important tool for language processing?,The regular expression,"The regular expression is a fundamental tool in language processing, and this chapter showed how to perform basic text normalization tasks.",0
What did this chapter show how to perform?,basic text normalization tasks,"The regular expression is a fundamental tool in language processing, and this chapter showed how to perform basic text normalization tasks.",108
How did this chapter show how to perform?,basic text normalization tasks,"The regular expression is a fundamental tool in language processing, and this chapter showed how to perform basic text normalization tasks.",108
What did the chapter show us how to do?,basic text normalization tasks,"The regular expression is a fundamental tool in language processing, and this chapter showed how to perform basic text normalization tasks.",108
What is the minimum edit distance?,minimum edit distance,The minimum edit distance was introduced.,4
What is the minimum editing distance?,minimum edit distance,The minimum edit distance was introduced.,4
What is the minimum edit distance?,minimum edit distance,The minimum edit distance was introduced.,4
What is a summary of the main points we covered?,Here,Here is a summary of the main points we covered.,0
What is the summary of the main points?,Here,Here is a summary of the main points we covered.,0
What is the summary of what we did?,Here,Here is a summary of the main points we covered.,0
What language can be used for pattern matching?,regular expression,The regular expression language can be used for pattern matching.,4
Do you know what language can be used for pattern matching?,regular expression,The regular expression language can be used for pattern matching.,4
How can a language be used for pattern matching?,regular expression,The regular expression language can be used for pattern matching.,4
Which language can be used for pattern matching?,regular expression,The regular expression language can be used for pattern matching.,4
What language can be used for matching?,regular expression,The regular expression language can be used for pattern matching.,4
What are two basic operations in regular expressions?,concatenation of symbols and disjunction of symbols,Basic operations in regular expressions include concatenation of symbols and disjunction of symbols.,48
What are the basic operations in regular expressions?,concatenation of symbols and disjunction of symbols,Basic operations in regular expressions include concatenation of symbols and disjunction of symbols.,48
What are the two basic operations in regular expressions?,concatenation of symbols and disjunction of symbols,Basic operations in regular expressions include concatenation of symbols and disjunction of symbols.,48
What are two basic operations?,concatenation of symbols and disjunction of symbols,Basic operations in regular expressions include concatenation of symbols and disjunction of symbols.,48
Concatenation of symbols and disjunction of symbols are two examples of what?,Basic operations in regular expressions,Basic operations in regular expressions include concatenation of symbols and disjunction of symbols.,0
What can be done by cascades of simple regular expression substitution?,Word tokenization and normalization,Word tokenization and normalization can be done by cascades of simple regular expression substitution.,0
"What algorithm can be used to stemming, stripping off affixes?",Porter algorithm,"The Porter algorithm can be used to stemming, stripping off affixes.",4
"What is the best way to stemming, stripping off affixes?",Porter algorithm,"The Porter algorithm can be used to stemming, stripping off affixes.",4
What is the best way to stemming and stripping off affixes?,Porter algorithm,"The Porter algorithm can be used to stemming, stripping off affixes.",4
"Is there a way to stemming, stripping off affixes?",Porter algorithm,"The Porter algorithm can be used to stemming, stripping off affixes.",4
"What can be done to stemming, stripping off affixes?",Porter algorithm,"The Porter algorithm can be used to stemming, stripping off affixes.",4
What kind of accuracy does this tool not have?,high,"It doesn't have high accuracy, but it is useful for some tasks.",16
What kind of accuracy does this tool have?,high,"It doesn't have high accuracy, but it is useful for some tasks.",16
What kind of accuracy does the tool have?,high,"It doesn't have high accuracy, but it is useful for some tasks.",16
What type of accuracy does this tool have?,high,"It doesn't have high accuracy, but it is useful for some tasks.",16
What kind of accuracy does this tool lack?,high,"It doesn't have high accuracy, but it is useful for some tasks.",16
How many operations does it take to edit one into the other?,minimum number,The minimum number of operations it takes to edit one into the other is the minimum edit distance.,4
How many operations do it take to change one into the other?,minimum number,The minimum number of operations it takes to edit one into the other is the minimum edit distance.,4
How many operations does it take to change one into the other?,minimum number,The minimum number of operations it takes to edit one into the other is the minimum edit distance.,4
How much time does it take to change one into the other?,minimum number,The minimum number of operations it takes to edit one into the other is the minimum edit distance.,4
How many operations does it take to change one to the other?,minimum number,The minimum number of operations it takes to edit one into the other is the minimum edit distance.,4
What is the minimum edit distance?,The minimum number of operations,The minimum number of operations it takes to edit one into the other is the minimum edit distance.,0
What is the minimum editing distance?,The minimum number of operations,The minimum number of operations it takes to edit one into the other is the minimum edit distance.,0
What is the minimum edit distance?,The minimum number of operations,The minimum number of operations it takes to edit one into the other is the minimum edit distance.,0
What can be used to calculate the minimum edit distance?,Dynamic programming,Dynamic programming can be used to calculate the minimum edit distance.,0
How can we calculate the minimum edit distance?,Dynamic programming,Dynamic programming can be used to calculate the minimum edit distance.,0
How can the minimum edit distance be calculated?,Dynamic programming,Dynamic programming can be used to calculate the minimum edit distance.,0
How can I calculate the minimum edit distance?,Dynamic programming,Dynamic programming can be used to calculate the minimum edit distance.,0
What can be used to calculate the minimum edit distance?,Dynamic programming,Dynamic programming can be used to calculate the minimum edit distance.,0
In what chapter were language modeling and the n-gram introduced?,this chapter,Language modeling and the n-gram were introduced in this chapter.,52
Which chapter introduced language modeling and the n-gram?,this chapter,Language modeling and the n-gram were introduced in this chapter.,52
What chapter introduced language modeling and the n-gram?,this chapter,Language modeling and the n-gram were introduced in this chapter.,52
Language models can be used to assign what to a sentence or a sequence of words?,a probability,Language models can be used to assign a probability to a sentence or a sequence of words.,38
What are models that estimate words from a fixed window?,n-grams,n-grams are models that estimate words from a fixed window.,0
What models estimate words from a window?,n-grams,n-grams are models that estimate words from a fixed window.,0
What are the models that estimate words from a window?,n-grams,n-grams are models that estimate words from a fixed window.,0
What are the models used to estimate words from a window?,n-grams,n-grams are models that estimate words from a fixed window.,0
What are the models that estimate the words from a window?,n-grams,n-grams are models that estimate words from a fixed window.,0
What is the name of the neophyte?,n,n,0
What is the name of a new person?,n,n,0
What is the name of the person?,n,n,0
What is the name of the new person?,n,n,0
What is the name of a person?,n,n,0
What does naivete mean?,n,n,0
What does naivete mean?,n,n,0
What is naivete?,n,n,0
What is the meaning of naivete?,n,n,0
What do naivete mean?,n,n,0
What can be estimated by counting and normalizing?,Gram probabilities,Gram probabilities can be estimated by counting and normalizing.,0
What can be estimated using counting and normalizing?,Gram probabilities,Gram probabilities can be estimated by counting and normalizing.,0
What can be estimated with counting and normalizing?,Gram probabilities,Gram probabilities can be estimated by counting and normalizing.,0
What can be estimated with the help of counting and normalizing?,Gram probabilities,Gram probabilities can be estimated by counting and normalizing.,0
How much can be estimated by counting and normalizing?,Gram probabilities,Gram probabilities can be estimated by counting and normalizing.,0
What is used to evaluate n-gram language models?,perplexity,n-gram language models are evaluated using perplexity.,43
What is the process for evaluating n-gram language models?,perplexity,n-gram language models are evaluated using perplexity.,43
What is done to evaluate n-gram language models?,perplexity,n-gram language models are evaluated using perplexity.,43
What is used to evaluate the models?,perplexity,n-gram language models are evaluated using perplexity.,43
What is used to evaluate language models?,perplexity,n-gram language models are evaluated using perplexity.,43
What is the geometric mean of the inverse test set probability computed by the model?,the perplexity of a test set,The geometric mean of the inverse test set probability computed by the model is the perplexity of a test set.,80
What is the geometric mean of the inverse test set probability?,the perplexity of a test set,The geometric mean of the inverse test set probability computed by the model is the perplexity of a test set.,80
What is the geometric mean of the inverse test set probabilities?,the perplexity of a test set,The geometric mean of the inverse test set probability computed by the model is the perplexity of a test set.,80
What is a more sophisticated way to estimate the probability of n-grams?,There is a more sophisticated way to estimate the probability of n-grams.,There is a more sophisticated way to estimate the probability of n-grams.,0
Is there a better way to estimate the probability of n-grams?,There is a more sophisticated way to estimate the probability of n-grams.,There is a more sophisticated way to estimate the probability of n-grams.,0
Is there a more advanced way to estimate the probability of n-grams?,There is a more sophisticated way to estimate the probability of n-grams.,There is a more sophisticated way to estimate the probability of n-grams.,0
What is a better way to estimate the probability?,There is a more sophisticated way to estimate the probability of n-grams.,There is a more sophisticated way to estimate the probability of n-grams.,0
Lower-order n-gram counts are used for what?,smoothing,Lower-order n-gram counts are used for smoothing.,39
What is the use of lower-order n-gram counts?,smoothing,Lower-order n-gram counts are used for smoothing.,39
What is required to create a probability distribution?,discounting,discounting is required to create a probability distribution.,0
What needs to be done to create a probability distribution?,discounting,discounting is required to create a probability distribution.,0
What is required to make a distribution?,discounting,discounting is required to create a probability distribution.,0
What is needed to make a probability distribution?,discounting,discounting is required to create a probability distribution.,0
What is needed to create a distribution?,discounting,discounting is required to create a probability distribution.,0
What is the probability of a word being a novel continuation used for?,probability of a word being a novel continuation is used.,The probability of a word being a novel continuation is used.,4
What is the likelihood of a word being a novel continuation?,probability of a word being a novel continuation is used.,The probability of a word being a novel continuation is used.,4
What is the chance of a word being a novel continuation?,probability of a word being a novel continuation is used.,The probability of a word being a novel continuation is used.,4
What is the chance that a word is a novel continuation?,probability of a word being a novel continuation is used.,The probability of a word being a novel continuation is used.,4
What is the likelihood of a word being a continuation?,probability of a word being a novel continuation is used.,The probability of a word being a novel continuation is used.,4
What is mixed with a lower-order continuation probability?,A discounted probability,A discounted probability is mixed with a lower-order continuation probability.,0
What is included with a lower-order continuation probability?,A discounted probability,A discounted probability is mixed with a lower-order continuation probability.,0
What is included in a lower-order continuation probability?,A discounted probability,A discounted probability is mixed with a lower-order continuation probability.,0
What was applied to the text categorization task of sentiment analysis?,naive Bayes model,The naive Bayes model was applied to the text categorization task of sentiment analysis.,4
What was done to the text categorization task?,naive Bayes model,The naive Bayes model was applied to the text categorization task of sentiment analysis.,4
What was applied to the sentiment analysis task?,naive Bayes model,The naive Bayes model was applied to the text categorization task of sentiment analysis.,4
What was done with the text categorization task?,naive Bayes model,The naive Bayes model was applied to the text categorization task of sentiment analysis.,4
What was done to the sentiment analysis task?,naive Bayes model,The naive Bayes model was applied to the text categorization task of sentiment analysis.,4
What was the naive Bayes model?,text categorization task of sentiment analysis,The naive Bayes model was applied to the text categorization task of sentiment analysis.,41
What was the naive Bayes model?,text categorization task of sentiment analysis,The naive Bayes model was applied to the text categorization task of sentiment analysis.,41
What was the Bayes model like?,text categorization task of sentiment analysis,The naive Bayes model was applied to the text categorization task of sentiment analysis.,41
What was the naive Bayes model like?,text categorization task of sentiment analysis,The naive Bayes model was applied to the text categorization task of sentiment analysis.,41
Language processing tasks can be seen as tasks of what?,classification,Language processing tasks can be seen as tasks of classification.,50
Text categorization assigns an entire text a class from a finite set of what?,sentiment analysis,"Text categorization, in which an entire text is assigned a class from a finite set, includes tasks such as sentiment analysis.",107
What type of analysis considers a text to reflect the positive or negative orientation of the writer?,Sentiment,Sentiment analysis considers a text to be a reflection of the positive or negative orientation of the writer.,0
What type of analysis considers a text to reflect the orientation of the writer?,Sentiment,Sentiment analysis considers a text to be a reflection of the positive or negative orientation of the writer.,0
Which type of analysis considers a text to reflect the orientation of the writer?,Sentiment,Sentiment analysis considers a text to be a reflection of the positive or negative orientation of the writer.,0
What type of analysis considers a text to be positive or negative?,Sentiment,Sentiment analysis considers a text to be a reflection of the positive or negative orientation of the writer.,0
What type of analysis considers a text to be either positive or negative?,Sentiment,Sentiment analysis considers a text to be a reflection of the positive or negative orientation of the writer.,0
The bag of words assumption and the conditional independence assumption are made by what model?,generative,The bag of words assumption and the conditional independence assumption are made by a generative model.,86
What model makes the bag of words assumption and the conditional independence assumption?,generative,The bag of words assumption and the conditional independence assumption are made by a generative model.,86
What model makes the bag of words assumption and theconditional independence assumption?,generative,The bag of words assumption and the conditional independence assumption are made by a generative model.,86
What model is used to make the bag of words assumption and the conditional independence assumption?,generative,The bag of words assumption and the conditional independence assumption are made by a generative model.,86
Bayes with binarized features seem to work better for what?,text classification tasks,Bayes with binarized features seem to work better for text classification tasks.,54
Bayes with binarized features seem to work better for what?,text classification tasks,Bayes with binarized features seem to work better for text classification tasks.,54
What are classifiers evaluated on?,recall and precision,Classifiers are evaluated on their recall and precision.,35
What are the results of the tests?,recall and precision,Classifiers are evaluated on their recall and precision.,35
What are the results of the evaluation?,recall and precision,Classifiers are evaluated on their recall and precision.,35
What are the results of the evaluation of the classifiers?,recall and precision,Classifiers are evaluated on their recall and precision.,35
What are the things that are evaluated?,recall and precision,Classifiers are evaluated on their recall and precision.,35
Classifiers are evaluated on what?,their recall and precision,Classifiers are evaluated on their recall and precision.,29
What are the things that are evaluated on what?,their recall and precision,Classifiers are evaluated on their recall and precision.,29
What is evaluated on what?,their recall and precision,Classifiers are evaluated on their recall and precision.,29
Is there an evaluation on what?,their recall and precision,Classifiers are evaluated on their recall and precision.,29
What is included in the training set?,The use of cross-validation,The use of cross-validation in the training set is included in the training set.,0
What is included in the training set?,The use of cross-validation,The use of cross-validation in the training set is included in the training set.,0
What is in the training set?,The use of cross-validation,The use of cross-validation in the training set is included in the training set.,0
What is included in the training?,The use of cross-validation,The use of cross-validation in the training set is included in the training set.,0
What should we use if we can't be sure that one version is better than the other?,a statistical significance test,"If we can't be sure that one version is better than the other, we should use a statistical significance test.",77
"If we can't be sure that one version is better than the other, what should we use?",a statistical significance test,"If we can't be sure that one version is better than the other, we should use a statistical significance test.",77
"If we can't be certain that one version is better than the other, what should we use?",a statistical significance test,"If we can't be sure that one version is better than the other, we should use a statistical significance test.",77
"If we can't be sure that one version is better than the other, what should we do?",a statistical significance test,"If we can't be sure that one version is better than the other, we should use a statistical significance test.",77
"If we can't be certain that one version is better than the other, what should we do?",a statistical significance test,"If we can't be sure that one version is better than the other, we should use a statistical significance test.",77
What should harms that may be caused by a model be reported in?,a model card,"The harms that may be caused by the model, including its training data and other components, should be reported in a model card.",115
What harms should be reported in a model?,a model card,"The harms that may be caused by the model, including its training data and other components, should be reported in a model card.",115
What harms may be caused by a model?,a model card,"The harms that may be caused by the model, including its training data and other components, should be reported in a model card.",115
What harms might be caused by a model?,a model card,"The harms that may be caused by the model, including its training data and other components, should be reported in a model card.",115
What harms should be reported when a model is used?,a model card,"The harms that may be caused by the model, including its training data and other components, should be reported in a model card.",115
What model of classification was introduced in this chapter?,model of classification,The model of classification was introduced in this chapter.,4
Which model of classification was introduced in this chapter?,model of classification,The model of classification was introduced in this chapter.,4
Which model of classification was introduced?,model of classification,The model of classification was introduced in this chapter.,4
What model of classification was introduced?,model of classification,The model of classification was introduced in this chapter.,4
What is the model of classification that was introduced in this chapter?,model of classification,The model of classification was introduced in this chapter.,4
What is supervised machine learning classification?,Logistic regression,"Logistic regression is a supervised machine learning classification that extracts real-valued features from the input, sums them, and passes the sum through a sigmoid function to generate a probability.",0
What is supervised machine learning?,Logistic regression,"Logistic regression is a supervised machine learning classification that extracts real-valued features from the input, sums them, and passes the sum through a sigmoid function to generate a probability.",0
What is the classification of machine learning?,Logistic regression,"Logistic regression is a supervised machine learning classification that extracts real-valued features from the input, sums them, and passes the sum through a sigmoid function to generate a probability.",0
What is the classification for machine learning?,Logistic regression,"Logistic regression is a supervised machine learning classification that extracts real-valued features from the input, sums them, and passes the sum through a sigmoid function to generate a probability.",0
What does supervised machine learning mean?,Logistic regression,"Logistic regression is a supervised machine learning classification that extracts real-valued features from the input, sums them, and passes the sum through a sigmoid function to generate a probability.",0
What does logistic regression extract from input?,real-valued features,"Logistic regression is a supervised machine learning classification that extracts real-valued features from the input, sums them, and passes the sum through a sigmoid function to generate a probability.",82
What is used to make a decision?,A threshold,A threshold is used to make a decision.,0
What is used to make a decision?,A threshold,A threshold is used to make a decision.,0
What is used when making a decision?,A threshold,A threshold is used to make a decision.,0
What's used to make a decision?,A threshold,A threshold is used to make a decision.,0
What is used in making a decision?,A threshold,A threshold is used to make a decision.,0
What can be used with more than one class?,Logistic regression,"Logistic regression can be used with more than one class, for example for text classification and part-of-speech labeling.",0
What can be used with more than one class?,Logistic regression,"Logistic regression can be used with more than one class, for example for text classification and part-of-speech labeling.",0
What can be done with more than one class?,Logistic regression,"Logistic regression can be used with more than one class, for example for text classification and part-of-speech labeling.",0
What can be used for more than one class?,Logistic regression,"Logistic regression can be used with more than one class, for example for text classification and part-of-speech labeling.",0
"For example, what can logistic regression be used for?",text classification and part-of-speech labeling,"Logistic regression can be used with more than one class, for example for text classification and part-of-speech labeling.",74
What can Logistic Regression be used for?,text classification and part-of-speech labeling,"Logistic regression can be used with more than one class, for example for text classification and part-of-speech labeling.",74
What can logistic regression be used for?,text classification and part-of-speech labeling,"Logistic regression can be used with more than one class, for example for text classification and part-of-speech labeling.",74
What function is used to compute probabilities?,softmax,The softmax function is used to compute probabilities.,4
What is the function used to compute probabilities?,softmax,The softmax function is used to compute probabilities.,4
Which function is used to compute probabilities?,softmax,The softmax function is used to compute probabilities.,4
What function is used to calculate probabilities?,softmax,The softmax function is used to compute probabilities.,4
What is the function that computes probabilities?,softmax,The softmax function is used to compute probabilities.,4
What is a problem that can be solved using iterative techniques?,Minimizing the loss function,Minimizing the loss function is a problem that can be solved using iterative techniques.,0
Is there a problem that can be solved using iterative techniques?,Minimizing the loss function,Minimizing the loss function is a problem that can be solved using iterative techniques.,0
How can iterative techniques be used to solve a problem?,Minimizing the loss function,Minimizing the loss function is a problem that can be solved using iterative techniques.,0
What is a problem that can be solved using iterative techniques?,Minimizing the loss function,Minimizing the loss function is a problem that can be solved using iterative techniques.,0
Is regularization used to avoid overfitting?,Regularization is used to avoid overfitting,Regularization is used to avoid overfitting.,0
Is regularization used to avoid overfitting?,Regularization is used to avoid overfitting,Regularization is used to avoid overfitting.,0
Is regularization used to prevent overfitting?,Regularization is used to avoid overfitting,Regularization is used to avoid overfitting.,0
Is regularization done to avoid overfitting?,Regularization is used to avoid overfitting,Regularization is used to avoid overfitting.,0
What is one of the most useful analytic tools?,Logistic regression,Logistic regression is one of the most useful analytic tools because of its ability to study the importance of individual features.,0
What is the most useful analytic tool?,Logistic regression,Logistic regression is one of the most useful analytic tools because of its ability to study the importance of individual features.,0
One of the most useful analytic tools is what?,Logistic regression,Logistic regression is one of the most useful analytic tools because of its ability to study the importance of individual features.,0
One of the most useful analytic tools?,Logistic regression,Logistic regression is one of the most useful analytic tools because of its ability to study the importance of individual features.,0
What are the most useful analytic tools?,Logistic regression,Logistic regression is one of the most useful analytic tools because of its ability to study the importance of individual features.,0
Logistic regression studies the importance of what?,individual features,Logistic regression is one of the most useful analytic tools because of its ability to study the importance of individual features.,111
What is modeled as a point in high-dimensional space?,A word,A word is modeled as a point in high-dimensional space.,0
What is a point in high-dimensional space?,A word,A word is modeled as a point in high-dimensional space.,0
What is a point in space?,A word,A word is modeled as a point in high-dimensional space.,0
What is the point in high-dimensional space?,A word,A word is modeled as a point in high-dimensional space.,0
What is a point in a high-dimensional space?,A word,A word is modeled as a point in high-dimensional space.,0
What is mapped to a fixed embedded in the chapter?,Each word,Each word is mapped to a fixed embedded in the chapter.,0
What is mapped to a fixed in the chapter?,Each word,Each word is mapped to a fixed embedded in the chapter.,0
What is mapped in the chapter?,Each word,Each word is mapped to a fixed embedded in the chapter.,0
What are the two classes of models?,sparse and dense,There are two classes of models: sparse and dense.,33
What are the two classes of models?,sparse and dense,There are two classes of models: sparse and dense.,33
What are the classes of models?,sparse and dense,There are two classes of models: sparse and dense.,33
What are the two different classes of models?,sparse and dense,There are two classes of models: sparse and dense.,33
Cells are functions of what?,co-occurrence counts,Cells are functions of co-occurrence counts and each corresponds to a word in the vocabulary.,23
What are the functions of cells?,co-occurrence counts,Cells are functions of co-occurrence counts and each corresponds to a word in the vocabulary.,23
Cell functions are what?,co-occurrence counts,Cells are functions of co-occurrence counts and each corresponds to a word in the vocabulary.,23
Cells are functions of what?,co-occurrence counts,Cells are functions of co-occurrence counts and each corresponds to a word in the vocabulary.,23
Each cell corresponds to a word in the vocabulary?,Cells,Cells are functions of co-occurrence counts and each corresponds to a word in the vocabulary.,0
What does the matrix have for each word in the vocabulary?,a row,The matrix has a row for each word in the vocabulary and a column for each document.,15
What is the matrix for each word?,a row,The matrix has a row for each word in the vocabulary and a column for each document.,15
What do the words in the matrix have in common?,a row,The matrix has a row for each word in the vocabulary and a column for each document.,15
What is the matrix for each word in the vocabulary?,a row,The matrix has a row for each word in the vocabulary and a column for each document.,15
What does the matrix contain for each word?,a row,The matrix has a row for each word in the vocabulary and a column for each document.,15
What is the column for each document?,matrix,The matrix has a row for each word in the vocabulary and a column for each document.,4
What is the column for each document?,matrix,The matrix has a row for each word in the vocabulary and a column for each document.,4
What column is used for each document?,matrix,The matrix has a row for each word in the vocabulary and a column for each document.,4
What is the column for the document?,matrix,The matrix has a row for each word in the vocabulary and a column for each document.,4
What is another name for the word-context matrix?,term-term,The word-context or term-term matrix has a row for each target word in the vocabulary and a column for each context term in the vocabulary.,20
Is there another name for the word-context matrix?,term-term,The word-context or term-term matrix has a row for each target word in the vocabulary and a column for each context term in the vocabulary.,20
What is the name of the word-context matrix?,term-term,The word-context or term-term matrix has a row for each target word in the vocabulary and a column for each context term in the vocabulary.,20
What is the row for each target word in the vocabulary?,The word-context or term-term matrix,The word-context or term-term matrix has a row for each target word in the vocabulary and a column for each context term in the vocabulary.,0
What is the row for each word?,The word-context or term-term matrix,The word-context or term-term matrix has a row for each target word in the vocabulary and a column for each context term in the vocabulary.,0
What is the row for each word in the vocabulary?,The word-context or term-term matrix,The word-context or term-term matrix has a row for each target word in the vocabulary and a column for each context term in the vocabulary.,0
What is the row for each target word?,The word-context or term-term matrix,The word-context or term-term matrix has a row for each target word in the vocabulary and a column for each context term in the vocabulary.,0
The column for each context term is what?,vocabulary,The word-context or term-term matrix has a row for each target word in the vocabulary and a column for each context term in the vocabulary.,128
What is the column for each term?,vocabulary,The word-context or term-term matrix has a row for each target word in the vocabulary and a column for each context term in the vocabulary.,128
What is the column for each context term?,vocabulary,The word-context or term-term matrix has a row for each target word in the vocabulary and a column for each context term in the vocabulary.,128
What is the column for the context term?,vocabulary,The word-context or term-term matrix has a row for each target word in the vocabulary and a column for each context term in the vocabulary.,128
What's the column for each term?,vocabulary,The word-context or term-term matrix has a row for each target word in the vocabulary and a column for each context term in the vocabulary.,128
What are the two most common sparse weights for word-context matrices?,tf-idf and PPMI,The tf-idf and PPMI are the two most common sparse weights for word-context matrices.,4
What are the sparse weights for word-context matrices?,tf-idf and PPMI,The tf-idf and PPMI are the two most common sparse weights for word-context matrices.,4
What are the most common sparse weights for word-context matrices?,tf-idf and PPMI,The tf-idf and PPMI are the two most common sparse weights for word-context matrices.,4
What are the two most common sparse weights for word-context matrices?,tf-idf and PPMI,The tf-idf and PPMI are the two most common sparse weights for word-context matrices.,4
What are the dimensions of the models?,50-1000,The models have dimensions of 50-1000.,30
What are the dimensions of the models?,50-1000,The models have dimensions of 50-1000.,30
What is the size of the models?,50-1000,The models have dimensions of 50-1000.,30
What are the proportions of the models?,50-1000,The models have dimensions of 50-1000.,30
What is a popular way to compute dense embeddeds?,Word2vec,Word2vec is a popular way to compute dense embeddeds.,0
What is the most popular way to compute dense embeddeds?,Word2vec,Word2vec is a popular way to compute dense embeddeds.,0
What is the best way to compute dense embeddeds?,Word2vec,Word2vec is a popular way to compute dense embeddeds.,0
Is there a way to compute dense embeddeds?,Word2vec,Word2vec is a popular way to compute dense embeddeds.,0
What is a good way to compute dense embeddeds?,Word2vec,Word2vec is a popular way to compute dense embeddeds.,0
What is the chance that two words are likely to appear near a text?,There,There is a chance that two words are likely to occur nearby in text.,0
What product is used to calculate the probability?,dot product,The dot product is used to calculate the probability.,4
Which product is used to calculate the probability?,dot product,The dot product is used to calculate the probability.,4
How is the probability calculated?,dot product,The dot product is used to calculate the probability.,4
What is used to calculate the probability?,dot product,The dot product is used to calculate the probability.,4
What method does Skip-gram use to train the classifier?,stochastic gradient descent,Skip-gram uses a method called stochastic gradient descent to train the classifier by learning embedded words that have a high dot product and a low dot product.,31
What is the method called that skip-gram uses to train a classifier by learning embedded words that have a high and low dot product?,stochastic gradient descent,Skip-gram uses a method called stochastic gradient descent to train the classifier by learning embedded words that have a high dot product and a low dot product.,31
What is the method used to train a classifier by learning words with high and low dot products?,stochastic gradient descent,Skip-gram uses a method called stochastic gradient descent to train the classifier by learning embedded words that have a high dot product and a low dot product.,31
What is the method used to train a classifier by learning words that have a high and low dot product?,stochastic gradient descent,Skip-gram uses a method called stochastic gradient descent to train the classifier by learning embedded words that have a high dot product and a low dot product.,31
What is the method used to train a classifier by learning words with a high and low dot product?,stochastic gradient descent,Skip-gram uses a method called stochastic gradient descent to train the classifier by learning embedded words that have a high dot product and a low dot product.,31
What is the method used to train a classifier by learning embedded words that have a high and low dot product?,stochastic gradient descent,Skip-gram uses a method called stochastic gradient descent to train the classifier by learning embedded words that have a high dot product and a low dot product.,31
What is the name of the method based on word co-occurrence probabilities?,GloVe,The method based on word co-occurrence probabilities is called GloVe.,63
What is the name of the method?,GloVe,The method based on word co-occurrence probabilities is called GloVe.,63
What is the name of the method that uses word co-occurrence probabilities?,GloVe,The method based on word co-occurrence probabilities is called GloVe.,63
What is GloVe?,The method based on word co-occurrence probabilities,The method based on word co-occurrence probabilities is called GloVe.,0
What is it called?,The method based on word co-occurrence probabilities,The method based on word co-occurrence probabilities is called GloVe.,0
What is the name of the thing?,The method based on word co-occurrence probabilities,The method based on word co-occurrence probabilities is called GloVe.,0
What is the name of the product?,The method based on word co-occurrence probabilities,The method based on word co-occurrence probabilities is called GloVe.,0
What is the name of this thing?,The method based on word co-occurrence probabilities,The method based on word co-occurrence probabilities is called GloVe.,0
What determines the similarities between Word and document?,the function of the dot product between the vectors,Word and document similarities are determined by the function of the dot product between the vectors.,49
What is the relationship between Word and document?,the function of the dot product between the vectors,Word and document similarities are determined by the function of the dot product between the vectors.,49
What is the difference between Word and a document?,the function of the dot product between the vectors,Word and document similarities are determined by the function of the dot product between the vectors.,49
What is the most popular metric?,normalized dot product,The normalized dot product is the most popular metric.,4
What is the most popular metric?,normalized dot product,The normalized dot product is the most popular metric.,4
Which is the most popular metric?,normalized dot product,The normalized dot product is the most popular metric.,4
What is the most used metric?,normalized dot product,The normalized dot product is the most popular metric.,4
What's the most popular metric?,normalized dot product,The normalized dot product is the most popular metric.,4
What were neural networks originally inspired by?,human neurons,Neural networks were originally inspired by human neurons but are now just an abstract computational device.,44
What are the neural networks that inspired them?,human neurons,Neural networks were originally inspired by human neurons but are now just an abstract computational device.,44
What are the original neural networks that inspired them?,human neurons,Neural networks were originally inspired by human neurons but are now just an abstract computational device.,44
What were the original neural networks that inspired them?,human neurons,Neural networks were originally inspired by human neurons but are now just an abstract computational device.,44
What are neural networks now just an abstract computational device?,Neural networks were originally inspired by human neurons,Neural networks were originally inspired by human neurons but are now just an abstract computational device.,0
What are neural networks now?,Neural networks were originally inspired by human neurons,Neural networks were originally inspired by human neurons but are now just an abstract computational device.,0
Neural units add a bias and apply a non- linear activation function like what?,"sigmoid, tanh, or rectified linear unit","Neural units add a bias and apply a non- linear activation function like sigmoid, tanh, or rectified linear unit.",73
What is the name of the unit in layer i connected to?,each unit in layer i + 1,Each unit in layer i is connected to each unit in layer i + 1 and there are no cycles.,37
What is the name of the unit I connected to?,each unit in layer i + 1,Each unit in layer i is connected to each unit in layer i + 1 and there are no cycles.,37
What is the name of the unit in the layer?,each unit in layer i + 1,Each unit in layer i is connected to each unit in layer i + 1 and there are no cycles.,37
What is the name of the unit I'm connected to?,each unit in layer i + 1,Each unit in layer i is connected to each unit in layer i + 1 and there are no cycles.,37
What is the name of the unit that I connected to?,each unit in layer i + 1,Each unit in layer i is connected to each unit in layer i + 1 and there are no cycles.,37
What happens when each unit is connected to each unit?,there are no cycles,Each unit in layer i is connected to each unit in layer i + 1 and there are no cycles.,66
"When each unit is connected, what happens?",there are no cycles,Each unit in layer i is connected to each unit in layer i + 1 and there are no cycles.,66
What happens when each unit is connected?,there are no cycles,Each unit in layer i is connected to each unit in layer i + 1 and there are no cycles.,66
What happens when each unit is connected to another?,there are no cycles,Each unit in layer i is connected to each unit in layer i + 1 and there are no cycles.,66
"When each unit is connected to each other, what happens?",there are no cycles,Each unit in layer i is connected to each unit in layer i + 1 and there are no cycles.,66
What is powered by the ability of early layers to learn representations that can be used by later layers?,Neural networks,Neural networks are powered by the ability of early layers to learn representations that can be used by later layers.,0
What is powered by the ability of early layers to learn?,Neural networks,Neural networks are powered by the ability of early layers to learn representations that can be used by later layers.,0
How are neural networks trained?,Neural networks are trained.,Neural networks are trained.,0
How are neural networks trained?,Neural networks are trained.,Neural networks are trained.,0
What is used to compute the loss function for a network?,error backpropagation,The error backpropagation is used to compute the loss function for a network.,4
What is used to calculate the loss function?,error backpropagation,The error backpropagation is used to compute the loss function for a network.,4
How is the loss function calculated for a network?,error backpropagation,The error backpropagation is used to compute the loss function for a network.,4
What is used to calculate the network's loss function?,error backpropagation,The error backpropagation is used to compute the loss function for a network.,4
What is used to calculate the loss function for a network?,error backpropagation,The error backpropagation is used to compute the loss function for a network.,4
What do neural language models use to estimate the probability of the next word?,a neural network,Neural language models use a neural network to estimate the probability of the next word.,27
Neural language models can learn embeddeds from scratch or what?,pretrained,Neural language models can learn embeddeds from scratch or pretrained them.,59
What were introduced in this chapter?,Part of speech and named entities,Part of speech and named entities were introduced in this chapter.,0
What new things were introduced in this chapter?,Part of speech and named entities,Part of speech and named entities were introduced in this chapter.,0
What was introduced in this chapter?,Part of speech and named entities,Part of speech and named entities were introduced in this chapter.,0
What were the new things introduced in this chapter?,Part of speech and named entities,Part of speech and named entities were introduced in this chapter.,0
What happened in this chapter?,Part of speech and named entities,Part of speech and named entities were introduced in this chapter.,0
What was introduced in the chapter that dealt with speech and named entities?,Part of speech,Part of speech and named entities were introduced in this chapter.,0
"In the chapter that dealt with speech and named entities, what was introduced?",Part of speech,Part of speech and named entities were introduced in this chapter.,0
How many closed class words are there?,small,"There is a small set of closed class words that are highly frequent, ambiguous, and act as function words.",11
How many closed class words are there?,small,"There is a small set of closed class words that are highly frequent, ambiguous, and act as function words.",11
What number of closed class words are there?,small,"There is a small set of closed class words that are highly frequent, ambiguous, and act as function words.",11
What do closed class terms act as?,function words,"There is a small set of closed class words that are highly frequent, ambiguous, and act as function words.",91
What do closed class terms mean?,function words,"There is a small set of closed class words that are highly frequent, ambiguous, and act as function words.",91
What are closed class terms?,function words,"There is a small set of closed class words that are highly frequent, ambiguous, and act as function words.",91
What do closed class terms do?,function words,"There is a small set of closed class words that are highly frequent, ambiguous, and act as function words.",91
What do closed class terms have to do with?,function words,"There is a small set of closed class words that are highly frequent, ambiguous, and act as function words.",91
How many part-of-speech tags are there?,between 40 and 200,There are between 40 and 200 part-of-speech tags.,10
What number of part-of-speech tags are there?,between 40 and 200,There are between 40 and 200 part-of-speech tags.,10
What is assigned to a sequence of words?,A part-of-speech label,A part-of-speech label is assigned to a sequence of words.,0
What is the order of the words?,A part-of-speech label,A part-of-speech label is assigned to a sequence of words.,0
What is the sequence of words?,A part-of-speech label,A part-of-speech label is assigned to a sequence of words.,0
What are many other types of named entities?,not strictly entities or even proper nouns,Many other types of named entities are not strictly entities or even proper nouns.,39
What are other types of entities?,not strictly entities or even proper nouns,Many other types of named entities are not strictly entities or even proper nouns.,39
What other types of entities are there?,not strictly entities or even proper nouns,Many other types of named entities are not strictly entities or even proper nouns.,39
What are some other types of entities?,not strictly entities or even proper nouns,Many other types of named entities are not strictly entities or even proper nouns.,39
What are the other types of entities?,not strictly entities or even proper nouns,Many other types of named entities are not strictly entities or even proper nouns.,39
What are the two approaches to sequence modeling?,generative and discriminative,Two approaches to sequence modeling are generative and discriminative.,40
What are the two approaches to sequence modeling?,generative and discriminative,Two approaches to sequence modeling are generative and discriminative.,40
What are the approaches to sequence modeling?,generative and discriminative,Two approaches to sequence modeling are generative and discriminative.,40
What are the two different approaches to sequence modeling?,generative and discriminative,Two approaches to sequence modeling are generative and discriminative.,40
What does the maximum likelihood estimation estimate on tag-labeled training corpora?,the probabilities in HMM taggers,The maximum likelihood estimation on tag-labeled training corpora estimates the probabilities in HMM taggers.,76
What is the maximum likelihood estimate for tag-labeled training corpora?,the probabilities in HMM taggers,The maximum likelihood estimation on tag-labeled training corpora estimates the probabilities in HMM taggers.,76
What is the maximum likelihood estimate?,the probabilities in HMM taggers,The maximum likelihood estimation on tag-labeled training corpora estimates the probabilities in HMM taggers.,76
What is the maximum likelihood estimate for tag-labeled trainingcorpora?,the probabilities in HMM taggers,The maximum likelihood estimation on tag-labeled training corpora estimates the probabilities in HMM taggers.,76
What is the maximum likelihood estimation for tag-labeled training corpora?,the probabilities in HMM taggers,The maximum likelihood estimation on tag-labeled training corpora estimates the probabilities in HMM taggers.,76
What algorithm is used to find the most likely tag sequence?,Viterbi,The most likely tag sequence is found using the Viterbi algorithm.,48
How do you find the most likely tag sequence?,Viterbi,The most likely tag sequence is found using the Viterbi algorithm.,48
What is the best way to find the most likely tag sequence?,Viterbi,The most likely tag sequence is found using the Viterbi algorithm.,48
What is the method for finding the most likely tag sequence?,Viterbi,The most likely tag sequence is found using the Viterbi algorithm.,48
What method is used to find the most likely tag sequence?,Viterbi,The most likely tag sequence is found using the Viterbi algorithm.,48
What can be chosen based on features that condition on the output tag?,The best tag sequence,"The best tag sequence can be chosen based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep.",0
What features can be chosen based on the output tag?,The best tag sequence,"The best tag sequence can be chosen based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep.",0
What features can be chosen based on the condition of the output tag?,The best tag sequence,"The best tag sequence can be chosen based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep.",0
What can be chosen based on the condition of the output tag?,The best tag sequence,"The best tag sequence can be chosen based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep.",0
What is the best tag sequence?,"output tag, the prior output tag, the entire input sequence, and the current timestep","The best tag sequence can be chosen based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep.",76
The best tag sequence?,"output tag, the prior output tag, the entire input sequence, and the current timestep","The best tag sequence can be chosen based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep.",76
What is the best tag sequence?,"output tag, the prior output tag, the entire input sequence, and the current timestep","The best tag sequence can be chosen based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep.",76
Which tag sequence is the best?,"output tag, the prior output tag, the entire input sequence, and the current timestep","The best tag sequence can be chosen based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep.",76
Which is the best tag sequence?,"output tag, the prior output tag, the entire input sequence, and the current timestep","The best tag sequence can be chosen based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep.",76
What is used for training?,The best sequence of tags and a version of the Forward- Backward algorithm,The best sequence of tags and a version of the Forward- Backward algorithm are used for training.,0
What is used for training?,The best sequence of tags and a version of the Forward- Backward algorithm,The best sequence of tags and a version of the Forward- Backward algorithm are used for training.,0
What do you use for training?,The best sequence of tags and a version of the Forward- Backward algorithm,The best sequence of tags and a version of the Forward- Backward algorithm are used for training.,0
What are the things used for training?,The best sequence of tags and a version of the Forward- Backward algorithm,The best sequence of tags and a version of the Forward- Backward algorithm are used for training.,0
What is done for training?,The best sequence of tags and a version of the Forward- Backward algorithm,The best sequence of tags and a version of the Forward- Backward algorithm are used for training.,0
What algorithm is used?,Forward- Backward,The best sequence of tags and a version of the Forward- Backward algorithm are used for training.,47
What is the method used?,Forward- Backward,The best sequence of tags and a version of the Forward- Backward algorithm are used for training.,47
What is it that is used?,Forward- Backward,The best sequence of tags and a version of the Forward- Backward algorithm are used for training.,47
What is the formula used?,Forward- Backward,The best sequence of tags and a version of the Forward- Backward algorithm are used for training.,47
What is the method used for it?,Forward- Backward,The best sequence of tags and a version of the Forward- Backward algorithm are used for training.,47
The concepts of recurrent neural networks and transformers have been introduced in what chapter?,this chapter,The concepts of recurrent neural networks and transformers have been introduced in this chapter.,83
What chapter has the concepts of recurrent neural networks and transformers been introduced?,this chapter,The concepts of recurrent neural networks and transformers have been introduced in this chapter.,83
What chapter has the concepts of recurrent neural networks and transformers introduced?,this chapter,The concepts of recurrent neural networks and transformers have been introduced in this chapter.,83
Which chapter has the concepts of recurrent neural networks and transformers introduced?,this chapter,The concepts of recurrent neural networks and transformers have been introduced in this chapter.,83
In what chapter have the concepts of recurrent neural networks and transformers been introduced?,this chapter,The concepts of recurrent neural networks and transformers have been introduced in this chapter.,83
What is a summary of the main points we covered?,Here,Here is a summary of the main points we covered.,0
What is the summary of the main points?,Here,Here is a summary of the main points we covered.,0
What is the summary of what we did?,Here,Here is a summary of the main points we covered.,0
The output of each neural unit is based on the current input at what?,t,The output of each neural unit is based on the current input at t and the hidden layer from time t  1.,64
What is the current input for each neural unit?,t,The output of each neural unit is based on the current input at t and the hidden layer from time t  1.,64
What is the hidden layer from time t 1?,neural unit,The output of each neural unit is based on the current input at t and the hidden layer from time t  1.,19
What is the hidden layer from the beginning?,neural unit,The output of each neural unit is based on the current input at t and the hidden layer from time t  1.,19
What is the hidden layer?,neural unit,The output of each neural unit is based on the current input at t and the hidden layer from time t  1.,19
What is the hidden layer in time?,neural unit,The output of each neural unit is based on the current input at t and the hidden layer from time t  1.,19
What is a simple extension of the backpropagation algorithm?,Backpropagation through time,Backpropagation through time is a simple extension of the backpropagation algorithm.,0
What is an extension of the backpropagation algorithm?,Backpropagation through time,Backpropagation through time is a simple extension of the backpropagation algorithm.,0
What is a simple extension of the backpropagation algorithm?,Backpropagation through time,Backpropagation through time is a simple extension of the backpropagation algorithm.,0
What is the extension of the backpropagation algorithm?,Backpropagation through time,Backpropagation through time is a simple extension of the backpropagation algorithm.,0
Modern systems use more complex gated architectures that explicitly decide what to remember and forget in their hidden and context layers?,simple recurrent networks fail,"Modern systems use more complex gated architectures that explicitly decide what to remember and forget in their hidden and context layers, which is why simple recurrent networks fail.",152
Simple recurrent networks fail because of what?,gated architectures that explicitly decide what to remember and forget in their hidden and context layers,"Modern systems use more complex gated architectures that explicitly decide what to remember and forget in their hidden and context layers, which is why simple recurrent networks fail.",32
Simple recurrent networks fail because of what?,gated architectures that explicitly decide what to remember and forget in their hidden and context layers,"Modern systems use more complex gated architectures that explicitly decide what to remember and forget in their hidden and context layers, which is why simple recurrent networks fail.",32
Simple recurrent networks fail because of something?,gated architectures that explicitly decide what to remember and forget in their hidden and context layers,"Modern systems use more complex gated architectures that explicitly decide what to remember and forget in their hidden and context layers, which is why simple recurrent networks fail.",32
What are non-recurrent networks called?,transformer networks,Non-recurrent networks are called transformer networks.,34
What are non-recurrent networks?,transformer networks,Non-recurrent networks are called transformer networks.,34
How does a selfattention layer map input sequence to output sequence of the same length?,based on a set of attention heads,"A selfattention layer maps input sequence to output sequence of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word",81
How does a selfattention layer map the input sequence to the output sequence?,based on a set of attention heads,"A selfattention layer maps input sequence to output sequence of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word",81
How does a selfattention layer map input sequence to output sequence of the same length?,based on a set of attention heads,"A selfattention layer maps input sequence to output sequence of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word",81
What model how the surrounding words are relevant for the processing of the current word?,"selfattention layer maps input sequence to output sequence of the same length, based on a set of attention heads","A selfattention layer maps input sequence to output sequence of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word",2
How do the surrounding words relate to the current word?,"selfattention layer maps input sequence to output sequence of the same length, based on a set of attention heads","A selfattention layer maps input sequence to output sequence of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word",2
What model is used to process the current word?,"selfattention layer maps input sequence to output sequence of the same length, based on a set of attention heads","A selfattention layer maps input sequence to output sequence of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word",2
What model is used for the processing of the current word?,"selfattention layer maps input sequence to output sequence of the same length, based on a set of attention heads","A selfattention layer maps input sequence to output sequence of the same length, based on a set of attention heads that each model how the surrounding words are relevant for the processing of the current word",2
A transformer block consists of a single attention layer followed by a feed forward layer with what?,residual connections,A transformer block consists of a single attention layer followed by a feed forward layer with residual connections.,95
What can be stacked to make more powerful networks?,Transformer blocks,Transformer blocks can be stacked to make more powerful networks.,0
What can be done to make networks more powerful?,Transformer blocks,Transformer blocks can be stacked to make more powerful networks.,0
What kind of applications are there for RNNs and transformers?,language-based,There are common language-based applications for RNNs and transformers.,17
What are the applications for RNNs and transformers?,language-based,There are common language-based applications for RNNs and transformers.,17
What types of applications are available for RNNs and transformers?,language-based,There are common language-based applications for RNNs and transformers.,17
What applications are available for RNNs and transformers?,language-based,There are common language-based applications for RNNs and transformers.,17
What types of applications are there for RNNs and transformers?,language-based,There are common language-based applications for RNNs and transformers.,17
What type of modeling is language modeling?,probabilistic,Language modeling is probabilistic.,21
What type of modeling is used?,probabilistic,Language modeling is probabilistic.,21
Which type of modeling is it?,probabilistic,Language modeling is probabilistic.,21
What is the type of modeling?,probabilistic,Language modeling is probabilistic.,21
What can be assigned to the next element of a sequence?,a probability,The next element of a sequence can be assigned a probability.,47
What can be assigned to the next part of the sequence?,a probability,The next element of a sequence can be assigned a probability.,47
What can be assigned to the next part of a sequence?,a probability,The next element of a sequence can be assigned a probability.,47
What should be assigned to the next part of the sequence?,a probability,The next element of a sequence can be assigned a probability.,47
What should be assigned to the next part of a sequence?,a probability,The next element of a sequence can be assigned a probability.,47
What is used for auto-regressive generation?,trained language model,A trained language model is used for auto-regressive generation.,2
What is used to generate auto-regressive generation?,trained language model,A trained language model is used for auto-regressive generation.,2
What do you use for auto-regressive generation?,trained language model,A trained language model is used for auto-regressive generation.,2
What is used in the generation of auto-regressive generation?,trained language model,A trained language model is used for auto-regressive generation.,2
What is used in auto-regressive generation?,trained language model,A trained language model is used for auto-regressive generation.,2
What is the name of the sequence?,Sequence,Sequence.,0
What is the name of the sequence?,Sequence,Sequence.,0
What is the title of the sequence?,Sequence,Sequence.,0
What's the name of the sequence?,Sequence,Sequence.,0
What does Sequence do?,Sequence,Sequence.,0
What does Sequence do?,Sequence,Sequence.,0
What do Sequence do?,Sequence,Sequence.,0
Sequence does what?,Sequence,Sequence.,0
What is assigned to each element of a sequence?,a label,Each element of a sequence is assigned a label.,39
What is assigned to each part of a sequence?,a label,Each element of a sequence is assigned a label.,39
What is assigned to each part of the sequence?,a label,Each element of a sequence is assigned a label.,39
What are the elements of a sequence?,a label,Each element of a sequence is assigned a label.,39
What is assigned to the parts of a sequence?,a label,Each element of a sequence is assigned a label.,39
A sequence classification is where an entire text is assigned to what category?,a category,A sequence classification is where an entire text is assigned to a category.,65
What is the difference between a sequence classification and a category classification?,a category,A sequence classification is where an entire text is assigned to a category.,65
What is a sequence classification?,a category,A sequence classification is where an entire text is assigned to a category.,65
What is one of the most widely used applications of NLP?,machine translation,"One of the most widely used applications of NLP is machine translation, and the encoder-decoder model is a key tool that has applications throughout the system.",51
What is one of the most used applications?,machine translation,"One of the most widely used applications of NLP is machine translation, and the encoder-decoder model is a key tool that has applications throughout the system.",51
What is one of the most widely used applications?,machine translation,"One of the most widely used applications of NLP is machine translation, and the encoder-decoder model is a key tool that has applications throughout the system.",51
What is one of the most popular applications?,machine translation,"One of the most widely used applications of NLP is machine translation, and the encoder-decoder model is a key tool that has applications throughout the system.",51
One of the most widely used applications?,machine translation,"One of the most widely used applications of NLP is machine translation, and the encoder-decoder model is a key tool that has applications throughout the system.",51
What is a key tool that has applications throughout the system?,encoder-decoder model,"One of the most widely used applications of NLP is machine translation, and the encoder-decoder model is a key tool that has applications throughout the system.",80
What is a tool used throughout the system?,encoder-decoder model,"One of the most widely used applications of NLP is machine translation, and the encoder-decoder model is a key tool that has applications throughout the system.",80
What is the most important tool in the system?,encoder-decoder model,"One of the most widely used applications of NLP is machine translation, and the encoder-decoder model is a key tool that has applications throughout the system.",80
What is a tool that is used throughout the system?,encoder-decoder model,"One of the most widely used applications of NLP is machine translation, and the encoder-decoder model is a key tool that has applications throughout the system.",80
What is a key tool that is used throughout the system?,encoder-decoder model,"One of the most widely used applications of NLP is machine translation, and the encoder-decoder model is a key tool that has applications throughout the system.",80
Structural and lexical differences make translation difficult?,Structural and lexical differences make translation difficult.,Structural and lexical differences make translation difficult.,0
Is it possible that structural and lexical differences make translation difficult?,Structural and lexical differences make translation difficult.,Structural and lexical differences make translation difficult.,0
Is translation difficult because of structural and lexical differences?,Structural and lexical differences make translation difficult.,Structural and lexical differences make translation difficult.,0
What field investigates some of the differences?,typology,Some of the differences are investigated by the linguistic field of typology.,68
What field looks at the differences?,typology,Some of the differences are investigated by the linguistic field of typology.,68
Some of the differences are investigated by what field?,typology,Some of the differences are investigated by the linguistic field of typology.,68
What is a transformer composed of?,an Encoder network,A transformer is composed of an Encoder network that takes an input sequence and creates a contextualized representation of it.,29
What is a transformer made of?,an Encoder network,A transformer is composed of an Encoder network that takes an input sequence and creates a contextualized representation of it.,29
What is the composition of a transformer?,an Encoder network,A transformer is composed of an Encoder network that takes an input sequence and creates a contextualized representation of it.,29
What is a transformer made out of?,an Encoder network,A transformer is composed of an Encoder network that takes an input sequence and creates a contextualized representation of it.,29
What network takes an input sequence and creates a contextualized representation of it?,Encoder network,A transformer is composed of an Encoder network that takes an input sequence and creates a contextualized representation of it.,32
What network creates a representation of an input sequence?,Encoder network,A transformer is composed of an Encoder network that takes an input sequence and creates a contextualized representation of it.,32
What network takes an input sequence and makes a representation of it?,Encoder network,A transformer is composed of an Encoder network that takes an input sequence and creates a contextualized representation of it.,32
What network takes an input sequence and creates a representation of it?,Encoder network,A transformer is composed of an Encoder network that takes an input sequence and creates a contextualized representation of it.,32
What network creates a contextualized representation of an input sequence?,Encoder network,A transformer is composed of an Encoder network that takes an input sequence and creates a contextualized representation of it.,32
A task-specific output sequence is generated from what context representation?,context representation,A task-specific output sequence is generated from this context representation.,55
What context representation is used to generate a task-specific output sequence?,context representation,A task-specific output sequence is generated from this context representation.,55
What context representation is used to generate the task-specific output sequence?,context representation,A task-specific output sequence is generated from this context representation.,55
What context representation is used to generate a task specific output sequence?,context representation,A task-specific output sequence is generated from this context representation.,55
What context representation is used to create a task-specific output sequence?,context representation,A task-specific output sequence is generated from this context representation.,55
What allows the decoder to see all the hidden states of the encoder?,attention mechanism in RNNs and cross-attention in transformers,The attention mechanism in RNNs and cross-attention in transformers allow the decoder to see all the hidden states of the encoder.,4
What makes it possible for the decoder to see all the hidden states?,attention mechanism in RNNs and cross-attention in transformers,The attention mechanism in RNNs and cross-attention in transformers allow the decoder to see all the hidden states of the encoder.,4
What makes it possible for the decoder to see all the hidden states?,attention mechanism in RNNs and cross-attention in transformers,The attention mechanism in RNNs and cross-attention in transformers allow the decoder to see all the hidden states of the encoder.,4
What is the most probable token to generate at each step?,greedy decoding,The single most probable token to generate at each step is called greedy decoding.,66
What is the most likely token to be generated at each step?,greedy decoding,The single most probable token to generate at each step is called greedy decoding.,66
What token is most likely to be generated at each step?,greedy decoding,The single most probable token to generate at each step is called greedy decoding.,66
What is the most likely token to be generated?,greedy decoding,The single most probable token to generate at each step is called greedy decoding.,66
What token is the most likely to be generated at each step?,greedy decoding,The single most probable token to generate at each step is called greedy decoding.,66
What is greedy decoding?,The single most probable token to generate at each step,The single most probable token to generate at each step is called greedy decoding.,0
What is greedy decoding?,The single most probable token to generate at each step,The single most probable token to generate at each step is called greedy decoding.,0
What is the purpose of decoding?,The single most probable token to generate at each step,The single most probable token to generate at each step is called greedy decoding.,0
What does greedy decoding mean?,The single most probable token to generate at each step,The single most probable token to generate at each step is called greedy decoding.,0
What is the purpose of greedy decoding?,The single most probable token to generate at each step,The single most probable token to generate at each step is called greedy decoding.,0
What do we do instead of choosing the best token to generate at each step?,we keep possible token at each step,"Instead of choosing the best token to generate at each step, we keep possible token at each step.",61
"Instead of choosing the best token to generate at each step, what should we do?",we keep possible token at each step,"Instead of choosing the best token to generate at each step, we keep possible token at each step.",61
What should we do instead of choosing the best token?,we keep possible token at each step,"Instead of choosing the best token to generate at each step, we keep possible token at each step.",61
What is the fixed-size memory footprint k?,The beam width,The beam width is the fixed-size memory footprint k.,0
What is the size of the memory footprint?,The beam width,The beam width is the fixed-size memory footprint k.,0
What is the fixed-size memory footprint?,The beam width,The beam width is the fixed-size memory footprint k.,0
What is the memory footprint?,The beam width,The beam width is the fixed-size memory footprint k.,0
A bitext is a text that appears in more than one language?,bitext is a text that appears in more than one language.,A bitext is a text that appears in more than one language.,2
What is a way of making use of monolingual corpora in the target language?,Backtranslation,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot engine backwards.,0
How can monolingual corpora be used in the target language?,Backtranslation,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot engine backwards.,0
Is there a way to use monolingual corpora in the target language?,Backtranslation,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot engine backwards.,0
What is the best way to use monolingual corpora in a language?,Backtranslation,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot engine backwards.,0
What is the best way to use monolingual corpora in the target language?,Backtranslation,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot engine backwards.,0
What is backtranslation?,a way of making use of monolingual corpora in the target language by running a pilot engine backwards,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot engine backwards.,19
What is backtranslation?,a way of making use of monolingual corpora in the target language by running a pilot engine backwards,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot engine backwards.,19
"Backtranslation, what is it?",a way of making use of monolingual corpora in the target language by running a pilot engine backwards,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot engine backwards.,19
What does backtranslation mean?,a way of making use of monolingual corpora in the target language by running a pilot engine backwards,Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot engine backwards.,19
How is translation's effectiveness measured?,how well it captures the meaning of the source sentence and how natural it is in the target language,The translation's effectiveness is measured by how well it captures the meaning of the source sentence and how natural it is in the target language.,47
Is translation's effectiveness measured?,how well it captures the meaning of the source sentence and how natural it is in the target language,The translation's effectiveness is measured by how well it captures the meaning of the source sentence and how natural it is in the target language.,47
How is translation measured?,how well it captures the meaning of the source sentence and how natural it is in the target language,The translation's effectiveness is measured by how well it captures the meaning of the source sentence and how natural it is in the target language.,47
What is translation's effectiveness measured?,how well it captures the meaning of the source sentence and how natural it is in the target language,The translation's effectiveness is measured by how well it captures the meaning of the source sentence and how natural it is in the target language.,47
How well does translation capture the meaning of the source sentence and how natural it is in target language?,effectiveness,The translation's effectiveness is measured by how well it captures the meaning of the source sentence and how natural it is in the target language.,18
How well does translation capture the meaning of the source sentence and how natural it is in the target language?,effectiveness,The translation's effectiveness is measured by how well it captures the meaning of the source sentence and how natural it is in the target language.,18
How well does translation capture the meaning of the source sentence and how natural is it in the target language?,effectiveness,The translation's effectiveness is measured by how well it captures the meaning of the source sentence and how natural it is in the target language.,18
How well does translation capture the meaning of the source sentence?,effectiveness,The translation's effectiveness is measured by how well it captures the meaning of the source sentence and how natural it is in the target language.,18
How well does translation capture the meaning of the source sentence and how natural it is in target language?,effectiveness,The translation's effectiveness is measured by how well it captures the meaning of the source sentence and how natural it is in the target language.,18
What measure character n-gram overlap with human translations?,chrF,"Automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, and more recent metrics based on embedded similarity are commonly used.",34
What is the correlation between character n-gram and human translations?,chrF,"Automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, and more recent metrics based on embedded similarity are commonly used.",34
What is the difference between character n-gram and human translations?,chrF,"Automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, and more recent metrics based on embedded similarity are commonly used.",34
What measure recent metrics based on embedded similarity are commonly used?,Automatic evaluation metrics,"Automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, and more recent metrics based on embedded similarity are commonly used.",0
What metrics are used to measure recent events?,Automatic evaluation metrics,"Automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, and more recent metrics based on embedded similarity are commonly used.",0
